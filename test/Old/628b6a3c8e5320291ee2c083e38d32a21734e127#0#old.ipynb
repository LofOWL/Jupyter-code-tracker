{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Bayesian Linear Regression Project\n",
    "\n",
    "In this notebook, we will implement a complete machine learning project, focusing on Bayesian Inference methods, in particular, Bayesian Linear Regression. We will go through the entire machine learning process, cleaning the data, exploring it to find trends, establishing a baseline model, evaluating several machine learning approaches for comparisons, implementing Bayesian Linear Regression, interpreting the results, and presenting the results. Let's get started!\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We are using data on student grades collected from a Portuguese secondary (high) school. This data is from the UCI machine learning repository, a great collection of datasets for model testing. The includes academic and personal characteristics of the students as well as final grades. The objective is to predict the final grade from the student data which makes this a __supervised, regression task__. \n",
    "\n",
    "We have a set of training data with known labels, and we want the model to learn a mapping from the features (explanatory variables) to the target (the label) in this case the final grade. It is a regression task because the final grade is a continuous value. This is a fairly standard machine learning problem, and we will walk through the entire proces. Once you have the workflow of a machine learning problem down, you can start tackling problems in a structed, efficient manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    " \n",
    "\n",
    "# Matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.size'] = 16\n",
    "matplotlib.rcParams['figure.figsize'] = (9, 9)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "from scipy.stats import percentileofscore\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard ML Models for comparison\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Splitting data into training/testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "\n",
    "# Distributions\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyMC3 for Bayesian Inference\n",
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data and Examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in class scores\n",
    "df = pd.read_csv('data/student-mat.csv')\n",
    "\n",
    "# Filter out grades that were 0\n",
    "df = df[~df['G3'].isin([0, 1])]\n",
    "\n",
    "df = df.rename(columns={'G3': 'Grade'})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Describe for Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Values Counts for Object Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        print('\\nColumn Name:', col,)\n",
    "        print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Grade'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Grade'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df['Grade'].value_counts().index, \n",
    "        df['Grade'].value_counts().values,\n",
    "         fill = 'navy', edgecolor = 'k', width = 1)\n",
    "plt.xlabel('Grade'); plt.ylabel('Count'); plt.title('Distribution of Final Grades');\n",
    "plt.xticks(list(range(5, 20)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df.ix[df['address'] == 'U', 'Grade'], label = 'Urban', shade = True)\n",
    "sns.kdeplot(df.ix[df['address'] == 'R', 'Grade'], label = 'Rural', shade = True)\n",
    "plt.xlabel('Grade'); plt.ylabel('Density'); plt.title('Density Plot of Final Grades by Location');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df.ix[df['guardian'] == 'father', 'Grade'], label = 'Father', shade = True)\n",
    "sns.kdeplot(df.ix[df['guardian'] == 'mother', 'Grade'], label = 'Mother', shade = True)\n",
    "sns.kdeplot(df.ix[df['guardian'] == 'other', 'Grade'], label = 'Other', shade = True)\n",
    "plt.xlabel('Grade'); plt.ylabel('Density'); plt.title('Density Plot of Final Grades by Guardian');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df.ix[df['internet'] == 'yes', 'Grade'], label = 'Internet', shade = True)\n",
    "sns.kdeplot(df.ix[df['internet'] == 'no', 'Grade'], label = 'No Internet', shade = True)\n",
    "plt.xlabel('Grade'); plt.ylabel('Density'); plt.title('Density Plot of Final Grades by Internet Access');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df.ix[df['school'] == 'GP', 'Grade'], label = 'GP', shade = True)\n",
    "sns.kdeplot(df.ix[df['school'] == 'MS', 'Grade'], label = 'MS', shade = True)\n",
    "plt.xlabel('Grade'); plt.ylabel('Count'); plt.title('Distribution of Final Grades by School');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schools = df.groupby(['school'])['address'].value_counts()\n",
    "schools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grade Percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['percentile'] = df['Grade'].apply(lambda x: percentileofscore(df['Grade'], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 6))\n",
    "plt.plot(df['Grade'], df['percentile'], 'o')\n",
    "plt.xticks(range(0, 20, 2), range(0, 20, 2))\n",
    "plt.xlabel('Score'); plt.ylabel('Percentile'); plt.title('Grade Percentiles');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('50th percentile score:', np.min(df.loc[df['percentile'] > 50, 'Grade']))\n",
    "print('Minimum Score needed for 90th percentile:', np.min(df.loc[df['percentile'] > 90, 'Grade']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations with Final Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()['Grade'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Dataframe Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only categorical variables\n",
    "category_df = df.select_dtypes('object')\n",
    "# One hot encode the variables\n",
    "dummy_df = pd.get_dummies(category_df)\n",
    "# Put the grade back in the dataframe\n",
    "dummy_df['Grade'] = df['Grade']\n",
    "dummy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df.corr()['Grade'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select 5 Most Correlated Variables with Final Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(df):\n",
    "    # Targets are final grade of student\n",
    "    labels = df['Grade']\n",
    "    \n",
    "    # Drop the school and the grades from features\n",
    "    df = df.drop(columns=['school', 'G1', 'G2', 'percentile'])\n",
    "    \n",
    "    # One-Hot Encoding of Categorical Variables\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    # Find correlations with the Grade\n",
    "    most_correlated = df.corr().abs()['Grade'].sort_values(ascending=False)\n",
    "    \n",
    "    # Maintain the top 5 most correlation features with Grade\n",
    "    most_correlated = most_correlated[:8]\n",
    "    \n",
    "    df = df.ix[:, most_correlated.index]\n",
    "    df = df.drop(columns = 'higher_no')\n",
    "    \n",
    "    # Split into training/testing sets with 25% split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, labels, \n",
    "                                                        test_size = 0.25,\n",
    "                                                        random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = format_data(df)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.rename(columns={'higher_yes': 'higher_edu', \n",
    "                                  'Medu': 'mother_edu',\n",
    "                                  'Fedu': 'father_edu'})\n",
    "\n",
    "X_test = X_test.rename(columns={'higher_yes': 'higher_edu', \n",
    "                                  'Medu': 'mother_edu',\n",
    "                                  'Fedu': 'father_edu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairs Plot of Used Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate correlation coefficient\n",
    "def corrfunc(x, y, **kws):\n",
    "    r, _ = stats.pearsonr(x, y)\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.1, .6), xycoords=ax.transAxes,\n",
    "               size = 24)\n",
    "    \n",
    "cmap = sns.cubehelix_palette(light=1, dark = 0.1,\n",
    "                             hue = 0.5, as_cmap=True)\n",
    "\n",
    "sns.set_context(font_scale=2)\n",
    "\n",
    "# Pair grid set up\n",
    "g = sns.PairGrid(X_train)\n",
    "\n",
    "# Scatter plot on the upper triangle\n",
    "g.map_upper(plt.scatter, s=10, color = 'red')\n",
    "\n",
    "# Distribution on the diagonal\n",
    "g.map_diag(sns.distplot, kde=False, color = 'red')\n",
    "\n",
    "# Correlation coefficients on the lower triangle\n",
    "g.map_lower(sns.kdeplot, cmap = cmap)\n",
    "g.map_lower(corrfunc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot = X_train.copy()\n",
    "X_plot['relation_median'] = (X_plot['Grade'] >= 12)\n",
    "X_plot['relation_median'] = X_plot['relation_median'].replace({True: 'above', False: 'below'})\n",
    "X_plot = X_plot.drop(columns='Grade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables by Relation to Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "for i, col in enumerate(X_plot.columns[:-1]):\n",
    "    plt.subplot(3, 2, i + 1)\n",
    "    subset_above = X_plot[X_plot['relation_median'] == 'above']\n",
    "    subset_below = X_plot[X_plot['relation_median'] == 'below']\n",
    "    sns.kdeplot(subset_above[col], label = 'Above Median', color = 'green')\n",
    "    sns.kdeplot(subset_below[col], label = 'Below Median', color = 'red')\n",
    "    plt.legend(); plt.title('Distribution of %s' % col)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "For this regression task, we will use two standard metrics:\n",
    "\n",
    "* Mean Absolute Error (MAE): Average of the absolute value of the difference between predictions and the true values\n",
    "* Root Mean Squared Error (RMSE): The square root of the average of the squared differences between the predictions and the true values.\n",
    "\n",
    "The mean absolute error is more interpretable, but the root mean squared error penalizes larger errors more heavily. Either one may be appropriate depending on the situation. \n",
    "[Here is a discussion](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(predictions, true):\n",
    "    mae = np.mean(abs(predictions - true))\n",
    "    rmse = np.sqrt(np.mean((predictions - true) ** 2))\n",
    "    \n",
    "    return mae, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Baseline\n",
    "\n",
    "For a regression task, a simple naive baseline is to guess the median value on the training set for all testing cases. If our machine learning model cannot better this simple baseline, then perhaps we should try a different approach! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_pred = X_train['Grade'].median()\n",
    "median_preds = [median_pred for _ in range(len(X_test))]\n",
    "true = X_test['Grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_mae, mb_rmse = evaluate_predictions(median_preds, true)\n",
    "print('Median Baseline  MAE: {:.4f}'.format(mb_mae))\n",
    "print('Median Baseline RMSE: {:.4f}'.format(mb_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_train, X_test, y_train, y_test):\n",
    "    # Names of models\n",
    "    model_name_list = ['Linear Regression', 'ElasticNet Regression',\n",
    "                      'Random Forest', 'Extra Trees', 'SVM',\n",
    "                       'Gradient Boosted', 'Baseline']\n",
    "    X_train = X_train.drop(columns='Grade')\n",
    "    X_test = X_test.drop(columns='Grade')\n",
    "    # Instantiate the models\n",
    "    model1 = LinearRegression()\n",
    "    model2 = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "    model3 = RandomForestRegressor(n_estimators=50)\n",
    "    model4 = ExtraTreesRegressor(n_estimators=50)\n",
    "    model5 = SVR(kernel='rbf', degree=3, C=1.0, gamma='auto')\n",
    "    model6 = GradientBoostingRegressor(n_estimators=20)\n",
    "    \n",
    "    # Dataframe for results\n",
    "    results = pd.DataFrame(columns=['mae', 'rmse'], index = model_name_list)\n",
    "    \n",
    "    # Train and predict with each model\n",
    "    for i, model in enumerate([model1, model2, model3, model4, model5, model6]):\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Metrics\n",
    "        mae = np.mean(abs(predictions - y_test))\n",
    "        rmse = np.sqrt(np.mean((predictions - y_test) ** 2))\n",
    "        \n",
    "        # Insert results into the dataframe\n",
    "        model_name = model_name_list[i]\n",
    "        results.ix[model_name, :] = [mae, rmse]\n",
    "    \n",
    "    # Median Value Baseline Metrics\n",
    "    baseline = np.median(y_train)\n",
    "    baseline_mae = np.mean(abs(baseline - y_test))\n",
    "    baseline_rmse = np.sqrt(np.mean((baseline - y_test) ** 2))\n",
    "    \n",
    "    results.ix['Baseline', :] = [baseline_mae, baseline_rmse]\n",
    "    \n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12, 8)\n",
    "matplotlib.rcParams['font.size'] = 16\n",
    "# Root mean squared error\n",
    "ax =  plt.subplot(1, 2, 1)\n",
    "results.sort_values('mae', ascending = True).plot.bar(y = 'mae', color = 'b', ax = ax)\n",
    "plt.title('Model Mean Absolute Error'); plt.ylabel('MAE');\n",
    "\n",
    "# Median absolute percentage error\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "results.sort_values('rmse', ascending = True).plot.bar(y = 'rmse', color = 'r', ax = ax)\n",
    "plt.title('Model Root Mean Squared Error'); plt.ylabel('RMSE');\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The Gradient Boosted regressor is {:0.2f}% better than the baseline.'.format(\n",
    "    (100 * abs(results.loc['Gradient Boosted', 'mae'] - results.loc['Baseline', 'mae'])) / results.loc['Baseline', 'mae']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formula from Ordinary Least Squares Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train.drop(columns='Grade'), y_train)\n",
    "\n",
    "ols_formula = 'Grade = %0.2f +' % lr.intercept_\n",
    "for i, col in enumerate(X_train.columns[1:]):\n",
    "    ols_formula += ' %0.2f * %s +' % (lr.coef_[i], col)\n",
    "    \n",
    "' '.join(ols_formula.split(' ')[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Bayesian Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formula for Bayesian Linear Regression (follows R formula syntax\n",
    "formula = 'Grade ~ ' + ' + '.join(['%s' % variable for variable in X_train.columns[1:]])\n",
    "formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model and Sample from Posterior\n",
    "\n",
    "We now build the model using the formula defined above and Normal Distributions for the model parameters. Then, we let a Markov Chain Monte Carlo algorithm draw samples from the posterior to approximate the posterior for each of the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context for the model\n",
    "with pm.Model() as normal_model:\n",
    "    \n",
    "    # The prior for the model parameters will be a normal distribution\n",
    "    family = pm.glm.families.Normal()\n",
    "    \n",
    "    # Creating the model requires a formula and data (and optionally a family)\n",
    "    pm.GLM.from_formula(formula, data = X_train, family = family)\n",
    "    \n",
    "    # Perform Markov Chain Monte Carlo sampling\n",
    "    normal_trace = pm.sample(draws=2000, chains = 2, tune = 500, njobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Bayesian Linear Regression Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traceplot of All Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the trace with a vertical line at the mean of the trace\n",
    "def plot_trace(trace):\n",
    "    # Traceplot with vertical lines at the mean value\n",
    "    ax = pm.traceplot(trace, figsize=(14, len(trace.varnames)*1.8),\n",
    "                      lines={k: v['mean'] for k, v in pm.df_summary(trace).iterrows()})\n",
    "    \n",
    "    matplotlib.rcParams['font.size'] = 16\n",
    "    \n",
    "    # Labels with the median value\n",
    "    for i, mn in enumerate(pm.df_summary(trace)['mean']):\n",
    "        ax[i, 0].annotate('{:0.2f}'.format(mn), xy = (mn, 0), xycoords = 'data', size = 8,\n",
    "                          xytext = (-18, 18), textcoords = 'offset points', rotation = 90,\n",
    "                          va = 'bottom', fontsize = 'large', color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trace(normal_trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(normal_trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left side of the traceplot is the marginal posterior: the values for the variable are on the x-axis with the probability for the variable (as determined by sampling) on the y-axis. The different colored lines indicate that we performed two chains of Markov Chain Monte Carlo. From the left side we can see that there is a range of values for each weight. The right side shows the different sample values drawn as the sampling process runs. \n",
    "\n",
    "Another method built into PyMC3 for examinig trace results is the forestplot which shows the distribution of each sampled parameter. This allows us to see the uncertainty in each sample. The forestplot is easily constructed from the trace using `pm.forestplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.forestplot(normal_trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the forest plot we can see the most likely value of the parameter (the dot) as well as the 95% confidence interval for the parameter. The `intercept` and `higher_yes` have larger uncertainty compared to the other variables. When we go to make predictions, we can estimate a single number using the most likely value and a range of possible values using all of the samples. \n",
    "\n",
    "Another built in plotting method in PyMC3 is the posterior distribution of all the model parameters. These histograms allow us to see how the model result is a distribution for the parameters rather than a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_posterior(normal_trace, figsize = (14, 14), text_size=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable in normal_trace.varnames:\n",
    "    print('Variable: {:15} Mean weight in model: {:.4f}'.format(variable, \n",
    "                                                                np.mean(normal_trace[variable])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretations of Weights\n",
    "\n",
    "Based on the sign and location of the weights, we can make the following inferences regarding the features in our dataset:\n",
    "\n",
    "* Previous class failures are negatively related to the students final grade\n",
    "* Higher education ambitions are positively related to the students grade\n",
    "* The mother's and father's education levels are positively related to the students final grade\n",
    "* Studying time per week is positively related to the students final grade\n",
    "* Absences are negatively related to the students final grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.df_summary(normal_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Formula from Bayesian Inference using Mean of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_formula = 'Grade = '\n",
    "for variable in normal_trace.varnames:\n",
    "    model_formula += ' %0.2f * %s +' % (np.mean(normal_trace[variable]), variable)\n",
    "\n",
    "' '.join(model_formula.split(' ')[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Bayesian Model Using Mean of Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trace(trace, X_train, X_test, y_train, y_test, model_results):\n",
    "    \n",
    "    # Dictionary of all sampled values for each parameter\n",
    "    var_dict = {}\n",
    "    for variable in trace.varnames:\n",
    "        var_dict[variable] = trace[variable]\n",
    "        \n",
    "    # Results into a dataframe\n",
    "    var_weights = pd.DataFrame(var_dict)\n",
    "    \n",
    "    # Means for all the weights\n",
    "    var_means = var_weights.mean(axis=0)\n",
    "    \n",
    "    # Create an intercept column\n",
    "    X_test['Intercept'] = 1\n",
    "    \n",
    "    # Align names of the test observations and means\n",
    "    names = X_test.columns[1:]\n",
    "    X_test = X_test.ix[:, names]\n",
    "    var_means = var_means[names]\n",
    "    \n",
    "    # Calculate estimate for each test observation using the average weights\n",
    "    results = pd.DataFrame(index = X_test.index, columns = ['estimate'])\n",
    "\n",
    "    for row in X_test.iterrows():\n",
    "        results.ix[row[0], 'estimate'] = np.dot(np.array(var_means), np.array(row[1]))\n",
    "        \n",
    "    # Metrics \n",
    "    actual = np.array(y_test)\n",
    "    errors = results['estimate'] - actual\n",
    "    mae = np.mean(abs(errors))\n",
    "    rmse = np.sqrt(np.mean(errors ** 2))\n",
    "    \n",
    "    print('Model  MAE: {:.4f}\\nModel RMSE: {:.4f}'.format(mae, rmse))\n",
    "    \n",
    "    # Add the results to the comparison dataframe\n",
    "    model_results.ix['Bayesian LR', :] = [mae, rmse]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot median absolute percentage error of all models\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    model_results.sort_values('mae', ascending = True).plot.bar(y = 'mae', color = 'r', ax = ax)\n",
    "    plt.title('Model Mean Absolute Error Comparison'); plt.ylabel('MAE'); \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Plot root mean squared error of all models\n",
    "    ax = plt.subplot(1, 2, 2)\n",
    "    model_results.sort_values('rmse', ascending = True).plot.bar(y = 'rmse', color = 'b', ax = ax)\n",
    "    plt.title('Model RMSE Comparison'); plt.ylabel('RMSE')\n",
    "    \n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_results = evaluate_trace(normal_trace, X_train, X_test, y_train, y_test, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions from Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trace, test_observation):\n",
    "    \n",
    "    print('Test Observation:')\n",
    "    print(test_observation)\n",
    "    var_dict = {}\n",
    "    for variable in trace.varnames:\n",
    "        var_dict[variable] = trace[variable]\n",
    "\n",
    "    # Results into a dataframe\n",
    "    var_weights = pd.DataFrame(var_dict)\n",
    "    sd_value = var_weights['sd'].mean()\n",
    "\n",
    "    # Actual Value\n",
    "    actual = test_observation['Grade']\n",
    "    \n",
    "    # Add in intercept term\n",
    "    test_observation['Intercept'] = 1\n",
    "    test_observation = test_observation.drop('Grade')\n",
    "    \n",
    "    # Align weights and test observation\n",
    "    var_weights = var_weights[test_observation.index]\n",
    "\n",
    "    # Means for all the weights\n",
    "    var_means = var_weights.mean(axis=0)\n",
    "\n",
    "    # Location of mean for observation\n",
    "    mean_loc = np.dot(var_means, test_observation)\n",
    "    \n",
    "    # Estimates of grade\n",
    "    estimates = np.random.normal(loc = mean_loc, scale = sd_value,\n",
    "                                 size = 1000)\n",
    "\n",
    "    plt.figure(figsize(8, 8))\n",
    "    sns.distplot(estimates, hist = True, kde = True, bins = 19,\n",
    "                 hist_kws = {'edgecolor': 'k', 'color': 'darkblue'},\n",
    "                kde_kws = {'linewidth' : 4},\n",
    "                label = 'Estimated Dist.')\n",
    "    plt.vlines(x = actual, ymin = 0, ymax = 5, \n",
    "               linestyles = '--', colors = 'red',\n",
    "               label = 'True Grade',\n",
    "              linewidth = 2.5)\n",
    "    plt.vlines(x = mean_loc, ymin = 0, ymax = 5, \n",
    "               linestyles = '-', colors = 'orange',\n",
    "               label = 'Mean Estimate',\n",
    "              linewidth = 2.5)\n",
    "    \n",
    "    plt.legend(loc = 1)\n",
    "    plt.title('Density Plot for Test Observation');\n",
    "    plt.xlabel('Grade'); plt.ylabel('Density');\n",
    "    \n",
    "    print('True Grade = %d' % actual)\n",
    "    print('Average Estimate = %0.4f' % mean_loc)\n",
    "    print('5%% Estimate = %0.4f    95%% Estimate = %0.4f' % (np.percentile(estimates, 5),\n",
    "                                       np.percentile(estimates, 95)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(normal_trace, X_test.iloc[41])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(normal_trace, X_test.iloc[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model(trace, new_observation):\n",
    "    print('New Observation')\n",
    "    print(new_observation)\n",
    "    # Dictionary of all sampled values for each parameter\n",
    "    var_dict = {}\n",
    "    for variable in trace.varnames:\n",
    "        var_dict[variable] = trace[variable]\n",
    "        \n",
    "    # Standard deviation\n",
    "    sd_value = var_dict['sd'].mean()\n",
    "    \n",
    "    # Results into a dataframe\n",
    "    var_weights = pd.DataFrame(var_dict)\n",
    "    \n",
    "    # Align weights and new observation\n",
    "    var_weights = var_weights[new_observation.index]\n",
    "    # Means of variables\n",
    "    var_means = var_weights.mean(axis=0)\n",
    "    \n",
    "    # Mean for observation\n",
    "    mean_loc = np.dot(var_means, new_observation)\n",
    "    \n",
    "    # Distribution of estimates\n",
    "    estimates = np.random.normal(loc = mean_loc, scale = sd_value,\n",
    "                                 size = 1000)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize(8, 8))\n",
    "    sns.distplot(estimates, hist = True, kde = True, bins = 19,\n",
    "                 hist_kws = {'edgecolor': 'k', 'color': 'darkblue'},\n",
    "                kde_kws = {'linewidth' : 4},\n",
    "                label = 'Estimated Dist.')\n",
    "    plt.vlines(x = mean_loc, ymin = 0, ymax = 5, \n",
    "               linestyles = '-', colors = 'orange', linewidth = 2.5)\n",
    "    plt.title('Density Plot for New Observation');\n",
    "    plt.xlabel('Grade'); plt.ylabel('Density');\n",
    "    \n",
    "    print('Average Estimate = %0.4f' % mean_loc)\n",
    "    print('5%% Estimate = %0.4f    95%% Estimate = %0.4f' % (np.percentile(estimates, 5),\n",
    "                                       np.percentile(estimates, 95)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = pd.Series({'Intercept': 1, 'mother_edu': 4, 'failures': 0, \n",
    "                            'higher_edu': 1, 'studytime': 3,\n",
    "                            'father_edu': 1, 'absences': 1})\n",
    "query_model(normal_trace, observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = pd.Series({'Intercept': 1, 'mother_edu': 2, 'failures': 2, \n",
    "                            'higher_edu': 1, 'studytime': 2,\n",
    "                            'father_edu': 3, 'absences': 4})\n",
    "query_model(normal_trace, observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Variable Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see the effect of changing one variable while holding the others constant, we can use the function `pm.plot_posterior_predictive_glm`. This takes a range of values to use for the variable, a linear model, and a number of samples. The function evaluates the linear model across the range of values for the number of samples. Each time, it draws a different value for the variable from the trace. This gives us an indication of the effect of a single variable and also the uncertainty in the model estimates. To see the effect of a single variable, we hold the others constant at their median values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_effect(query_var, trace, X):\n",
    "    steady_vars = list(X.columns)\n",
    "    steady_vars.remove(query_var)\n",
    "    \n",
    "    def lm(x, samples):\n",
    "        prediction = samples['Intercept'] + samples[query_var] * x\n",
    "        \n",
    "        for var in steady_vars:\n",
    "            # Mutliply the variable by the median value\n",
    "            prediction += samples[var] * X[var].median()\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    figsize(6, 6)\n",
    "    \n",
    "    # Find the minimum and maximum values for the range\n",
    "    var_min = X[query_var].min()\n",
    "    var_max = X[query_var].max()\n",
    "    \n",
    "    pm.plot_posterior_predictive_glm(trace, eval=np.linspace(var_min, var_max, 100), \n",
    "                                     lm=lm, samples=100, color='blue', alpha = 0.4, lw = 2)\n",
    "    \n",
    "    plt.xlabel('%s' % query_var, size = 16)\n",
    "    plt.ylabel('Grade', size = 16)\n",
    "    plt.title(\"Posterior of Grade vs %s\" % query_var, size = 18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_effect('mother_edu', normal_trace, X_train.drop(columns='Grade'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_effect('studytime', normal_trace, X_train.drop(columns='Grade'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_effect('absences', normal_trace, X_train.drop(columns='Grade'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_effect('failures', normal_trace, X_train.drop(columns='Grade'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_effect('father_edu', normal_trace, X_train.drop(columns='Grade'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Prior Distribution\n",
    "\n",
    "We can perform the exact same Bayesian Linear Modeling using a Student's T-distribution as the prior family for our model parameters. A Student's T Distribution has more weight in the tails of the distribution so it is more robust to outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X values for plotting\n",
    "x = np.linspace(-5, 5, num = 500)\n",
    "\n",
    "# Generate pdf of normal distribution\n",
    "y_norm = scipy.stats.norm.pdf(x)\n",
    "\n",
    "# PDF of t-distribution with 2 degrees of freedom\n",
    "y_t = scipy.stats.t.pdf(x, df = 2)\n",
    "\n",
    "plt.plot(x, y_norm, 'b-', label = 'Normal')\n",
    "plt.plot(x, y_t, 'r-', label = 'T with 2 df')\n",
    "plt.legend(prop = {'size': 18}, loc = 1)\n",
    "plt.xlabel('x'); plt.ylabel('Probability'); plt.title('Normal vs T Distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context for model\n",
    "with pm.Model() as t_model:\n",
    "    # Family is Student's T in this case\n",
    "    family = pm.glm.families.StudentT(df = 2)\n",
    "    \n",
    "    # Formula, data, family\n",
    "    pm.GLM.from_formula(formula, data = X_train, family = family)\n",
    "    \n",
    "    # Sample from the posterior \n",
    "    t_trace = pm.sample(draws=2000, tune=500, njobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trace(t_trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_t(trace, test_observation):\n",
    "    \n",
    "    var_dict = {}\n",
    "    for variable in trace.varnames:\n",
    "        var_dict[variable] = trace[variable]\n",
    "\n",
    "    # Results into a dataframe\n",
    "    var_weights = pd.DataFrame(var_dict)\n",
    "\n",
    "    # Actual Value\n",
    "    actual = test_observation['Grade']\n",
    "    \n",
    "    # Add in intercept term\n",
    "    test_observation['Intercept'] = 1\n",
    "    test_observation = test_observation.drop('Grade')\n",
    "    \n",
    "    # Align weights and test observation\n",
    "    var_weights = var_weights[test_observation.index]\n",
    "\n",
    "    # Means for all the weights\n",
    "    var_means = var_weights.mean(axis=0)\n",
    "\n",
    "    # Location of mean for observation\n",
    "    mean_loc = np.dot(var_means, test_observation)\n",
    "    \n",
    "    # Estimates of grade\n",
    "    estimates = mean_loc + np.random.standard_t(df = 2, size = 1000)\n",
    "\n",
    "    plt.figure(figsize(8, 8))\n",
    "    sns.distplot(estimates, hist = True, kde = True, bins = 19,\n",
    "                 hist_kws = {'edgecolor': 'k', 'color': 'darkblue'},\n",
    "                kde_kws = {'linewidth' : 4},\n",
    "                label = 'Estimated Dist.')\n",
    "    plt.vlines(x = actual, ymin = 0, ymax = 5, \n",
    "               linestyles = '--', colors = 'red',\n",
    "               label = 'True Grade',\n",
    "              linewidth = 2.5)\n",
    "    plt.vlines(x = mean_loc, ymin = 0, ymax = 5, \n",
    "               linestyles = '-', colors = 'orange',\n",
    "               label = 'Mean Estimate',\n",
    "              linewidth = 2.5)\n",
    "    \n",
    "    plt.legend(loc = 1)\n",
    "    plt.title('Density Plot for Test Observation');\n",
    "    plt.xlabel('Grade'); plt.ylabel('Density');\n",
    "    \n",
    "    print('True Grade = %d' % actual)\n",
    "    print('Average Estimate = %0.4f' % mean_loc)\n",
    "    print('5%% Estimate = %0.4f    95%% Estimate = %0.4f' % (np.percentile(estimates, 5),\n",
    "                                       np.percentile(estimates, 95)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_t(t_trace, X_test.iloc[60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(normal_trace, X_test.iloc[60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model_t(trace, new_observation):\n",
    "    \n",
    "    # Dictionary of all sampled values for each parameter\n",
    "    var_dict = {}\n",
    "    for variable in trace.varnames:\n",
    "        var_dict[variable] = trace[variable]\n",
    "        \n",
    "\n",
    "    \n",
    "    # Results into a dataframe\n",
    "    var_weights = pd.DataFrame(var_dict)\n",
    "    \n",
    "    # Align weights and new observation\n",
    "    var_weights = var_weights[new_observation.index]\n",
    "    # Means of variables\n",
    "    var_means = var_weights.mean(axis=0)\n",
    "    \n",
    "    # Mean for observation\n",
    "    mean_loc = np.dot(var_means, new_observation)\n",
    "    \n",
    "    # Distribution of estimates\n",
    "    estimates = mean_loc +  np.random.standard_t(df = 2, size = 1000)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize(8, 8))\n",
    "    sns.distplot(estimates, hist = True, kde = True, bins = 19,\n",
    "                 hist_kws = {'edgecolor': 'k', 'color': 'darkblue'},\n",
    "                kde_kws = {'linewidth' : 4},\n",
    "                label = 'Estimated Dist.')\n",
    "    plt.vlines(x = mean_loc, ymin = 0, ymax = 5, \n",
    "               linestyles = '-', colors = 'orange', linewidth = 2.5)\n",
    "    plt.title('Density Plot for New Observation');\n",
    "    plt.xlabel('Grade'); plt.ylabel('Density');\n",
    "    \n",
    "    print('Average Estimate = %0.4f' % mean_loc)\n",
    "    print('5%% Estimate = %0.4f    95%% Estimate = %0.4f' % (np.percentile(estimates, 5),\n",
    "                                       np.percentile(estimates, 95)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = pd.Series({'Intercept': 1, 'mother_edu': 4, 'failures': 0, \n",
    "                            'higher_edu': 1, 'studytime': 3,\n",
    "                            'father_edu': 1, 'absences': 1})\n",
    "query_model(normal_trace, observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = pd.Series({'Intercept': 1, 'mother_edu': 4, 'failures': 0, \n",
    "                            'higher_edu': 1, 'studytime': 3,\n",
    "                            'father_edu': 1, 'absences': 1})\n",
    "query_model_t(t_trace, observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some variation between the t-distribution estimates and those from the normal distribution as prior model. We can try other types of prior distributions as well, but for now I will halt with the normal and the t-distribution. Choosing appropriate priors is one of the hardest aspect of Bayesian Modeling, but we can get around that by having more data. As the amount of data the model learns from increases, the prior has less of an effect because each time the posterior is updated based on the new data. Essentially machine learning models perform inference with no priors, basing the final model entirely on the data. In the case of limited samples, Bayesian Inference can be a better method for building models because it provides a reasonable estimate in situations with few data points (as long as the prior is reasonable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook we looked at using Bayesian Linear Regression to predict student performance based on five factors. Rather than specify probabilities for the Bayesian network which is basically impossible for continuous variables, we framed the problem as a machine learning task. In addition to the standard machine learning models that learn from observations, we also used Bayesian Linear Regression to create a model mapping the features (student characteristics) to the targets (final grade). The advantages of Bayesian Linear Regression are that if we use sensible priors, we can still get a decent estimate with few samples, and the final weights are not a single number, but a distribution componsed of every sample drawn during the sampling run. We can then make predictions using all the sampled weights to form a distribution of expected values rather than a single answer. \n",
    "\n",
    "The Bayesian  Linear Regression did not perform as well as the other methods in terms of the two metrics we choose. This might not be the ideal case for a Bayesian inference approach but we saw that Bayesian Linear Regression produced intuitive estimates for the model weights and gave predictions for new students that align with our expectations for the factors influencing student performance. To summarize, although Bayesian Linear Regression did not outperform the standard machine learning methods, it gave us a chance to learn another tool for use in evaluation and making sense of data. There are likely situations in which Bayesian inference is better suited than standard machine learning, and it is best to be prepared for those situations when we find them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
