[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["# Introduction: Testing Cyclical Encoding of Features for Machine Learning"]}, {"block": 1, "type": "code", "linesLength": 4, "startIndex": 1, "lines": ["import pandas as pd\n", "import numpy as np\n", "\n", "import glob"]}, {"block": 2, "type": "code", "linesLength": 2, "startIndex": 5, "lines": ["building_data_files = glob.glob('data/building*')\n", "len(building_data_files)"]}, {"block": 3, "type": "code", "linesLength": 3, "startIndex": 7, "lines": ["data = pd.read_csv(building_data_files[10], parse_dates=['timestamp'], index_col=0).set_index('timestamp')\n", "data.head()\n", "data.info()"]}, {"block": 4, "type": "code", "linesLength": 37, "startIndex": 10, "lines": ["from sklearn.base import BaseEstimator, TransformerMixin\n", "\n", "\n", "class DateTimeFeatures(BaseEstimator, TransformerMixin):\n", "    def __init__(self):\n", "        pass\n", "\n", "    def fit(self, X, y=None):\n", "        return self\n", "\n", "    def transform(self, X, y=None):\n", "        field = X.index\n", "        X[\"time_of_day\"] = field.hour + field.minute / 60\n", "        X[\"day_of_year\"] = field.dayofyear\n", "        return X\n", "\n", "\n", "class CyclicalDateTimeFeatures(BaseEstimator, TransformerMixin):\n", "    def __init__(self):\n", "        pass\n", "\n", "    def fit(self, X, y=None):\n", "        return self\n", "\n", "    def transform(self, X, y=None):\n", "        X[\"sin_time_of_day\"], X[\"cos_time_of_day\"] = _cyclical_encoding(\n", "            X[\"time_of_day\"], period=24\n", "        )\n", "        X[\"sin_day_of_year\"], X[\"cos_day_of_year\"] = _cyclical_encoding(\n", "            X[\"day_of_year\"], period=366\n", "        )\n", "        return X\n", "\n", "\n", "def _cyclical_encoding(series, period):\n", "    base = 2 * np.pi * series / period\n", "    return np.sin(base), np.cos(base)"]}, {"block": 5, "type": "code", "linesLength": 11, "startIndex": 47, "lines": ["from sklearn.pipeline import Pipeline\n", "\n", "transforms = Pipeline(\n", "    steps=[\n", "        (\"date_time_features\", DateTimeFeatures()),\n", "        (\"cylical_date_time_features\", CyclicalDateTimeFeatures()),\n", "    ]\n", ")\n", "\n", "transformed_data = transforms.transform(data)\n", "transformed_data.head()"]}, {"block": 6, "type": "code", "linesLength": 10, "startIndex": 58, "lines": ["class WeeklyValidator(BaseEstimator):\n", "    def __init__(self):\n", "        pass\n", "    \n", "    def fit(self, X, y):\n", "        pass\n", "    \n", "    def predict(self, X):\n", "        pass\n", "    "]}, {"block": 7, "type": "code", "linesLength": 46, "startIndex": 68, "lines": ["def run_weekly_validation(models, data):\n", "    \n", "    all_predictions = []\n", "    \n", "    feature_sets = [['time_of_day', 'day_of_year', 'temperature'], \n", "                    ['sin_time_of_day', 'cos_time_of_day', 'sin_day_of_year', 'cos_day_of_year', 'temperature']]\n", "    # Iterate through features\n", "    for feature_set in feature_sets:\n", "        features='standard' if 'sin_time_of_day' not in feature_set else 'cyclical'\n", "        print(f'Using features: {features}')\n", "        # Subset to data\n", "        X = data[feature_set + ['energy']].copy()\n", "        \n", "        # Iterate through models\n", "        for model in models:\n", "            model_name = model.__class__.__name__\n", "            print(f'Using model: {model_name}')\n", "            \n", "            # Iterate through weeks in the dataset\n", "            # Must group by string formatted week and year\n", "            for (week, year), X_test in tqdm.tqdm(X.groupby([X.index.strftime('%U'), X.index.strftime('%Y')]), desc='Weeks'):\n", "                \n", "                # Subset to training data\n", "                X_train = X[X.index < X_test.index.min()].copy()\n", "                \n", "                # Can not train or test on zero observations\n", "                if len(X_train) == 0 or len(X_test) == 0:\n", "                    continue\n", "                    \n", "                # Targets\n", "                y_train = X_train.pop('energy')\n", "                y_test = X_test.pop('energy')\n", "                \n", "                model.fit(X_train, y_train)\n", "                predictions = model.predict(X_test)\n", "                \n", "                # Record predictions along with actual values, model, and feature set in a dataframe\n", "                predictions = pd.DataFrame(dict(predicted=predictions,\n", "                                                actual=y_test, \n", "                                                model=model_name, \n", "                                                features=features),\n", "                                           index=X_test.index)\n", "                \n", "                all_predictions.append(predictions)\n", "    # Return list of dataframes\n", "    return all_predictions"]}, {"block": 8, "type": "code", "linesLength": 2, "startIndex": 114, "lines": ["import tqdm\n", "import black"]}, {"block": 9, "type": "code", "linesLength": 6, "startIndex": 116, "lines": ["from sklearn.linear_model import LinearRegression\n", "from sklearn.ensemble import RandomForestRegressor\n", "\n", "# Create linear model and random forest model for regression\n", "models = [LinearRegression(n_jobs=-1), RandomForestRegressor(n_estimators=100, max_depth=None, n_jobs=-1, random_state=100)]\n", "# validation = run_weekly_validation(models, data)"]}, {"block": 10, "type": "code", "linesLength": 15, "startIndex": 122, "lines": ["def run_all_buildings(building_data_files):\n", "    # Run validation for all buildings\n", "    for building_file_name in tqdm.tqdm(building_data_files, desc='Buildings'):\n", "        building_data = pd.read_csv(building_file_name, parse_dates=['timestamp']).set_index('timestamp')\n", "        # Create sets of features\n", "        building_data = transforms.transform(building_data)\n", "        \n", "        # Run the validation and save the results\n", "        building_validation = run_weekly_validation(models, building_data)\n", "        # Convert from list of dataframes to single dataframe\n", "        building_validation = pd.concat(building_validation).reset_index().sort_values(['model', 'features', 'timestamp']).set_index('timestamp')\n", "        # Save off results for analysis\n", "        building_validation.to_csv(f\"{building_file_name.replace('energy_data', 'validation_results').replace('data', 'validation_results')}\")\n", "                                   \n", "run_all_buildings(building_data_files)"]}, {"block": 11, "type": "code", "linesLength": 3, "startIndex": 137, "lines": ["validation = pd.concat(validation)\n", "validation.head()\n", "validation.tail()"]}, {"block": 12, "type": "code", "linesLength": 2, "startIndex": 140, "lines": ["validation['model'].unique()\n", "validation['features'].unique()"]}, {"block": 13, "type": "code", "linesLength": 2, "startIndex": 142, "lines": ["from IPython.core.interactiveshell import InteractiveShell\n", "InteractiveShell.ast_node_interactivity = 'all'"]}, {"block": 14, "type": "code", "linesLength": 1, "startIndex": 144, "lines": ["validation['model'].unique()"]}, {"block": 15, "type": "code", "linesLength": 2, "startIndex": 145, "lines": ["def calculate_results(results):\n", "    pass"]}, {"block": 16, "type": "code", "linesLength": 2, "startIndex": 147, "lines": ["def graph_results(results):\n", "    pass"]}, {"block": 17, "type": "code", "linesLength": 0, "startIndex": 149, "lines": []}]