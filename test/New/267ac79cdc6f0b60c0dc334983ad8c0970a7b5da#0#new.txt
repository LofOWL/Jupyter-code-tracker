[{"block": 0, "type": "markdown", "linesLength": 2, "startIndex": 0, "lines": ["<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n", "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction:-Stats-for-Medium-Articles\" data-toc-modified-id=\"Introduction:-Stats-for-Medium-Articles-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction: Stats for Medium Articles</a></span></li><li><span><a href=\"#Metadata-for-Article\" data-toc-modified-id=\"Metadata-for-Article-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Metadata for Article</a></span><ul class=\"toc-item\"><li><span><a href=\"#Published-and-Begun-Times\" data-toc-modified-id=\"Published-and-Begun-Times-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Published and Begun Times</a></span></li><li><span><a href=\"#Reading-Time\" data-toc-modified-id=\"Reading-Time-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Reading Time</a></span></li><li><span><a href=\"#Views,-Reads,-Reading-Ratio,-Number-of-Fans\" data-toc-modified-id=\"Views,-Reads,-Reading-Ratio,-Number-of-Fans-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Views, Reads, Reading Ratio, Number of Fans</a></span></li><li><span><a href=\"#Publication\" data-toc-modified-id=\"Publication-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Publication</a></span></li><li><span><a href=\"#Getting-Article-Link\" data-toc-modified-id=\"Getting-Article-Link-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Getting Article Link</a></span></li><li><span><a href=\"#Retrieving-Article\" data-toc-modified-id=\"Retrieving-Article-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Retrieving Article</a></span></li></ul></li><li><span><a href=\"#Article-Data\" data-toc-modified-id=\"Article-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Article Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Article-Title-and-Number-of-Words\" data-toc-modified-id=\"Article-Title-and-Number-of-Words-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Article Title and Number of Words</a></span></li><li><span><a href=\"#Number-of-Claps\" data-toc-modified-id=\"Number-of-Claps-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Number of Claps</a></span></li><li><span><a href=\"#Tags\" data-toc-modified-id=\"Tags-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Tags</a></span></li><li><span><a href=\"#Number-of-Responses\" data-toc-modified-id=\"Number-of-Responses-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Number of Responses</a></span></li><li><span><a href=\"#Convert-to-DataFrame\" data-toc-modified-id=\"Convert-to-DataFrame-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Convert to DataFrame</a></span></li><li><span><a href=\"#Tags-to-Columns\" data-toc-modified-id=\"Tags-to-Columns-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Tags to Columns</a></span></li></ul></li><li><span><a href=\"#Function-to-Process-Single-Entry\" data-toc-modified-id=\"Function-to-Process-Single-Entry-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Function to Process Single Entry</a></span><ul class=\"toc-item\"><li><span><a href=\"#Process-Sequentially\" data-toc-modified-id=\"Process-Sequentially-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Process Sequentially</a></span></li></ul></li><li><span><a href=\"#Process-in-Parallel\" data-toc-modified-id=\"Process-in-Parallel-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Process in Parallel</a></span><ul class=\"toc-item\"><li><span><a href=\"#Function-to-Process-Articles-in-Parallel\" data-toc-modified-id=\"Function-to-Process-Articles-in-Parallel-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Function to Process Articles in Parallel</a></span></li></ul></li><li><span><a href=\"#Analysis\" data-toc-modified-id=\"Analysis-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Correlation-Heatmap\" data-toc-modified-id=\"Correlation-Heatmap-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Correlation Heatmap</a></span></li><li><span><a href=\"#Scatterplot-Matrix\" data-toc-modified-id=\"Scatterplot-Matrix-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Scatterplot Matrix</a></span></li></ul></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Conclusions</a></span></li></ul></div>"]}, {"block": 1, "type": "markdown", "linesLength": 5, "startIndex": 2, "lines": ["# Introduction: Stats for Medium Articles\n", "\n", "In this notebook, we will explore my medium article statistics. We'll work with the raw HTML of the stats page. To apply to your own medium data, go to the stats page, make sure to scroll all the way down to the bottom so all the articles are loaded, right click, and hit 'save as'. Save the file as `stats.html` in the `data/` directory. You can also save the responses to do a similar analysis.\n", "\n", "![](images/stats-saving-medium.gif)"]}, {"block": 2, "type": "markdown", "linesLength": 3, "startIndex": 7, "lines": ["We'll use the information on this page. \n", "\n", "![](images/stat_graph.png)"]}, {"block": 3, "type": "code", "linesLength": 27, "startIndex": 10, "lines": ["# Data science imports\n", "import pandas as pd\n", "import numpy as np\n", "\n", "# Options for pandas\n", "pd.options.display.max_columns = 25\n", "\n", "# Display all cell outputs\n", "from IPython.core.interactiveshell import InteractiveShell\n", "InteractiveShell.ast_node_interactivity = 'all'\n", "\n", "# Interactive plotting\n", "import plotly.plotly as py\n", "import plotly.graph_objs as go\n", "import cufflinks\n", "cufflinks.go_offline()\n", "\n", "# Parsing articles\n", "from bs4 import BeautifulSoup\n", "\n", "# Utilities\n", "from collections import Counter, defaultdict\n", "from itertools import chain\n", "import re\n", "\n", "# Getting webpages\n", "import requests"]}, {"block": 4, "type": "markdown", "linesLength": 1, "startIndex": 37, "lines": ["Make sure to scroll all the way down on the stats page and then save."]}, {"block": 5, "type": "code", "linesLength": 2, "startIndex": 38, "lines": ["soup = BeautifulSoup(open('data/stats.html', 'r'), 'html.parser')\n", "soup.text[:100]"]}, {"block": 6, "type": "code", "linesLength": 2, "startIndex": 40, "lines": ["res_soup = BeautifulSoup(open('data/stats-responses.html', 'r'), 'html.parser')\n", "res_soup.text[:100]"]}, {"block": 7, "type": "code", "linesLength": 2, "startIndex": 42, "lines": ["table_rows = soup.find_all(attrs={'class':\"sortableTable-row js-statsTableRow\"})\n", "print(f'Found {len(table_rows)} articles.')"]}, {"block": 8, "type": "code", "linesLength": 2, "startIndex": 44, "lines": ["res_table_rows = res_soup.find_all(attrs={'class':\"sortableTable-row js-statsTableRow\"})\n", "print(f'Found {len(res_table_rows)} responses.')"]}, {"block": 9, "type": "markdown", "linesLength": 3, "startIndex": 46, "lines": ["# Metadata for Article\n", "\n", "We'll scrape the article listing in this table to retrieve the metadata. Later, we'll go to the article itself for additional data."]}, {"block": 10, "type": "markdown", "linesLength": 1, "startIndex": 49, "lines": ["![](images/table.png)"]}, {"block": 11, "type": "code", "linesLength": 2, "startIndex": 50, "lines": ["entry = table_rows[1]\n", "entry"]}, {"block": 12, "type": "markdown", "linesLength": 1, "startIndex": 52, "lines": ["## Published and Begun Times"]}, {"block": 13, "type": "code", "linesLength": 1, "startIndex": 53, "lines": ["entry.get('data-timestamp')"]}, {"block": 14, "type": "code", "linesLength": 4, "startIndex": 54, "lines": ["def convert_timestamp(ts: int, tz: str):\n", "    return pd.to_datetime(ts, origin='unix', unit='ms').tz_localize('UTC').tz_convert(tz)\n", "\n", "convert_timestamp(entry.get('data-timestamp'), 'America/New_York')"]}, {"block": 15, "type": "markdown", "linesLength": 3, "startIndex": 58, "lines": ["This is the time at which the article was started. \n", "\n", "The time at which the article was published is also in the data."]}, {"block": 16, "type": "code", "linesLength": 5, "startIndex": 61, "lines": ["published_time = entry.find_all(attrs={'class': 'sortableTable-value'})[0].text\n", "\n", "published_time\n", "\n", "convert_timestamp(published_time, 'America/Chicago')"]}, {"block": 17, "type": "markdown", "linesLength": 1, "startIndex": 66, "lines": ["## Reading Time"]}, {"block": 18, "type": "code", "linesLength": 2, "startIndex": 67, "lines": ["read_time = int(entry.find_all(attrs={'class':'readingTime'})[0].get('title').split(' ')[0])\n", "read_time"]}, {"block": 19, "type": "markdown", "linesLength": 1, "startIndex": 69, "lines": ["We can get more metadata by searcing the table about the entry."]}, {"block": 20, "type": "code", "linesLength": 2, "startIndex": 70, "lines": ["for i in entry.find_all(attrs={'class':'sortableTable-value'}):\n", "    print(i)"]}, {"block": 21, "type": "markdown", "linesLength": 1, "startIndex": 72, "lines": ["The first piece of information is the published timestamp but the labels for the others are hidden."]}, {"block": 22, "type": "code", "linesLength": 2, "startIndex": 73, "lines": ["for i in entry.find_all(attrs={'class':'u-sm-show'}):\n", "    print(i) "]}, {"block": 23, "type": "markdown", "linesLength": 3, "startIndex": 75, "lines": ["## Views, Reads, Reading Ratio, Number of Fans\n", "\n", "Let's quickly iterate through these entries to get the information. This assumes the order of the information is the same (which might need to be updated if Medium updates the stats page)."]}, {"block": 24, "type": "code", "linesLength": 11, "startIndex": 78, "lines": ["entry_dict = {}\n", "for value, key in zip(entry.find_all(attrs={'class':'sortableTable-value'}),\n", "            ['published_timestamp', 'views', 'reads', 'ratio', 'fans']):\n", "    entry_dict[key] = float(value.text) if key == 'ratio' else int(value.text)\n", "    \n", "entry_dict['published_timestamp'] = convert_timestamp(entry_dict['published_timestamp'], 'America/Chicago')\n", "entry_dict['started_timestamp'] = convert_timestamp(entry.get('data-timestamp'), 'America/Chicago')\n", "entry_dict['read_time'] = int(entry.find_all(attrs={'class':'readingTime'})[0].get('title').split(' ')[0])\n", "\n", "from pprint import pprint\n", "pprint(entry_dict)"]}, {"block": 25, "type": "markdown", "linesLength": 3, "startIndex": 89, "lines": ["## Publication\n", "\n", "To figure out the publication - if there is one - we use the following code."]}, {"block": 26, "type": "code", "linesLength": 1, "startIndex": 92, "lines": ["entry=table_rows[0]"]}, {"block": 27, "type": "code", "linesLength": 7, "startIndex": 93, "lines": ["publication = entry.find_all(attrs={'class': 'sortableTable-text'})\n", "if 'In' in publication[0].text:\n", "    publication = publication[0].text.split('In ')[1].split('View')[0]\n", "else:\n", "    publication = 'None'\n", "    \n", "publication"]}, {"block": 28, "type": "markdown", "linesLength": 1, "startIndex": 100, "lines": ["This seems a little overcomplicated, but it gets the job done. If there is a better way, I'd enjoy hearing it! "]}, {"block": 29, "type": "markdown", "linesLength": 1, "startIndex": 101, "lines": ["## Getting Article Link"]}, {"block": 30, "type": "markdown", "linesLength": 1, "startIndex": 102, "lines": ["We'll want to store the article link so we can go to it and find more information."]}, {"block": 31, "type": "code", "linesLength": 2, "startIndex": 103, "lines": ["link = entry.find_all(text='View story', attrs={'class': 'sortableTable-link'})[0].get('href')\n", "link"]}, {"block": 32, "type": "markdown", "linesLength": 3, "startIndex": 105, "lines": ["## Retrieving Article\n", "\n", "To get the article itself, we use the amazing `requests` library."]}, {"block": 33, "type": "code", "linesLength": 2, "startIndex": 108, "lines": ["r = requests.get(link)\n", "type(r)"]}, {"block": 34, "type": "code", "linesLength": 1, "startIndex": 110, "lines": ["len(r.content)"]}, {"block": 35, "type": "markdown", "linesLength": 1, "startIndex": 111, "lines": ["# Article Data"]}, {"block": 36, "type": "markdown", "linesLength": 1, "startIndex": 112, "lines": ["We can go to the article itself to extract more information, such as the number of words, the number of claps, and the tags."]}, {"block": 37, "type": "markdown", "linesLength": 1, "startIndex": 113, "lines": ["## Article Title and Number of Words"]}, {"block": 38, "type": "code", "linesLength": 21, "startIndex": 114, "lines": ["# Retrieve the article and create a soup\n", "entry_content = requests.get(link).content\n", "entry_soup = BeautifulSoup(entry_content)\n", "\n", "title = entry_soup.h1.text\n", "\n", "# Main text entries\n", "entry_text = [p.text for p in entry_soup.find_all(['h1', 'h2', 'h3', 'p', 'blockquote'])]\n", "\n", "# Make sure to catch everything\n", "entry_text.extend(s.text for s in entry_soup.find_all(attrs={'class':'graf graf--li graf-after--li'}))\n", "entry_text.extend(s.text for s in entry_soup.find_all(attrs={'class': 'graf graf--li graf-after--p'}))\n", "entry_text.extend(s.text for s in entry_soup.find_all(attrs={'class': 'graf graf--li graf-after--blockquote'}))\n", "entry_text.extend(s.text for s in entry_soup.find_all(attrs={'class': 'graf graf--li graf-after--pullquote'}))\n", "\n", "entry_text = ' '.join(entry_text)\n", "\n", "# Word count\n", "word_count = len(re.findall(r\"[\\w']+|[.,!?;]\", entry_text))\n", "\n", "print(f'\"{title}\" has {word_count} words with a read time of {entry_dict[\"read_time\"]} minutes.')"]}, {"block": 39, "type": "markdown", "linesLength": 1, "startIndex": 135, "lines": ["## Number of Claps"]}, {"block": 40, "type": "code", "linesLength": 13, "startIndex": 136, "lines": ["# Number of claps\n", "clap_pattern = re.compile('^[0-9]{1,} claps|^[0-9]{1,}.[0-9]{1,}K claps|^[0-9]{1,}K claps')\n", "claps = entry_soup.find_all(text = clap_pattern)\n", "\n", "if len(claps) > 0:\n", "    if 'K' in claps[0]:\n", "        clap_number = int(1e3 * float(claps[0].split('K')[0]))\n", "    else:\n", "        clap_number = int(claps[0].split(' ')[0])\n", "else:\n", "    clap_number = 0\n", "\n", "print(f'\"{title}\" has {clap_number} claps from {entry_dict[\"fans\"]} fans.')"]}, {"block": 41, "type": "markdown", "linesLength": 1, "startIndex": 149, "lines": ["## Tags"]}, {"block": 42, "type": "code", "linesLength": 4, "startIndex": 150, "lines": ["# Post tags\n", "tags = entry_soup.find_all(attrs={'class': 'tags tags--postTags tags--borderless'})\n", "tags = [li.text for li in tags[0].find_all('li')]\n", "tags"]}, {"block": 43, "type": "markdown", "linesLength": 1, "startIndex": 154, "lines": ["## Number of Responses"]}, {"block": 44, "type": "code", "linesLength": 4, "startIndex": 155, "lines": ["responses = entry_soup.find_all(attrs={'class':  'button button--chromeless u-baseColor--buttonNormal u-marginRight12',\n", "                           'data-action': 'scroll-to-responses'})\n", "num_responses = int(responses[0].text) if len(responses) > 0 else 0\n", "num_responses"]}, {"block": 45, "type": "markdown", "linesLength": 1, "startIndex": 159, "lines": ["## Convert to DataFrame"]}, {"block": 46, "type": "markdown", "linesLength": 1, "startIndex": 160, "lines": ["First we'll put all of the information in a dictionary, and then convert to a dataframe. This allows for ease of analysis and visualization."]}, {"block": 47, "type": "code", "linesLength": 8, "startIndex": 161, "lines": ["# Store in dictionary with title as key\n", "entry_dict['title'] = title\n", "entry_dict['publication'] = publication\n", "entry_dict['text'] = entry_text\n", "entry_dict['word_count'] = word_count\n", "entry_dict['claps'] = clap_number\n", "entry_dict['tags'] = tags\n", "entry_dict['num_responses'] = num_responses"]}, {"block": 48, "type": "code", "linesLength": 2, "startIndex": 169, "lines": ["df = pd.DataFrame([entry_dict])\n", "df.head()"]}, {"block": 49, "type": "code", "linesLength": 5, "startIndex": 171, "lines": ["# Add extra columns with more data\n", "df['claps_per_word'] = df['claps'] / df['word_count']\n", "df['edit_days'] = (df['published_timestamp'] - df['started_timestamp']).dt.total_seconds() / (60 * 60 * 24)\n", "\n", "df.head()"]}, {"block": 50, "type": "markdown", "linesLength": 3, "startIndex": 176, "lines": ["## Tags to Columns\n", "\n", "Next we'll one-hot-encode the five most popular tags. Each tag now becomes one column, where the value is 1 if the story has a tag and 0 otherwise (this will be relevant when we have multiple articles with different tags)."]}, {"block": 51, "type": "code", "linesLength": 11, "startIndex": 179, "lines": ["# Add 5 most common tags with flag column if data has it\n", "n = 5\n", "all_tags = list(chain(*df['tags'].tolist()))\n", "tag_counts = Counter(all_tags)\n", "tags = tag_counts.most_common(n)\n", "\n", "for tag, count in tags:\n", "    flag = [1 if tag in tags else 0 for tags in df['tags']]\n", "    df.loc[:, f'<tag>{tag}'] = flag\n", "    \n", "df.head()"]}, {"block": 52, "type": "markdown", "linesLength": 3, "startIndex": 190, "lines": ["# Function to Process Single Entry\n", "\n", "We'll encapsulate all of the steps in a single function. This function is designed to be run in parallel, but we can also run it on a single entry."]}, {"block": 53, "type": "code", "linesLength": 109, "startIndex": 193, "lines": ["def process_entry(entry, parallel=True, tz='America/Chicago'):\n", "    \"\"\"\n", "    Extract data from one entry in table\n", "\n", "    :param entry: BeautifulSoup tag\n", "    :param parallel: Boolean for whether function is being run in parallel\n", "    :param tz: string representing timezone for started and published time\n", "\n", "    :return entry_dict: dictionary with data about entry\n", "\n", "    \"\"\"\n", "    # Convert to soup when running in parallel\n", "    if parallel:\n", "        entry = BeautifulSoup(entry, features='lxml').body.tr\n", "\n", "    entry_dict = {}\n", "    # Extract information\n", "    for value, key in zip(entry.find_all(attrs={'class': 'sortableTable-value'}),\n", "                          ['published_date', 'views', 'reads', 'ratio', 'fans']):\n", "        entry_dict[key] = float(\n", "            value.text) if key == 'ratio' else int(value.text)\n", "\n", "    entry_dict['read_time'] = int(entry.find_all(attrs={'class': 'readingTime'})[\n", "                                  0].get('title').split(' ')[0])\n", "\n", "    # Unlisted vs published\n", "    entry_dict['type'] = 'unlisted' if len(\n", "        entry.find_all(text=' Unlisted')) > 0 else 'published'\n", "\n", "    # Publication \n", "    \n", "    publication = entry.find_all(attrs={'class': 'sortableTable-text'})\n", "    if 'In' in publication[0].text:\n", "        entry_dict['publication'] = publication[0].text.split('In ')[1].split('View')[0]\n", "    else:\n", "        entry_dict['publication'] = 'None'\n", "    \n", "\n", "    # Convert datetimes\n", "    entry_dict['published_date'] = convert_timestamp(\n", "        entry_dict['published_date'], tz=tz)\n", "    entry_dict['started_date'] = convert_timestamp(\n", "        entry.get('data-timestamp'), tz=tz)\n", "\n", "    # Get the link\n", "    link = entry.find_all(text='View story',\n", "                               attrs={'class': 'sortableTable-link'})[0].get('href')\n", "\n", "    # Retrieve the article and create a soup\n", "    entry = requests.get(link).content\n", "    entry_soup = BeautifulSoup(entry, features='lxml')\n", "\n", "    # Get the title\n", "    try:\n", "        title = entry_soup.h1.text\n", "    except:\n", "        title = 'response'\n", "\n", "    # Main text entries\n", "    entry_text = [p.text for p in entry_soup.find_all(\n", "        ['h1', 'h2', 'h3', 'p', 'blockquote'])]\n", "\n", "    # Make sure to catch everything\n", "    entry_text.extend(s.text for s in entry_soup.find_all(\n", "        attrs={'class': 'graf graf--li graf-after--li'}))\n", "    entry_text.extend(s.text for s in entry_soup.find_all(\n", "        attrs={'class': 'graf graf--li graf-after--p'}))\n", "    entry_text.extend(s.text for s in entry_soup.find_all(\n", "        attrs={'class': 'graf graf--li graf-after--blockquote'}))\n", "    entry_text.extend(s.text for s in entry_soup.find_all(\n", "        attrs={'class': 'graf graf--li graf-after--pullquote'}))\n", "\n", "    entry_text = ' '.join(entry_text)\n", "\n", "    # Word count\n", "    word_count = len(re.findall(r\"[\\w']+|[.,!?;]\", entry_text))\n", "\n", "    # Number of claps\n", "    clap_pattern = re.compile(\n", "        '^[0-9]{1,} claps|^[0-9]{1,}.[0-9]{1,}K claps|^[0-9]{1,}K claps')\n", "    claps = entry_soup.find_all(text=clap_pattern)\n", "\n", "    if len(claps) > 0:\n", "        if 'K' in claps[0]:\n", "            clap_number = int(1e3 * float(claps[0].split('K')[0]))\n", "        else:\n", "            clap_number = int(claps[0].split(' ')[0])\n", "    else:\n", "        clap_number = 0\n", "\n", "    # Post tags\n", "    tags = entry_soup.find_all(\n", "        attrs={'class': 'tags tags--postTags tags--borderless'})\n", "    tags = [li.text for li in tags[0].find_all('li')]\n", "\n", "    # Responses to entry\n", "    responses = entry_soup.find_all(attrs={'class': 'button button--chromeless u-baseColor--buttonNormal u-marginRight12',\n", "                                           'data-action': 'scroll-to-responses'})\n", "    num_responses = int(responses[0].text) if len(responses) > 0 else 0\n", "\n", "    # Store in dictionary\n", "    entry_dict['title'] = title\n", "    entry_dict['text'] = entry_text\n", "    entry_dict['word_count'] = word_count\n", "    entry_dict['claps'] = clap_number\n", "    entry_dict['tags'] = tags\n", "    entry_dict['num_responses'] = num_responses\n", "\n", "    return entry_dict"]}, {"block": 54, "type": "code", "linesLength": 3, "startIndex": 302, "lines": ["entry_dict = process_entry(entry, parallel=False)\n", "entry_dict['title']\n", "entry_dict['word_count']"]}, {"block": 55, "type": "code", "linesLength": 8, "startIndex": 305, "lines": ["results = []\n", "\n", "entry_dict = process_entry(entry, parallel=False)\n", "results.append(entry_dict)\n", "results.append(process_entry(table_rows[1], parallel=False))\n", "\n", "df = pd.DataFrame(results)\n", "df.head()"]}, {"block": 56, "type": "markdown", "linesLength": 3, "startIndex": 313, "lines": ["## Process Sequentially \n", "\n", "To see how long this takes when running in a sequence, run the following cell."]}, {"block": 57, "type": "code", "linesLength": 9, "startIndex": 316, "lines": ["from timeit import default_timer as timer\n", "results = []\n", "\n", "start = timer()\n", "for i, e in enumerate(table_rows):\n", "    print(f'{100 * i / len(table_rows):.2f}% complete. Total read time: {sum([t[\"read_time\"] for t in results])} minutes', \n", "          end = '\\r')\n", "    results.append(process_entry(e, parallel=False))\n", "end = timer()"]}, {"block": 58, "type": "code", "linesLength": 1, "startIndex": 325, "lines": ["print(f'Total time {end-start:.2f} seconds for processing {len(results)} articles.')"]}, {"block": 59, "type": "markdown", "linesLength": 1, "startIndex": 326, "lines": ["Almost all of the time is taken up by accessing the articles using requests. We'll vastly speed up this operation by using multiprocessing."]}, {"block": 60, "type": "code", "linesLength": 2, "startIndex": 327, "lines": ["df = pd.DataFrame(results)\n", "df.head()"]}, {"block": 61, "type": "markdown", "linesLength": 5, "startIndex": 329, "lines": ["# Process in Parallel\n", "\n", "To speed things up, we want to run the operations in parallel.\n", "\n", "First, we'll need to convert the table rows to strings because of how pool pickles objects. Beautiful soup objects cannot be pickled, but strings can be. This is slightly inconvenient, but the results will come out fine."]}, {"block": 62, "type": "code", "linesLength": 11, "startIndex": 334, "lines": ["from multiprocessing import Pool\n", "import sys\n", "\n", "table_rows_str = [str(r) for r in table_rows]\n", "\n", "start = timer()\n", "pool = Pool(processes=50)\n", "results = pool.map(process_entry, table_rows_str)\n", "end = timer()\n", "pool.close()\n", "pool.join()"]}, {"block": 63, "type": "code", "linesLength": 1, "startIndex": 345, "lines": ["print(f'Processed {len(results)} articles in {end-start:.2f} seconds.')"]}, {"block": 64, "type": "code", "linesLength": 2, "startIndex": 346, "lines": ["df = pd.DataFrame(results)\n", "df.head()"]}, {"block": 65, "type": "markdown", "linesLength": 3, "startIndex": 348, "lines": ["## Function to Process Articles in Parallel\n", "\n", "The below function processes all of the entries in parallel. It returns a dataframe with the relevant information."]}, {"block": 66, "type": "code", "linesLength": 50, "startIndex": 351, "lines": ["def process_in_parallel(table_rows, processes=20):\n", "    \"\"\"\n", "    Process all the stats in a table in parallel\n", "\n", "    :note: make sure to set the correct time zone in `process_entry`\n", "\n", "    :param table_rows: BeautifulSoup table rows\n", "    \n", "    :param processes: integer number of processes (threads) to use in parallel\n", "\n", "    :return df: dataframe of information about each post\n", "\n", "    \"\"\"\n", "    # Convert to strings for multiprocessing\n", "    table_rows_str = [str(r) for r in table_rows]\n", "\n", "    # Process each article in paralllel\n", "    pool = Pool(processes=processes)\n", "    results = []\n", "    start = timer()\n", "    for i, r in enumerate(pool.imap_unordered(process_entry, table_rows_str)):\n", "        # Report progress\n", "        print(f'{100 * i / len(table_rows_str):.2f}% complete.', end='\\r')\n", "        results.append(r)\n", "    pool.close()\n", "    pool.join()\n", "    end = timer()\n", "    print(\n", "        f'Processed {len(table_rows_str)} articles in {end-start:.2f} seconds.')\n", "\n", "    # Convert to dataframe\n", "    df = pd.DataFrame(results)\n", "    # Add extra columns with more data\n", "    df['claps_per_word'] = df['claps'] / df['word_count']\n", "    df['edit_days'] = (df['published_date'] - df['started_date']\n", "                       ).dt.total_seconds() / (60 * 60 * 24)\n", "\n", "    # 5 most common tags (might want to include more tags)\n", "    n = 5\n", "    all_tags = list(chain(*df['tags'].tolist()))\n", "    tag_counts = Counter(all_tags)\n", "    tags = tag_counts.most_common(n)\n", "\n", "    # Adding columns with indication of tag\n", "    for tag, count in tags:\n", "        flag = [1 if tag in tags else 0 for tags in df['tags']]\n", "        df.loc[:, f'<tag>{tag}'] = flag\n", "\n", "    df.sort_values('published_date', inplace=True)\n", "    return df"]}, {"block": 67, "type": "code", "linesLength": 2, "startIndex": 401, "lines": ["df = process_in_parallel(table_rows, processes=50)\n", "df.head()"]}, {"block": 68, "type": "markdown", "linesLength": 3, "startIndex": 403, "lines": ["# Analysis\n", "\n", "We'll do a little analysis here, but save the main analysis for a separate notebook."]}, {"block": 69, "type": "markdown", "linesLength": 1, "startIndex": 406, "lines": ["## Correlation Heatmap"]}, {"block": 70, "type": "code", "linesLength": 11, "startIndex": 407, "lines": ["import plotly.figure_factory as ff\n", "from plotly.offline import iplot\n", "\n", "corrs = df.corr()\n", "figure = ff.create_annotated_heatmap(\n", "    z=corrs.values,\n", "    x=list(corrs.columns),\n", "    y=list(corrs.index),\n", "    colorscale='Portland',\n", "    annotation_text=corrs.round(2).values)\n", "iplot(figure)"]}, {"block": 71, "type": "markdown", "linesLength": 1, "startIndex": 418, "lines": ["## Scatterplot Matrix"]}, {"block": 72, "type": "code", "linesLength": 5, "startIndex": 419, "lines": ["figure = ff.create_scatterplotmatrix(df[['read_time', 'claps', 'type']],\n", "                                     index = 'type',\n", "                                     diag='histogram', width=800, height=800)\n", "# figure\n", "iplot(figure)"]}, {"block": 73, "type": "code", "linesLength": 5, "startIndex": 424, "lines": ["figure = ff.create_scatterplotmatrix(df[['read_time', 'claps', 'publication']],\n", "                                     index = 'publication',\n", "                                     diag='histogram', width=800, height=800)\n", "# figure\n", "iplot(figure)"]}, {"block": 74, "type": "code", "linesLength": 5, "startIndex": 429, "lines": ["df.iplot(x = 'published_date', y = 'views', mode='markers', \n", "         text='title', layout=dict(yaxis=dict(type='log', \n", "                                              title = 'Views (log scale)'),\n", "                                  xaxis=dict(title='Published Date'),\n", "                                  title='Views vs Published Date'))"]}, {"block": 75, "type": "code", "linesLength": 1, "startIndex": 434, "lines": ["df[df['type'] == 'published'].iplot(x = 'published_date', y = 'word_count', mode='markers');"]}, {"block": 76, "type": "code", "linesLength": 1, "startIndex": 435, "lines": ["df.iplot(x = 'edit_days', y = 'word_count', mode='markers', categories='type')"]}, {"block": 77, "type": "code", "linesLength": 1, "startIndex": 436, "lines": ["df[df['edit_days'] < 0]"]}, {"block": 78, "type": "markdown", "linesLength": 1, "startIndex": 437, "lines": ["I am not sure what is going on here. These two articles appear to have negative editing days."]}, {"block": 79, "type": "code", "linesLength": 3, "startIndex": 438, "lines": ["df[df['type'] == 'published'].iplot(x = 'published_date', y = 'claps',\n", "                                    mode = 'markers', xTitle='Date', yTitle='Number of Claps',\n", "                                    title = 'Number of Claps over Time')"]}, {"block": 80, "type": "code", "linesLength": 3, "startIndex": 441, "lines": ["df[df['type'] == 'published'].iplot(x = 'published_date', y = 'views',\n", "                                    mode = 'markers', xTitle='Date', yTitle='Number of Views',\n", "                                    title = 'Number of Views over Time')"]}, {"block": 81, "type": "code", "linesLength": 8, "startIndex": 444, "lines": ["figure = ff.create_scatterplotmatrix(df[['read_time', 'claps', 'word_count',\n", "                                         'num_responses', 'edit_days','publication']],\n", "                                     index = 'publication', \n", "                                     diag='histogram', \n", "                                     size=8, width=1000, height=1000,\n", "                                     title='Medium Stats Scatterplot Matrix')\n", "\n", "iplot(figure)"]}, {"block": 82, "type": "markdown", "linesLength": 5, "startIndex": 452, "lines": ["# Conclusions\n", "\n", "In this notebook, we developed functions for rapidly retrieving and formatting Medium article statistics. These functions are meant to be used by anyone who wants to analyze their own (or my) Medium stats. \n", "\n", "In the next notebook, `Analysis`, we'll do a deep dive into the statistics both quantitatively and visually. We'll see if there are any insights waiting to be discovered in this data! "]}, {"block": 83, "type": "code", "linesLength": 0, "startIndex": 457, "lines": []}]