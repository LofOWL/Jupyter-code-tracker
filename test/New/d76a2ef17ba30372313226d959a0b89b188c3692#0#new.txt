[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["# Basic Bayesian Linear Regression Implementation"]}, {"block": 1, "type": "code", "linesLength": 18, "startIndex": 1, "lines": ["# Pandas and numpy for data manipulation\n", "import pandas as pd\n", "import numpy as np\n", "\n", "# Matplotlib and seaborn for visualization\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline \n", "\n", "import seaborn as sns\n", "\n", "# Linear Regression to verify implementation\n", "from sklearn.linear_model import LinearRegression\n", "\n", "# Scipy for statistics\n", "import scipy\n", "\n", "# PyMC3 for Bayesian Inference\n", "import pymc3 as pm"]}, {"block": 2, "type": "markdown", "linesLength": 1, "startIndex": 19, "lines": ["# Load in Exercise Data"]}, {"block": 3, "type": "code", "linesLength": 7, "startIndex": 20, "lines": ["exercise = pd.read_csv('data/exercise.csv')\n", "calories = pd.read_csv('data/calories.csv')\n", "df = pd.merge(exercise, calories, on = 'User_ID')\n", "df = df[df['Calories'] < 300]\n", "df = df.reset_index()\n", "df['Intercept'] = 1\n", "df.head()"]}, {"block": 4, "type": "markdown", "linesLength": 1, "startIndex": 27, "lines": ["# Plot Relationship"]}, {"block": 5, "type": "code", "linesLength": 5, "startIndex": 28, "lines": ["plt.figure(figsize=(8, 8))\n", "\n", "plt.plot(df['Duration'], df['Calories'], 'bo');\n", "plt.xlabel('Duration (min)', size = 18); plt.ylabel('Calories', size = 18); \n", "plt.title('Calories burned vs Duration of Exercise', size = 20);"]}, {"block": 6, "type": "code", "linesLength": 3, "startIndex": 33, "lines": ["# Create the features and response\n", "X = df.loc[:, ['Intercept', 'Duration']]\n", "y = df.ix[:, 'Calories']"]}, {"block": 7, "type": "markdown", "linesLength": 1, "startIndex": 36, "lines": ["# Implement Ordinary Least Squares Linear Regression by Hand"]}, {"block": 8, "type": "code", "linesLength": 6, "startIndex": 37, "lines": ["# Takes a matrix of features (with intercept as first column) \n", "# and response vector and calculates linear regression coefficients\n", "def linear_regression(X, y):\n", "    # Equation for linear regression coefficients\n", "    beta = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T), y)\n", "    return beta"]}, {"block": 9, "type": "code", "linesLength": 4, "startIndex": 43, "lines": ["# Run the by hand implementation\n", "by_hand_coefs = linear_regression(X, y)\n", "print('Intercept calculated by hand:', by_hand_coefs[0])\n", "print('Slope calculated by hand: ', by_hand_coefs[1])"]}, {"block": 10, "type": "code", "linesLength": 9, "startIndex": 47, "lines": ["xs = np.linspace(4, 31, 1000)\n", "ys = by_hand_coefs[0] + by_hand_coefs[1] * xs\n", "\n", "plt.figure(figsize=(8, 8))\n", "plt.plot(df['Duration'], df['Calories'], 'bo', label = 'observations', alpha = 0.8);\n", "plt.xlabel('Duration (min)', size = 18); plt.ylabel('Calories', size = 18); \n", "plt.plot(xs, ys, 'r--', label = 'OLS Fit', linewidth = 3)\n", "plt.legend(prop={'size': 16})\n", "plt.title('Calories burned vs Duration of Exercise', size = 20);"]}, {"block": 11, "type": "markdown", "linesLength": 1, "startIndex": 56, "lines": ["## Prediction for Datapoint"]}, {"block": 12, "type": "code", "linesLength": 2, "startIndex": 57, "lines": ["print('Exercising for 15.5 minutes will burn an estimated {:.2f} calories.'.format(\n", "    by_hand_coefs[0] + by_hand_coefs[1] * 15.5))"]}, {"block": 13, "type": "markdown", "linesLength": 1, "startIndex": 59, "lines": ["# Verify with Scikit-learn Implementation"]}, {"block": 14, "type": "code", "linesLength": 5, "startIndex": 60, "lines": ["# Create the model and fit on the data\n", "lr = LinearRegression()\n", "lr.fit(X.Duration.reshape(-1, 1), y)\n", "print('Intercept from library:', lr.intercept_)\n", "print('Slope from library:', lr.coef_[0])"]}, {"block": 15, "type": "markdown", "linesLength": 5, "startIndex": 65, "lines": ["# Bayesian Linear Regression\n", "\n", "### PyMC3 for Bayesian Inference\n", "\n", "Implement MCMC to find the posterior distribution of the model parameters. Rather than a single point estimate of the model weights, Bayesian linear regression will give us a posterior distribution for the model weights."]}, {"block": 16, "type": "markdown", "linesLength": 1, "startIndex": 70, "lines": ["## Model with 500 Observations"]}, {"block": 17, "type": "code", "linesLength": 21, "startIndex": 71, "lines": ["with pm.Model() as linear_model_500:\n", "    # Intercept\n", "    intercept = pm.Normal('Intercept', mu = 0, sd = 10)\n", "    \n", "    # Slope \n", "    slope = pm.Normal('slope', mu = 0, sd = 10)\n", "    \n", "    # Standard deviation\n", "    sigma = pm.HalfNormal('sigma', sd = 10)\n", "    \n", "    # Estimate of mean\n", "    mean = intercept + slope * X.loc[0:499, 'Duration']\n", "    \n", "    # Observed values\n", "    Y_obs = pm.Normal('Y_obs', mu = mean, sd = sigma, observed = y.values[0:500])\n", "    \n", "    # Sampler\n", "    step = pm.NUTS()\n", "\n", "    # Posterior distribution\n", "    linear_trace_500 = pm.sample(1000, step)"]}, {"block": 18, "type": "markdown", "linesLength": 1, "startIndex": 92, "lines": ["## Model with all Observations"]}, {"block": 19, "type": "code", "linesLength": 21, "startIndex": 93, "lines": ["with pm.Model() as linear_model:\n", "    # Intercept\n", "    intercept = pm.Normal('Intercept', mu = 0, sd = 10)\n", "    \n", "    # Slope \n", "    slope = pm.Normal('slope', mu = 0, sd = 10)\n", "    \n", "    # Standard deviation\n", "    sigma = pm.HalfNormal('sigma', sd = 10)\n", "    \n", "    # Estimate of mean\n", "    mean = intercept + slope * X.loc[:, 'Duration']\n", "    \n", "    # Observed values\n", "    Y_obs = pm.Normal('Y_obs', mu = mean, sd = sigma, observed = y.values)\n", "    \n", "    # Sampler\n", "    step = pm.NUTS()\n", "\n", "    # Posterior distribution\n", "    linear_trace = pm.sample(1000, step)"]}, {"block": 20, "type": "markdown", "linesLength": 5, "startIndex": 114, "lines": ["# Bayesian Model Results\n", "\n", "The Bayesian Model provides more opportunities for interpretation than the ordinary least squares regression because it provides a posterior distribution. We can use this distribution to find the most likely single value as well as the entire range of likely values for our model parameters.\n", "\n", "PyMC3 has many built in tools for visualizing and inspecting model runs. These let us see the distributions and provide estimates with a level of uncertainty, which should be a necessary part of any model."]}, {"block": 21, "type": "markdown", "linesLength": 1, "startIndex": 119, "lines": ["## Trace of All Model Parameters"]}, {"block": 22, "type": "code", "linesLength": 1, "startIndex": 120, "lines": ["pm.traceplot(linear_trace, figsize = (12, 12));"]}, {"block": 23, "type": "markdown", "linesLength": 1, "startIndex": 121, "lines": ["## Posterior Distribution of Model Parameters"]}, {"block": 24, "type": "code", "linesLength": 1, "startIndex": 122, "lines": ["pm.plot_posterior(linear_trace, figsize = (12, 10), text_size = 20);"]}, {"block": 25, "type": "markdown", "linesLength": 1, "startIndex": 123, "lines": ["## Confidence Intervals for Model Parameters"]}, {"block": 26, "type": "code", "linesLength": 1, "startIndex": 124, "lines": ["pm.forestplot(linear_trace);"]}, {"block": 27, "type": "markdown", "linesLength": 3, "startIndex": 125, "lines": ["# Predictions of Response Sampled from the Posterior\n", "\n", "We can now generate predictions of the linear regression line using the model results. The following plot shows 1000 different estimates of the regression line drawn from the posterior. The distribution of the lines gives an estimate of the uncertainty in the estimate. Bayesian Linear Regression has the benefit that it gives us a posterior __distribution__ rather than a __single point estimate__ in the frequentist ordinary least squares regression."]}, {"block": 28, "type": "markdown", "linesLength": 1, "startIndex": 128, "lines": ["## All Observations"]}, {"block": 29, "type": "code", "linesLength": 9, "startIndex": 129, "lines": ["plt.figure(figsize = (8, 8))\n", "pm.plot_posterior_predictive_glm(linear_trace, samples = 100, eval=np.linspace(2, 30, 100), linewidth = 1, \n", "                                 color = 'red', alpha = 0.8, label = 'Bayesian Posterior Fits',\n", "                                lm = lambda x, sample: sample['Intercept'] + sample['slope'] * x);\n", "plt.scatter(X['Duration'], y.values, s = 12, alpha = 0.8, c = 'blue', label = 'Observations')\n", "plt.plot(X['Duration'], by_hand_coefs[0] + X['Duration'] * by_hand_coefs[1], 'k--', label = 'OLS Fit', linewidth = 1.4)\n", "plt.title('Posterior Predictions with all Observations', size = 20); plt.xlabel('Duration (min)', size = 18);\n", "plt.ylabel('Calories', size = 18);\n", "plt.legend(prop={'size': 16});"]}, {"block": 30, "type": "code", "linesLength": 1, "startIndex": 138, "lines": ["pm.df_summary(linear_trace)"]}, {"block": 31, "type": "markdown", "linesLength": 1, "startIndex": 139, "lines": ["## Limited Observations"]}, {"block": 32, "type": "code", "linesLength": 9, "startIndex": 140, "lines": ["plt.figure(figsize = (8, 8))\n", "pm.plot_posterior_predictive_glm(linear_trace_500, samples = 100, eval=np.linspace(2, 30, 100), linewidth = 1, \n", "                                 color = 'red', alpha = 0.8, label = 'Bayesian Posterior Fits',\n", "                                lm = lambda x, sample: sample['Intercept'] + sample['slope'] * x);\n", "plt.scatter(X['Duration'][:500], y.values[:500], s = 12, alpha = 0.8, c = 'blue', label = 'Observations')\n", "plt.plot(X['Duration'], by_hand_coefs[0] + X['Duration'] * by_hand_coefs[1], 'k--', label = 'OLS Fit', linewidth = 1.4)\n", "plt.title('Posterior Predictions with Limited Observations', size = 20); plt.xlabel('Duration (min)', size = 18);\n", "plt.ylabel('Calories', size = 18);\n", "plt.legend(prop={'size': 16});"]}, {"block": 33, "type": "code", "linesLength": 1, "startIndex": 149, "lines": ["pm.df_summary(linear_trace_500)"]}, {"block": 34, "type": "markdown", "linesLength": 1, "startIndex": 150, "lines": ["# Specific Prediction for One Datapoint"]}, {"block": 35, "type": "code", "linesLength": 1, "startIndex": 151, "lines": ["bayes_prediction = linear_trace['Intercept'] + linear_trace['slope'] * 15.5"]}, {"block": 36, "type": "code", "linesLength": 10, "startIndex": 152, "lines": ["plt.figure(figsize = (8, 8))\n", "plt.style.use('fivethirtyeight')\n", "sns.kdeplot(bayes_prediction, label = 'Bayes Posterior Prediction')\n", "plt.vlines(x = by_hand_coefs[0] + by_hand_coefs[1] * 15.5, \n", "           ymin = 0, ymax = 2.5, \n", "           label = 'OLS Prediction',\n", "          colors = 'red', linestyles='--')\n", "plt.legend();\n", "plt.xlabel('Calories Burned', size = 18), plt.ylabel('Probability Density', size = 18);\n", "plt.title('Posterior Prediction for 15.5 Minutes', size = 20);"]}, {"block": 37, "type": "code", "linesLength": 0, "startIndex": 162, "lines": []}]