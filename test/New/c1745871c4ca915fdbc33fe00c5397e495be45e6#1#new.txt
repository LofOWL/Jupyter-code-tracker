[{"block": 0, "type": "markdown", "linesLength": 3, "startIndex": 0, "lines": ["# Introduction: Encoding Time Series Features\n", "\n", "In this notebook, we will explore the options for encoding times and dates in a time series problem. The primary objective is to determine the optimal method for representing time in a time-series problem, particularly as it relates to building energy data."]}, {"block": 1, "type": "code", "linesLength": 20, "startIndex": 3, "lines": ["# Standard Data Science Helpers\n", "import numpy as np\n", "import pandas as pd\n", "import os\n", "\n", "import plotly.plotly as py\n", "import plotly.graph_objs as go\n", "from plotly.offline import iplot, init_notebook_mode\n", "init_notebook_mode(connected=True)\n", "\n", "import cufflinks as cf\n", "cf.set_config_file(world_readable=True, theme=\"pearl\")\n", "cf.go_offline(connected=True)\n", "\n", "# Extra options\n", "pd.options.display.max_rows = 10\n", "pd.options.display.max_columns = 30\n", "# Show all code cells outputs\n", "from IPython.core.interactiveshell import InteractiveShell\n", "InteractiveShell.ast_node_interactivity = 'all'"]}, {"block": 2, "type": "code", "linesLength": 5, "startIndex": 23, "lines": ["%load_ext autoreload\n", "%autoreload 2\n", "\n", "from warnings import filterwarnings\n", "filterwarnings('ignore', category=RuntimeWarning)"]}, {"block": 3, "type": "code", "linesLength": 1, "startIndex": 28, "lines": ["from time_features_utils import create_time_features"]}, {"block": 4, "type": "code", "linesLength": 3, "startIndex": 29, "lines": ["data = pd.read_csv('data/building_1.csv', parse_dates=['timestamp'], \n", "                    usecols=['timestamp', 'energy'])\n", "data.head()"]}, {"block": 5, "type": "code", "linesLength": 8, "startIndex": 32, "lines": ["# Remove missing values\n", "data = data.dropna(subset=['energy'])\n", "\n", "# Count frequencies\n", "freq_counts = data['timestamp'].diff(1).value_counts()\n", "# Most common frequency in minutes\n", "freq = round(freq_counts.idxmax().total_seconds() / 60)\n", "freq"]}, {"block": 6, "type": "code", "linesLength": 1, "startIndex": 40, "lines": ["data = data.set_index('timestamp').sort_index()"]}, {"block": 7, "type": "code", "linesLength": 3, "startIndex": 41, "lines": ["data = pd.concat(\n", "    [data, create_time_features(data.index, cyc_encode=True)], axis=1)\n", "data.columns"]}, {"block": 8, "type": "code", "linesLength": 1, "startIndex": 44, "lines": ["data[[c for c in data if ('sin' in c) or ('cos' in c)]].describe()"]}, {"block": 9, "type": "code", "linesLength": 1, "startIndex": 45, "lines": ["data.head()"]}, {"block": 10, "type": "code", "linesLength": 1, "startIndex": 46, "lines": ["data.loc[(data.index.week == 1) & (data.index.year == 2015), 'timestamp_fracday'].iplot()"]}, {"block": 11, "type": "code", "linesLength": 1, "startIndex": 47, "lines": ["data.loc[(data.index.month == 1) & (data.index.year == 2015), 'timestamp_fracweek'].iplot()"]}, {"block": 12, "type": "code", "linesLength": 1, "startIndex": 48, "lines": ["data.loc[(data.index.year == 2015), 'timestamp_fracmonth'].iplot()"]}, {"block": 13, "type": "code", "linesLength": 1, "startIndex": 49, "lines": ["data.loc[:, 'timestamp_fracyear'].iplot()"]}, {"block": 14, "type": "code", "linesLength": 1, "startIndex": 50, "lines": ["data.loc[(data.index.week == 1) & (data.index.year == 2015), ['sin_timestamp_fracday', 'cos_timestamp_fracday']].iplot()"]}, {"block": 15, "type": "code", "linesLength": 1, "startIndex": 51, "lines": ["data.loc[(data.index.month == 1) & (data.index.year == 2015), ['sin_timestamp_fracweek', 'cos_timestamp_fracweek']].iplot()"]}, {"block": 16, "type": "code", "linesLength": 1, "startIndex": 52, "lines": ["data.loc[(data.index.year == 2015), ['sin_timestamp_fracmonth', 'cos_timestamp_fracmonth']].iplot()"]}, {"block": 17, "type": "code", "linesLength": 1, "startIndex": 53, "lines": ["data.loc[:, ['sin_timestamp_fracyear', 'cos_timestamp_fracyear']].iplot()"]}, {"block": 18, "type": "code", "linesLength": 1, "startIndex": 54, "lines": ["np.unique(data.index.day)"]}, {"block": 19, "type": "code", "linesLength": 17, "startIndex": 55, "lines": ["def data_reading(building_filename):\n", "    \"\"\"\n", "    Read in building energy data from csv file\n", "    \"\"\"\n", "    data = pd.read_csv(building_filename, parse_dates=['timestamp'], \n", "                    usecols=['timestamp', 'energy'])\n", "    # Remove missing values\n", "    data = data.dropna(subset=['energy'])\n", "    \n", "    # Count frequencies\n", "    freq_counts = data['timestamp'].diff(1).value_counts()\n", "    # Most common frequency in minutes\n", "    freq = round(freq_counts.idxmax().total_seconds() / 60)\n", "    \n", "    # Set the index\n", "    data = data.set_index('timestamp').sort_index()\n", "    return data, freq, len(data)"]}, {"block": 20, "type": "code", "linesLength": 26, "startIndex": 72, "lines": ["def data_testing(building_filename, model, splits=4):\n", "    \"\"\"\n", "    Test the model on the building energy data in a csv file\n", "    \n", "    :param filename: string filename of building\n", "    :param model: sklearn compatible model\n", "    :param splits: integer number of time series splits (default is 4)\n", "    \n", "    :return: dataframe of results\n", "    \"\"\"\n", "    # File the building id for indexing\n", "    building_id = building_filename.split('_')[-1].split('.csv')[0]\n", "    # Read in the file\n", "    data, freq, dpoints = data_reading(building_filename)\n", "    # Test the model on the data\n", "    results = test_time_date_features(data, model, splits=splits)\n", "    \n", "    model_name = type(model).__name__\n", "    \n", "    # Record the results\n", "    results['freq'] = freq\n", "    results['dpoints'] = dpoints\n", "    results['building_id'] = building_id\n", "    results['model'] = model_name\n", "    results['splits'] = splits\n", "    return results"]}, {"block": 21, "type": "code", "linesLength": 3, "startIndex": 98, "lines": ["def mape(y_true, y_pred):\n", "    # The mean absolute percentage error between true values and prediction\n", "    return 100 * np.mean(np.abs((y_pred - y_true) / y_true))"]}, {"block": 22, "type": "code", "linesLength": 107, "startIndex": 101, "lines": ["from sklearn.model_selection import TimeSeriesSplit\n", "\n", "def test_time_date_features(data, model, splits=4):\n", "    \"\"\"\n", "    Test different time and date encoding methods on building energy data\n", "    \n", "    \n", "    :param data: building energy dataset to use\n", "    :param model: sklearn compatible model\n", "    :param splits: integer number of splits for time series evaluation (default is 4)\n", "    \n", "    :return: dataframe of results\n", "    \n", "    \"\"\"\n", "\n", "    # Create the time features\n", "    data = pd.concat(\n", "        [data, create_time_features(data.index, cyc_encode=True)], axis=1)\n", "\n", "    # Targets\n", "    y = data.pop('energy')\n", "\n", "    # Each set of features\n", "    \n", "    # Baseline typical\n", "    baseline_features = [\n", "        'timestamp_' + t\n", "        for t in ['minute', 'hour', 'dayofweek', 'day', 'week', 'month', 'dayofyear', 'year']\n", "    ]\n", "    \n", "    # Cyclical encoding of baseline\n", "    baseline_cyc_features = [\n", "        'sin_' + t for t in baseline_features\n", "        if t not in ['timestamp_dayofyear', 'timestamp_year', 'timestamp_minute']\n", "    ] + [\n", "        'cos_' + t for t in baseline_features\n", "        if t not in ['timestamp_dayofyear', 'timestamp_year', 'timestamp_minute']\n", "    ]\n", "\n", "    # Fractional features\n", "    frac_features = [\n", "        'timestamp_' + t\n", "        for t in ['fracday', 'fracweek', 'fracmonth', 'fracyear']\n", "    ]\n", "    \n", "    # Cyclical encoding of fractional\n", "    frac_cyc_features = ['sin_' + t for t in frac_features\n", "                         ] + ['cos_' + t for t in frac_features]\n", "\n", "    # Intuition based features\n", "    # Cylical time of day\n", "    # Day of week\n", "    # Cyclical time of year\n", "    domain_features = [\n", "        'sin_timestamp_fracday', 'cos_timestamp_fracday', 'timestamp_dayofweek',\n", "        'sin_timestamp_fracyear', 'cos_timestamp_fracyear'\n", "    ]\n", "\n", "    # Dictionary to hold results\n", "    results = {}\n", "    \n", "    # Lists for storing results\n", "    scores = []\n", "    stds = []\n", "    methods = []\n", "    test_points = []\n", "    method_names = ['baseline', 'baseline_cyc', 'frac', 'frac_cyc', 'domain']\n", "\n", "    tss = TimeSeriesSplit(n_splits=splits)\n", "    \n", "    # Iterate through each set of features\n", "    for features, name in zip(\n", "        [baseline_features, baseline_cyc_features, frac_features, frac_cyc_features, domain_features],\n", "            method_names):\n", "        \n", "        # Create a new list to hold results for each method\n", "        method_scores = []\n", "        dataset = data[features].copy()\n", "        \n", "        # Drop any columns with only 1 value\n", "        to_drop = dataset.columns[(dataset.nunique() == 1)]\n", "        dataset = dataset.drop(columns=to_drop)\n", "        \n", "\n", "        # Time series based evaluation\n", "        for train_idx, test_idx in tss.split(dataset):\n", "\n", "            X_train, X_test = dataset.iloc[train_idx], dataset.iloc[test_idx]\n", "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n", "\n", "            model.fit(X_train, y_train)\n", "            predictions = model.predict(X_test)\n", "            \n", "            mape_value = mape(y_test, predictions)\n", "            # Record score as an accuracy so higher is better\n", "            method_scores.append(100 - mape_value)\n", "        \n", "         # Find the stats for the method\n", "        scores.append(np.median(method_scores))\n", "        stds.append(np.std(method_scores))\n", "        methods.append(name)\n", "        \n", "        # Number of testing points is always the same\n", "        test_points.append(len(test_idx))\n", "    \n", "    results = pd.DataFrame(dict(score=scores, std=stds, method=methods, test_points=test_points))\n", "    return results"]}, {"block": 23, "type": "code", "linesLength": 9, "startIndex": 208, "lines": ["from sklearn.linear_model import LinearRegression\n", "from sklearn.ensemble import RandomForestRegressor\n", "\n", "\n", "linear_model = LinearRegression()\n", "random_model = RandomForestRegressor(random_state=42, n_estimators=10, n_jobs=-1)\n", "rl = data_testing('data/building_1.csv', linear_model, splits=5)\n", "rf = data_testing('data/building_1.csv', random_model, splits=5)\n", "rl.append(rf)"]}, {"block": 24, "type": "code", "linesLength": 5, "startIndex": 217, "lines": ["linear_model = LinearRegression()\n", "random_model = RandomForestRegressor(random_state=42, n_estimators=100, n_jobs=-1)\n", "rl = data_testing('data/building_1.csv', linear_model, splits=5)\n", "rf = data_testing('data/building_1.csv', random_model, splits=5)\n", "rl.append(rf)"]}, {"block": 25, "type": "code", "linesLength": 5, "startIndex": 222, "lines": ["linear_model = LinearRegression()\n", "random_model = RandomForestRegressor(random_state=42, n_estimators=10, max_depth=10, n_jobs=-1)\n", "rl = data_testing('data/building_1.csv', linear_model, splits=5)\n", "rf = data_testing('data/building_1.csv', random_model, splits=5)\n", "rl.append(rf)"]}, {"block": 26, "type": "code", "linesLength": 5, "startIndex": 227, "lines": ["linear_model = LinearRegression()\n", "random_model = RandomForestRegressor(random_state=42, n_estimators=100, max_depth=10, n_jobs=-1)\n", "rl = data_testing('data/building_1.csv', linear_model, splits=5)\n", "rf = data_testing('data/building_1.csv', random_model, splits=5)\n", "rl.append(rf)"]}, {"block": 27, "type": "code", "linesLength": 15, "startIndex": 232, "lines": ["from tqdm import tqdm_notebook\n", "\n", "linear_model = LinearRegression()\n", "random_model = RandomForestRegressor(random_state=42, n_estimators=10, max_depth=10, n_jobs=-1)\n", "\n", "def run_all_buildings():\n", "    results = []\n", "    for file in tqdm_notebook(os.listdir('data')):\n", "        if file.endswith('.csv'):\n", "            results.append(data_testing(f'data/{file}', linear_model, splits=5))\n", "            results.append(data_testing(f'data/{file}', random_model, splits=5))\n", "            \n", "    results = pd.concat(results)\n", "    results.to_csv(f'results/results.csv')\n", "    return results"]}, {"block": 28, "type": "code", "linesLength": 1, "startIndex": 247, "lines": ["results = run_all_buildings()"]}, {"block": 29, "type": "code", "linesLength": 1, "startIndex": 248, "lines": ["all_linear_results.groupby('building_id').apply(lambda x: x.loc[x['score'].idxmax(), 'method']).value_counts()"]}, {"block": 30, "type": "code", "linesLength": 1, "startIndex": 249, "lines": ["all_linear_results.pivot_table(index='building_id', columns='method', values='score').iplot()"]}, {"block": 31, "type": "code", "linesLength": 1, "startIndex": 250, "lines": ["all_linear_results.set_index('building_id').iplot(y='score', categories='method')"]}, {"block": 32, "type": "code", "linesLength": 1, "startIndex": 251, "lines": ["all_random_results.groupby('building_id').apply(lambda x: x.loc[x['score'].idxmax, 'method']).value_counts()"]}]