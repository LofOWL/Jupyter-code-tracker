[{"block": 0, "type": "markdown", "linesLength": 3, "startIndex": 0, "lines": ["# Introduction: Stats for Medium Articles\n", "\n", "In this notebook, we will explore my medium article statistics. We'll work with the raw HTML of the stats page."]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 3, "lines": ["![](images/stat_graph.png)"]}, {"block": 2, "type": "code", "linesLength": 27, "startIndex": 4, "lines": ["# Data science imports\n", "import pandas as pd\n", "import numpy as np\n", "\n", "# Options for pandas\n", "pd.options.display.max_columns = 25\n", "\n", "# Display all cell outputs\n", "from IPython.core.interactiveshell import InteractiveShell\n", "InteractiveShell.ast_node_interactivity = 'all'\n", "\n", "# Interactive plotting\n", "import plotly.plotly as py\n", "import plotly.graph_objs as go\n", "import cufflinks\n", "cufflinks.go_offline()\n", "\n", "# Parsing articles\n", "from bs4 import BeautifulSoup\n", "\n", "# Utilities\n", "from collections import Counter, defaultdict\n", "from itertools import chain\n", "import re\n", "\n", "# Getting webpages\n", "import requests"]}, {"block": 3, "type": "markdown", "linesLength": 1, "startIndex": 31, "lines": ["Make sure to scroll all the way down on the stats page and then save."]}, {"block": 4, "type": "code", "linesLength": 2, "startIndex": 32, "lines": ["soup = BeautifulSoup(open('data/stats.html', 'r'), 'html.parser')\n", "soup.text[:100]"]}, {"block": 5, "type": "code", "linesLength": 2, "startIndex": 34, "lines": ["res_soup = BeautifulSoup(open('data/stats-responses.html', 'r'), 'html.parser')\n", "res_soup.text[:100]"]}, {"block": 6, "type": "code", "linesLength": 2, "startIndex": 36, "lines": ["table_rows = soup.find_all(attrs={'class':\"sortableTable-row js-statsTableRow\"})\n", "print(f'Found {len(table_rows)} articles.')"]}, {"block": 7, "type": "code", "linesLength": 2, "startIndex": 38, "lines": ["res_table_rows = res_soup.find_all(attrs={'class':\"sortableTable-row js-statsTableRow\"})\n", "print(f'Found {len(res_table_rows)} responses.')"]}, {"block": 8, "type": "markdown", "linesLength": 1, "startIndex": 40, "lines": ["# Individual Article"]}, {"block": 9, "type": "markdown", "linesLength": 1, "startIndex": 41, "lines": ["![](images/table.png)"]}, {"block": 10, "type": "code", "linesLength": 2, "startIndex": 42, "lines": ["entry = table_rows[0]\n", "entry"]}, {"block": 11, "type": "code", "linesLength": 1, "startIndex": 44, "lines": ["entry.get('data-timestamp')"]}, {"block": 12, "type": "code", "linesLength": 4, "startIndex": 45, "lines": ["def convert_timestamp(ts: int, tz: str):\n", "    return pd.to_datetime(ts, origin='unix', unit='ms').tz_localize('UTC').tz_convert(tz)\n", "\n", "convert_timestamp(entry.get('data-timestamp'), 'America/New_York')"]}, {"block": 13, "type": "markdown", "linesLength": 3, "startIndex": 49, "lines": ["This is the time at which the article was started. \n", "\n", "The time at which the article was published is also in the data."]}, {"block": 14, "type": "code", "linesLength": 2, "startIndex": 52, "lines": ["published_time = entry.find_all(attrs={'class': 'sortableTable-value'})[0].text\n", "published_time"]}, {"block": 15, "type": "code", "linesLength": 1, "startIndex": 54, "lines": ["convert_timestamp(published_time, 'America/Chicago')"]}, {"block": 16, "type": "code", "linesLength": 1, "startIndex": 55, "lines": ["int(entry.find_all(attrs={'class':'readingTime'})[0].get('title').split(' ')[0])"]}, {"block": 17, "type": "markdown", "linesLength": 1, "startIndex": 56, "lines": ["We can get all of the interesting information by searching the table."]}, {"block": 18, "type": "code", "linesLength": 2, "startIndex": 57, "lines": ["for i in entry.find_all(attrs={'class':'sortableTable-value'}):\n", "    print(i)"]}, {"block": 19, "type": "code", "linesLength": 2, "startIndex": 59, "lines": ["for i in entry.find_all(attrs={'class':'u-sm-show'}):\n", "    print(i) "]}, {"block": 20, "type": "code", "linesLength": 11, "startIndex": 61, "lines": ["entry_dict = {}\n", "for value, key in zip(entry.find_all(attrs={'class':'sortableTable-value'}),\n", "            ['published_timestamp', 'views', 'reads', 'ratio', 'fans']):\n", "    entry_dict[key] = float(value.text) if key == 'ratio' else int(value.text)\n", "    \n", "entry_dict['published_timestamp'] = convert_timestamp(entry_dict['published_timestamp'], 'America/Chicago')\n", "entry_dict['started_timestamp'] = convert_timestamp(entry.get('data-timestamp'), 'America/Chicago')\n", "entry_dict['read_time'] = int(entry.find_all(attrs={'class':'readingTime'})[0].get('title').split(' ')[0])\n", "\n", "from pprint import pprint\n", "pprint(entry_dict)"]}, {"block": 21, "type": "markdown", "linesLength": 1, "startIndex": 72, "lines": ["## Getting Story Link"]}, {"block": 22, "type": "markdown", "linesLength": 1, "startIndex": 73, "lines": ["We'll want to store the article link so we can go to it and find more information."]}, {"block": 23, "type": "code", "linesLength": 2, "startIndex": 74, "lines": ["link = entry.find_all(text='View story', attrs={'class': 'sortableTable-link'})[0].get('href')\n", "link"]}, {"block": 24, "type": "code", "linesLength": 2, "startIndex": 76, "lines": ["r = requests.get(link)\n", "type(r)"]}, {"block": 25, "type": "code", "linesLength": 1, "startIndex": 78, "lines": ["len(r.content)"]}, {"block": 26, "type": "markdown", "linesLength": 1, "startIndex": 79, "lines": ["# Getting Information from Story"]}, {"block": 27, "type": "markdown", "linesLength": 1, "startIndex": 80, "lines": ["We can go to the article itself to extract more information, such as the number of words, the number of claps, and the tags."]}, {"block": 28, "type": "markdown", "linesLength": 1, "startIndex": 81, "lines": ["## Article Title and Number of Words"]}, {"block": 29, "type": "code", "linesLength": 14, "startIndex": 82, "lines": ["# Retrieve the article and create a soup\n", "entry_content = requests.get(link).content\n", "entry_soup = BeautifulSoup(entry_content)\n", "\n", "title = entry_soup.h1.text\n", "\n", "# Text as single long string\n", "entry_text = [p.text for p in entry_soup.find_all('p')]\n", "entry_text = ' '.join(entry_text)\n", "\n", "# Word count\n", "word_count = len(entry_text.split(' '))\n", "\n", "print(f'\"{title}\" has {word_count} words with a read time of {entry_dict[\"read_time\"]} minutes.')"]}, {"block": 30, "type": "markdown", "linesLength": 1, "startIndex": 96, "lines": ["## Post Number of Claps"]}, {"block": 31, "type": "code", "linesLength": 13, "startIndex": 97, "lines": ["# Number of claps\n", "clap_pattern = re.compile('^[0-9]{1,} claps|^[0-9]{1,}.[0-9]{1,}K claps|^[0-9]{1,}K claps')\n", "claps = entry_soup.find_all(text = clap_pattern)\n", "\n", "if len(claps) > 0:\n", "    if 'K' in claps[0]:\n", "        clap_number = int(1e3 * float(claps[0].split('K')[0]))\n", "    else:\n", "        clap_number = int(claps[0].split(' ')[0])\n", "else:\n", "    clap_number = 0\n", "\n", "print(f'\"{title}\" has {clap_number} claps from {entry_dict[\"fans\"]} fans.')"]}, {"block": 32, "type": "markdown", "linesLength": 1, "startIndex": 110, "lines": ["## Post Tags"]}, {"block": 33, "type": "code", "linesLength": 4, "startIndex": 111, "lines": ["# Post tags\n", "tags = entry_soup.find_all(attrs={'class': 'tags tags--postTags tags--borderless'})\n", "tags = [li.text for li in tags[0].find_all('li')]\n", "tags"]}, {"block": 34, "type": "markdown", "linesLength": 1, "startIndex": 115, "lines": ["## Convert to DataFrame"]}, {"block": 35, "type": "markdown", "linesLength": 1, "startIndex": 116, "lines": ["First we'll put all of the information in a dictionary, and then convert to a dataframe. This allows for ease of analysis and visualization."]}, {"block": 36, "type": "code", "linesLength": 6, "startIndex": 117, "lines": ["# Store in dictionary with title as key\n", "entry_dict['title'] = title\n", "entry_dict['text'] = entry_text\n", "entry_dict['word_count'] = word_count\n", "entry_dict['claps'] = clap_number\n", "entry_dict['tags'] = tags"]}, {"block": 37, "type": "code", "linesLength": 2, "startIndex": 123, "lines": ["df = pd.DataFrame([entry_dict])\n", "df.head()"]}, {"block": 38, "type": "code", "linesLength": 7, "startIndex": 125, "lines": ["# Add extra columns with more data\n", "df['response'] = ['response' if x == True else 'article' for x in df['title'].str.contains('response')]\n", "df['claps_per_word'] = df['claps'] / df['word_count']\n", "df['words_per_minute'] = df['word_count'] / df['read_time']\n", "df['edit_days'] = (df['published_timestamp'] - df['started_timestamp']).dt.total_seconds() / (60 * 60 * 24)\n", "\n", "df.head()"]}, {"block": 39, "type": "markdown", "linesLength": 3, "startIndex": 132, "lines": ["## Tags to Columns\n", "\n", "Next we'll one-hot-encode the five most popular tags. Each tag now becomes one column, where the value is 1 if the story has a tag and 0 otherwise (this will be relevant when we have multiple articles with different tags)."]}, {"block": 40, "type": "code", "linesLength": 11, "startIndex": 135, "lines": ["# Add 5 most common tags with flag column if data has it\n", "n = 5\n", "all_tags = list(chain(*df['tags'].tolist()))\n", "tag_counts = Counter(all_tags)\n", "tags = tag_counts.most_common(n)\n", "\n", "for tag, count in tags:\n", "    flag = [1 if tag in tags else 0 for tags in df['tags']]\n", "    df.loc[:, f'<tag>{tag}'] = flag\n", "    \n", "df.head()"]}, {"block": 41, "type": "code", "linesLength": 1, "startIndex": 146, "lines": ["type(entry)"]}, {"block": 42, "type": "markdown", "linesLength": 3, "startIndex": 147, "lines": ["# Function to Process Single Entry\n", "\n", "We'll encapsulate all of the steps in a single function. This function is designed to be run in parallel, but we can also run it on a single entry."]}, {"block": 43, "type": "code", "linesLength": 65, "startIndex": 150, "lines": ["def process_table_entry(entry, parallel=True, tz='America/Chicago'):\n", "    \"\"\"\n", "    Extract data from one entry in table\n", "    \n", "    :param entry: BeautifulSoup tag\n", "    :param parallel: Boolean for whether function is being run in parallel\n", "    :param tz: string representing timezone for started and published time\n", "    \n", "    :return entry_dict: dictionary with data about entry\n", "    \n", "    \"\"\"\n", "    if parallel:\n", "        entry = BeautifulSoup(entry, 'html.parser')\n", "        \n", "    entry_dict = {}\n", "    for value, key in zip(entry.find_all(attrs={'class':'sortableTable-value'}),\n", "            ['published_timestamp', 'views', 'reads', 'ratio', 'fans']):\n", "        entry_dict[key] = float(value.text) if key == 'ratio' else int(value.text)\n", "    \n", "    entry_dict['published_timestamp'] = convert_timestamp(entry_dict['published_timestamp'], tz=tz)\n", "    entry_dict['started_timestamp'] = convert_timestamp(entry.get('data-timestamp'), tz=tz)\n", "    entry_dict['read_time'] = int(entry.find_all(attrs={'class':'readingTime'})[0].get('title').split(' ')[0])\n", "    entry_dict['unlisted'] = True if len(entry.find_all(text=' Unlisted')) > 0 else False\n", "    \n", "    link = entry.find_all(text='View story', \n", "                               attrs={'class': 'sortableTable-link'})[0].get('href')\n", "    \n", "    # Retrieve the article and create a soup\n", "    entry = requests.get(link).content\n", "    entry_soup = BeautifulSoup(entry)\n", "    \n", "\n", "    title = entry_soup.h1.text\n", "\n", "    # Text as single long string\n", "    entry_text = [p.text for p in entry_soup.find_all('p')]\n", "    entry_text = ' '.join(entry_text)\n", "\n", "    # Word count\n", "    word_count = len(entry_text.split(' '))\n", "\n", "    # Number of claps\n", "    clap_pattern = re.compile('^[0-9]{1,} claps|^[0-9]{1,}.[0-9]{1,}K claps|^[0-9]{1,}K claps')\n", "    claps = entry_soup.find_all(text = clap_pattern)\n", "\n", "    if len(claps) > 0:\n", "        if 'K' in claps[0]:\n", "            clap_number = int(1e3 * float(claps[0].split('K')[0]))\n", "        else:\n", "            clap_number = int(claps[0].split(' ')[0])\n", "    else:\n", "        clap_number = 0\n", "\n", "    # Post tags\n", "    tags = entry_soup.find_all(attrs={'class': 'tags tags--postTags tags--borderless'})\n", "    tags = [li.text for li in tags[0].find_all('li')]\n", "        \n", "    # Store in dictionary with title as key\n", "    entry_dict['title'] = title\n", "    entry_dict['text'] = entry_text\n", "    entry_dict['word_count'] = word_count\n", "    entry_dict['claps'] = clap_number\n", "    entry_dict['tags'] = tags\n", "    \n", "    return entry_dict"]}, {"block": 44, "type": "code", "linesLength": 8, "startIndex": 215, "lines": ["results = []\n", "\n", "entry_dict = process_table_entry(entry, parallel=False)\n", "results.append(entry_dict)\n", "results.append(process_table_entry(table_rows[1], parallel=False))\n", "\n", "df = pd.DataFrame(results)\n", "df.head()"]}, {"block": 45, "type": "markdown", "linesLength": 3, "startIndex": 223, "lines": ["## Process Sequentially \n", "\n", "To see how long this takes when running in a sequence, run the following cell."]}, {"block": 46, "type": "code", "linesLength": 1, "startIndex": 226, "lines": ["from timeit import default_timer as timer"]}, {"block": 47, "type": "code", "linesLength": 8, "startIndex": 227, "lines": ["results = []\n", "\n", "start = timer()\n", "for i, e in enumerate(table_rows):\n", "    print(f'{100 * i / len(table_rows):.2f}% complete. Total read time: {sum([t[\"read_time\"] for t in results])}', \n", "          end = '\\r')\n", "    results.append(process_table_entry(e, parallel=False))\n", "end = timer()"]}, {"block": 48, "type": "code", "linesLength": 1, "startIndex": 235, "lines": ["print(f'Total time {end-start:.2f} seconds for processing {len(results)} articles.')"]}, {"block": 49, "type": "markdown", "linesLength": 1, "startIndex": 236, "lines": ["Almost all of the time is taken up by accessing the articles using requests. Later, we'll vastly speed up this operation by using multiprocessing."]}, {"block": 50, "type": "code", "linesLength": 1, "startIndex": 237, "lines": ["df = pd.DataFrame(results)"]}, {"block": 51, "type": "code", "linesLength": 15, "startIndex": 238, "lines": ["df['response'] = ['response' if x == True else 'article' for x in df['title'].str.contains('response')]\n", "df['claps_per_word'] = df['claps'] / df['word_count']\n", "df['words_per_minute'] = df['word_count'] / df['read_time']\n", "df['edit_days'] = (df['published_timestamp'] - df['started_timestamp']).dt.total_seconds() / (60 * 60 * 24)\n", "\n", "n = 5\n", "all_tags = list(chain(*df['tags'].tolist()))\n", "tag_counts = Counter(all_tags)\n", "tags = tag_counts.most_common(n)\n", "\n", "for tag, count in tags:\n", "    flag = [1 if tag in tags else 0 for tags in df['tags']]\n", "    df.loc[:, f'<tag>{tag}'] = flag\n", "    \n", "df.head()"]}, {"block": 52, "type": "code", "linesLength": 1, "startIndex": 253, "lines": ["df.corr()"]}, {"block": 53, "type": "code", "linesLength": 9, "startIndex": 254, "lines": ["import plotly.figure_factory as ff\n", "from plotly.offline import iplot \n", "\n", "corrs = df.corr()\n", "figure = ff.create_annotated_heatmap(z = corrs.values, x =list(corrs.columns), \n", "                            y=list(corrs.index), showscale=True, \n", "                                     colorscale='Picnic',\n", "                                     annotation_text=corrs.round(2).values)\n", "iplot(figure)"]}, {"block": 54, "type": "code", "linesLength": 4, "startIndex": 263, "lines": ["figure = ff.create_scatterplotmatrix(df[['read_time', 'claps', 'unlisted']], index = 'unlisted',\n", "                                     diag='histogram')\n", "# figure\n", "iplot(figure)"]}, {"block": 55, "type": "code", "linesLength": 3, "startIndex": 267, "lines": ["colorscales = ['Greys', 'YlGnBu', 'Greens', 'YlOrRd', 'Bluered', 'RdBu',\n", "            'Reds', 'Blues', 'Picnic', 'Rainbow', 'Portland', 'Jet',\n", "            'Hot', 'Blackbody', 'Earth', 'Electric', 'Viridis', 'Cividis']"]}, {"block": 56, "type": "markdown", "linesLength": 5, "startIndex": 270, "lines": ["# Process in Parallel\n", "\n", "To speed things up, we want to run the operations in parallel.\n", "\n", "First, we'll need to convert the table rows to strings because of how pool pickles objects. Beatufiu"]}, {"block": 57, "type": "code", "linesLength": 10, "startIndex": 275, "lines": ["from multiprocessing import Pool\n", "import sys\n", "\n", "table_rows_str = [str(r) for r in table_rows]\n", "\n", "pool = Pool(processes=10)\n", "r = pool.map(process_table_entry, \n", "             table_rows_str)\n", "pool.close()\n", "pool.join()"]}, {"block": 58, "type": "code", "linesLength": 2, "startIndex": 285, "lines": ["df = pd.DataFrame(r)\n", "df.head()"]}, {"block": 59, "type": "code", "linesLength": 16, "startIndex": 287, "lines": ["from itertools import chain\n", "\n", "# Add extra columns with more data\n", "df['response'] = ['response' if x == True else 'article' for x in df['title'].str.contains('response')]\n", "df['claps_per_word'] = df['claps'] / df['word_count']\n", "df['words_per_minute'] = df['word_count'] / df['read_time']\n", "\n", "# Add 10 most common tags with flag if data has it\n", "n = 10\n", "all_tags = list(chain(*df['tags'].tolist()))\n", "tag_counts = Counter(all_tags)\n", "tags = tag_counts.most_common(n)\n", "\n", "for tag, count in tags:\n", "    flag = [1 if tag in tags else 0 for tags in df['tags']]\n", "    df.loc[:, f'<tag>{tag}'] = flag"]}, {"block": 60, "type": "code", "linesLength": 1, "startIndex": 303, "lines": ["df.corr()"]}, {"block": 61, "type": "code", "linesLength": 1, "startIndex": 304, "lines": ["from timeit import default_timer as timer"]}, {"block": 62, "type": "code", "linesLength": 40, "startIndex": 305, "lines": ["def process_in_parallel(table_rows, processes):\n", "    \"\"\"\n", "    Process all the stats in a table in parallel\n", "    \n", "    :param table_rows: BeautifulSoup table rows\n", "    \n", "    :return df: dataframe of information about each post\n", "    \n", "    \"\"\"\n", "    table_rows_str = [str(r) for r in table_rows]\n", "    \n", "    pool = Pool(processes=processes)\n", "    results = [] \n", "    start = timer()\n", "    for i, r in enumerate(pool.imap_unordered(process_table_entry, table_rows_str)):\n", "        print(f'{100 * i / len(table_rows_str):.2f}% complete.', end = '\\r')\n", "        results.append(r)\n", "    pool.close()\n", "    pool.join()\n", "    end = timer()\n", "    print(f'Processed {len(table_rows_str)} articles in {end-start:.2f} seconds.')\n", "    df = pd.DataFrame(results)\n", "    \n", "    # Add extra columns with more data\n", "    df['response'] = ['response' if x == True else 'article' for x in df['title'].str.contains('response')]\n", "    df['claps_per_word'] = df['claps'] / df['word_count']\n", "    df['words_per_minute'] = df['word_count'] / df['read_time']\n", "\n", "    # Add 10 most common tags with flag if data has it\n", "    n = 10\n", "    all_tags = list(chain(*df['tags'].tolist()))\n", "    tag_counts = Counter(all_tags)\n", "    tags = tag_counts.most_common(n)\n", "\n", "    for tag, count in tags:\n", "        flag = [1 if tag in tags else 0 for tags in df['tags']]\n", "        df.loc[:, f'<tag>{tag}'] = flag\n", "    \n", "\n", "    return df"]}, {"block": 63, "type": "code", "linesLength": 1, "startIndex": 345, "lines": ["df = process_in_parallel(table_rows, processes=20)"]}, {"block": 64, "type": "code", "linesLength": 1, "startIndex": 346, "lines": ["df['unlisted'] = df['unlisted'].astype(str)"]}, {"block": 65, "type": "code", "linesLength": 1, "startIndex": 347, "lines": ["df.iplot(x = 'published_timestamp', y = 'word_count', categories='unlisted');"]}, {"block": 66, "type": "code", "linesLength": 2, "startIndex": 348, "lines": ["df[df['unlisted'] == 'False'].iplot(x = 'published_timestamp', y = 'read_time',\n", "                                    mode = 'markers')"]}, {"block": 67, "type": "code", "linesLength": 1, "startIndex": 350, "lines": ["df.head()"]}, {"block": 68, "type": "code", "linesLength": 1, "startIndex": 351, "lines": ["df.shape"]}, {"block": 69, "type": "code", "linesLength": 0, "startIndex": 352, "lines": []}]