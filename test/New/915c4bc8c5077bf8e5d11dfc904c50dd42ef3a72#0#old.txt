[{"block": 0, "type": "markdown", "linesLength": 5, "startIndex": 0, "lines": ["# Improving the Random Forest Model Part 1\n", "\n", "## Using More Data and Feature Reduction\n", "\n", "This is the first of three parts that will examine improving the simple random forest model created in a [previous blog post](https://medium.com/@williamkoehrsen/random-forest-in-python-24d0893d51c0). The first post will focus on achieving better performance through additional data and choosing the most important features."]}, {"block": 1, "type": "markdown", "linesLength": 7, "startIndex": 5, "lines": ["## Three Approaches to Making a Better ML Model\n", "\n", "1. More high-quality data\n", "2. Hyperparameter tuning of algorithm\n", "3. Try different algorithm\n", "\n", "We will look at approach 1. here with the other two methods to be explored later!"]}, {"block": 2, "type": "markdown", "linesLength": 5, "startIndex": 12, "lines": ["# Recap of Previous Work\n", "\n", "We are working on a supervised, regression machine learning task where the goal is to predict the maximum temperature tomorrow (in Seattle, WA) from past historical data. The first attempt used one year of data for training. \n", "\n", "The code below recreates the simple random forest model written up in [my article](https://medium.com/@williamkoehrsen/random-forest-in-python-24d0893d51c0) on Medium. We use the default parameters except for increasing the number of decision trees (n_estimators) to 1000. The original model achieves a mean average error of 3.83 degrees and an accuracy of 94%."]}, {"block": 3, "type": "code", "linesLength": 64, "startIndex": 17, "lines": ["# Pandas is used for data manipulation\n", "import pandas as pd\n", "\n", "# Read in data as pandas dataframe and display first 5 rows\n", "original_features = pd.read_csv('data/temps.csv')\n", "original_features = pd.get_dummies(original_features)\n", "\n", "# Use numpy to convert to arrays\n", "import numpy as np\n", "\n", "# Labels are the values we want to predict\n", "original_labels = np.array(original_features['actual'])\n", "\n", "# Remove the labels from the features\n", "# axis 1 refers to the columns\n", "original_features= original_features.drop('actual', axis = 1)\n", "\n", "# Saving feature names for later use\n", "original_feature_list = list(original_features.columns)\n", "\n", "# Convert to numpy array\n", "original_features = np.array(original_features)\n", "\n", "# Using Skicit-learn to split data into training and testing sets\n", "from sklearn.model_selection import train_test_split\n", "\n", "# Split the data into training and testing sets\n", "original_train_features, original_test_features, original_train_labels, original_test_labels = train_test_split(original_features, original_labels, test_size = 0.25, random_state = 42)\n", "\n", "# The baseline predictions are the historical averages\n", "baseline_preds = original_test_features[:, original_feature_list.index('average')]\n", "\n", "# Baseline errors, and display average baseline error\n", "baseline_errors = abs(baseline_preds - original_test_labels)\n", "print('Average baseline error: ', round(np.mean(baseline_errors), 2), 'degrees.')\n", "\n", "# Import the model we are using\n", "from sklearn.ensemble import RandomForestRegressor\n", "\n", "# Instantiate model \n", "rf = RandomForestRegressor(n_estimators= 1000, random_state=42)\n", "\n", "# Train the model on training data\n", "rf.fit(original_train_features, original_train_labels);\n", "\n", "# Use the forest's predict method on the test data\n", "predictions = rf.predict(original_test_features)\n", "\n", "# Calculate the absolute errors\n", "errors = abs(predictions - original_test_labels)\n", "\n", "# Print out the mean absolute error (mae)\n", "print('Average model error:', round(np.mean(errors), 2), 'degrees.')\n", "\n", "# Compare to baseline\n", "improvement_baseline = 100 * abs(np.mean(errors) - np.mean(baseline_errors)) / np.mean(baseline_errors)\n", "print('Improvement over baseline:', round(improvement_baseline, 2), '%.')\n", "\n", "# Calculate mean absolute percentage error (MAPE)\n", "mape = 100 * (errors / original_test_labels)\n", "\n", "# Calculate and display accuracy\n", "accuracy = 100 - np.mean(mape)\n", "print('Accuracy:', round(accuracy, 2), '%.')"]}, {"block": 4, "type": "markdown", "linesLength": 5, "startIndex": 81, "lines": ["# Collect More Data\n", "\n", "All data is obtained from the [NOAA climate data online](https://www.ncdc.noaa.gov/cdo-web/) tool. Feel free to check for your city! \n", "\n", "This time we will use 6 years of historical data and include some additional variables to augment the temperature features used in the simple model. "]}, {"block": 5, "type": "code", "linesLength": 6, "startIndex": 86, "lines": ["# Pandas is used for data manipulation\n", "import pandas as pd\n", "\n", "# Read in data as a dataframe\n", "features = pd.read_csv('data/temps_extended.csv')\n", "features.head(5)"]}, {"block": 6, "type": "code", "linesLength": 1, "startIndex": 92, "lines": ["print('We have {} days of data with {} variables.'.format(*features.shape))"]}, {"block": 7, "type": "markdown", "linesLength": 1, "startIndex": 93, "lines": ["## Numerical and Visual Inspection of Data"]}, {"block": 8, "type": "code", "linesLength": 1, "startIndex": 94, "lines": ["round(features.describe(), 2)"]}, {"block": 9, "type": "code", "linesLength": 19, "startIndex": 95, "lines": ["# Use datetime for dealing with dates\n", "import datetime\n", "\n", "# Get years, months, and days\n", "years = features['year']\n", "months = features['month']\n", "days = features['day']\n", "\n", "# List and then convert to datetime object\n", "dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\n", "dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in dates]\n", "\n", "# Import matplotlib for plotting and use magic command for Jupyter Notebooks\n", "import matplotlib.pyplot as plt\n", "\n", "%matplotlib inline\n", "\n", "# Set the style\n", "plt.style.use('fivethirtyeight')"]}, {"block": 10, "type": "code", "linesLength": 21, "startIndex": 114, "lines": ["# Set up the plotting layout\n", "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize = (15,10))\n", "fig.autofmt_xdate(rotation = 45)\n", "\n", "# Actual max temperature measurement\n", "ax1.plot(dates, features['actual'])\n", "ax1.set_xlabel(''); ax1.set_ylabel('Temperature (F)'); ax1.set_title('Max Temp')\n", "\n", "# Temperature from 1 day ago\n", "ax2.plot(dates, features['temp_1'])\n", "ax2.set_xlabel(''); ax2.set_ylabel('Temperature (F)'); ax2.set_title('Prior Max Temp')\n", "\n", "# Temperature from 2 days ago\n", "ax3.plot(dates, features['temp_2'])\n", "ax3.set_xlabel('Date'); ax3.set_ylabel('Temperature (F)'); ax3.set_title('Two Days Prior Max Temp')\n", "\n", "# Friend Estimate\n", "ax4.plot(dates, features['friend'])\n", "ax4.set_xlabel('Date'); ax4.set_ylabel('Temperature (F)'); ax4.set_title('Friend Estimate')\n", "\n", "plt.tight_layout(pad=2)"]}, {"block": 11, "type": "code", "linesLength": 21, "startIndex": 135, "lines": ["# Set up the plotting layout\n", "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize = (15,10))\n", "fig.autofmt_xdate(rotation = 45)\n", "\n", "# Historical Average Max Temp\n", "ax1.plot(dates, features['average'])\n", "ax1.set_xlabel(''); ax1.set_ylabel('Temperature (F)'); ax1.set_title('Historical Avg Max Temp')\n", "\n", "# Prior Avg Wind Speed \n", "ax2.plot(dates, features['ws_1'], 'r-')\n", "ax2.set_xlabel(''); ax2.set_ylabel('Wind Speed (mph)'); ax2.set_title('Prior Wind Speed')\n", "\n", "# Prior Precipitation\n", "ax3.plot(dates, features['prcp_1'], 'r-')\n", "ax3.set_xlabel('Date'); ax3.set_ylabel('Precipitation (in)'); ax3.set_title('Prior Precipitation')\n", "\n", "# Prior Snowdepth\n", "ax4.plot(dates, features['snwd_1'], 'ro')\n", "ax4.set_xlabel('Date'); ax4.set_ylabel('Snow Depth (in)'); ax4.set_title('Prior Snow Depth')\n", "\n", "plt.tight_layout(pad=2)"]}, {"block": 12, "type": "markdown", "linesLength": 3, "startIndex": 156, "lines": ["## Pairplots\n", "\n", "One of my favorite graphs to make is a pairplot which shows all the relationships between variables in a dataset. We can do this with the seaborn library and examine the plots to see which variables are highly correlated. We would suspect those that are more correlated with max temperature would be more useful for prediction."]}, {"block": 13, "type": "code", "linesLength": 16, "startIndex": 159, "lines": ["# Create columns of seasons for pair plotting colors\n", "seasons = []\n", "\n", "for month in features['month']:\n", "    if month in [1, 2, 12]:\n", "        seasons.append('winter')\n", "    elif month in [3, 4, 5]:\n", "        seasons.append('spring')\n", "    elif month in [6, 7, 8]:\n", "        seasons.append('summer')\n", "    elif month in [9, 10, 11]:\n", "        seasons.append('fall')\n", "\n", "# Will only use six variables for plotting pairs\n", "reduced_features = features[['temp_1', 'prcp_1', 'average', 'actual']]\n", "reduced_features['season'] = seasons"]}, {"block": 14, "type": "code", "linesLength": 10, "startIndex": 175, "lines": ["# Use seaborn for pair plots\n", "import seaborn as sns\n", "sns.set(style=\"ticks\", color_codes=True);\n", "\n", "# Create a custom color palete\n", "palette = sns.xkcd_palette(['dark blue', 'dark green', 'gold', 'orange'])\n", "\n", "# Make the pair plot with a some aesthetic changes\n", "sns.pairplot(reduced_features, hue = 'season', diag_kind = 'kde', palette= palette, plot_kws=dict(alpha = 0.7),\n", "                   diag_kws=dict(shade=True)); "]}, {"block": 15, "type": "markdown", "linesLength": 1, "startIndex": 185, "lines": ["## Data Preparation"]}, {"block": 16, "type": "code", "linesLength": 21, "startIndex": 186, "lines": ["# One Hot Encoding\n", "features = pd.get_dummies(features)\n", "\n", "# Extract features and labels\n", "labels = features['actual']\n", "features = features.drop('actual', axis = 1)\n", "\n", "# List of features for later use\n", "feature_list = list(features.columns)\n", "\n", "# Convert to numpy arrays\n", "import numpy as np\n", "\n", "features = np.array(features)\n", "labels = np.array(labels)\n", "\n", "# Training and Testing Sets\n", "from sklearn.model_selection import train_test_split\n", "\n", "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, \n", "                                                                            test_size = 0.25, random_state = 42)"]}, {"block": 17, "type": "code", "linesLength": 4, "startIndex": 207, "lines": ["print('Training Features Shape:', train_features.shape)\n", "print('Training Labels Shape:', train_labels.shape)\n", "print('Testing Features Shape:', test_features.shape)\n", "print('Testing Labels Shape:', test_labels.shape)"]}, {"block": 18, "type": "markdown", "linesLength": 3, "startIndex": 211, "lines": ["## Establish New Baseline\n", "\n", "The new baseline will be the predictions of the model trained on only one year of data but tested on the expanded testing set. In order to make predictions, we will need to restrict the features to those in the original one year of data (exclude the wind speed, precipitation, and snow depth). Testing with the same test set allows us to assess the effect of using additional training data. "]}, {"block": 19, "type": "code", "linesLength": 23, "startIndex": 214, "lines": ["# Find the original feature indices \n", "original_feature_indices = [feature_list.index(feature) for feature in\n", "                                      feature_list if feature not in\n", "                                      ['ws_1', 'prcp_1', 'snwd_1']]\n", "\n", "# Create a test set of the original features\n", "original_test_features = test_features[:, original_feature_indices]\n", "\n", "# Make predictions on test data using the model trained on original data\n", "baseline_predictions = rf.predict(original_test_features)\n", "\n", "# Performance metrics\n", "baseline_errors = abs(baseline_predictions - test_labels)\n", "\n", "print('Metrics for Random Forest Trained on Original Data')\n", "print('Average absolute error:', round(np.mean(baseline_errors), 2), 'degrees.')\n", "\n", "# Calculate mean absolute percentage error (MAPE)\n", "baseline_mape = 100 * np.mean((baseline_errors / test_labels))\n", "\n", "# Calculate and display accuracy\n", "baseline_accuracy = 100 - baseline_mape\n", "print('Accuracy:', round(baseline_accuracy, 2), '%.')"]}, {"block": 20, "type": "markdown", "linesLength": 3, "startIndex": 237, "lines": ["## Train on Expanded Data and Features\n", "\n", "The rf_exp uses the same number of decision trees (n_estimators) but is trained on the longer dataset with 3 additional features."]}, {"block": 21, "type": "code", "linesLength": 5, "startIndex": 240, "lines": ["# Instantiate random forest and train on new features\n", "from sklearn.ensemble import RandomForestRegressor\n", "\n", "rf_exp = RandomForestRegressor(n_estimators= 1000, random_state=42)\n", "rf_exp.fit(train_features, train_labels);"]}, {"block": 22, "type": "markdown", "linesLength": 1, "startIndex": 245, "lines": ["## Metrics for Expanded Data and Features"]}, {"block": 23, "type": "code", "linesLength": 19, "startIndex": 246, "lines": ["# Make predictions on test data\n", "predictions = rf_exp.predict(test_features)\n", "\n", "# Performance metrics\n", "errors = abs(predictions - test_labels)\n", "\n", "print('Metrics for Random Forest Trained on Expanded Data')\n", "print('Average absolute error:', round(np.mean(errors), 4), 'degrees.')\n", "\n", "# Calculate mean absolute percentage error (MAPE)\n", "mape = np.mean(100 * (errors / test_labels))\n", "\n", "# Compare to baseline\n", "improvement_baseline = 100 * abs(mape - baseline_mape) / baseline_mape\n", "print('Improvement over baseline:', round(improvement_baseline, 2), '%.')\n", "\n", "# Calculate and display accuracy\n", "accuracy = 100 - mape\n", "print('Accuracy:', round(accuracy, 2), '%.')"]}, {"block": 24, "type": "markdown", "linesLength": 4, "startIndex": 265, "lines": ["Using more data (more data points and more features) has decreased the absolute error, increased performance relative to the baseline, and increased accuracy.\n", "Although the exact metrics will depend on the random state used, overall, we can be confident using additional high-quality data improves our model.\n", "\n", "At this point, our model can predict the maximum temperature for tomrrow with an average error of __3.7__ degrees resulting in an accuracy of __93.7%__."]}, {"block": 25, "type": "markdown", "linesLength": 3, "startIndex": 269, "lines": ["# Feature Reduction\n", "\n", "From previous experience and the graphs produced at the beginning, we know that some features are not useful for our temperature prediction problem. To reduce the number of features, which will reduce runtime, hopefully without significantly reducing performance, we can examine the feature importances from the random forest."]}, {"block": 26, "type": "markdown", "linesLength": 1, "startIndex": 272, "lines": ["### Feature Importances"]}, {"block": 27, "type": "code", "linesLength": 11, "startIndex": 273, "lines": ["# Get numerical feature importances\n", "importances = list(rf_exp.feature_importances_)\n", "\n", "# List of tuples with variable and importance\n", "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n", "\n", "# Sort the feature importances by most important first\n", "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n", "\n", "# Print out the feature and importances \n", "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"]}, {"block": 28, "type": "markdown", "linesLength": 1, "startIndex": 284, "lines": ["#### Visualize Feature Importances"]}, {"block": 29, "type": "code", "linesLength": 14, "startIndex": 285, "lines": ["# Reset style \n", "plt.style.use('fivethirtyeight')\n", "\n", "# list of x locations for plotting\n", "x_values = list(range(len(importances)))\n", "\n", "# Make a bar chart\n", "plt.bar(x_values, importances, orientation = 'vertical', color = 'r', edgecolor = 'k', linewidth = 1.2)\n", "\n", "# Tick labels for x axis\n", "plt.xticks(x_values, feature_list, rotation='vertical')\n", "\n", "# Axis labels and title\n", "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');"]}, {"block": 30, "type": "code", "linesLength": 18, "startIndex": 299, "lines": ["# List of features sorted from most to least important\n", "sorted_importances = [importance[1] for importance in feature_importances]\n", "sorted_features = [importance[0] for importance in feature_importances]\n", "\n", "# Cumulative importances\n", "cumulative_importances = np.cumsum(sorted_importances)\n", "\n", "# Make a line graph\n", "plt.plot(x_values, cumulative_importances, 'g-')\n", "\n", "# Draw line at 95% of importance retained\n", "plt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n", "\n", "# Format x ticks and labels\n", "plt.xticks(x_values, sorted_features, rotation = 'vertical')\n", "\n", "# Axis labels and title\n", "plt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');"]}, {"block": 31, "type": "markdown", "linesLength": 4, "startIndex": 317, "lines": ["### Limit Number of Features \n", "\n", "We will now reduce the number of features in use by the model to only those required to account for 95% of the importance. \n", "The same number of features must be used in the training and testing sets."]}, {"block": 32, "type": "code", "linesLength": 3, "startIndex": 321, "lines": ["# Find number of features for cumulative importance of 95%\n", "# Add 1 because Python is zero-indexed\n", "print('Number of features for 95% importance:', np.where(cumulative_importances > 0.95)[0][0] + 1)"]}, {"block": 33, "type": "code", "linesLength": 12, "startIndex": 324, "lines": ["# Extract the names of the most important features\n", "important_feature_names = [feature[0] for feature in feature_importances[0:6]]\n", "# Find the columns of the most important features\n", "important_indices = [feature_list.index(feature) for feature in important_feature_names]\n", "\n", "# Create training and testing sets with only the important features\n", "important_train_features = train_features[:, important_indices]\n", "important_test_features = test_features[:, important_indices]\n", "\n", "# Sanity check on operations\n", "print('Important train features shape:', important_train_features.shape)\n", "print('Important test features shape:', important_test_features.shape)\n"]}, {"block": 34, "type": "markdown", "linesLength": 1, "startIndex": 336, "lines": ["### Training on Important Features"]}, {"block": 35, "type": "code", "linesLength": 2, "startIndex": 337, "lines": ["# Train the expanded model on only the important features\n", "rf_exp.fit(important_train_features, train_labels);"]}, {"block": 36, "type": "markdown", "linesLength": 1, "startIndex": 339, "lines": ["### Evaluate on Important features"]}, {"block": 37, "type": "code", "linesLength": 14, "startIndex": 340, "lines": ["# Make predictions on test data\n", "predictions = rf_exp.predict(important_test_features)\n", "\n", "# Performance metrics\n", "errors = abs(predictions - test_labels)\n", "\n", "print('Average absolute error:', round(np.mean(errors), 4), 'degrees.')\n", "\n", "# Calculate mean absolute percentage error (MAPE)\n", "mape = 100 * (errors / test_labels)\n", "\n", "# Calculate and display accuracy\n", "accuracy = 100 - np.mean(mape)\n", "print('Accuracy:', round(accuracy, 2), '%.')"]}, {"block": 38, "type": "markdown", "linesLength": 1, "startIndex": 354, "lines": ["Using only the eight most important features (instead of all 16) results in a minor decrease in accuracy by 0.17 degrees. For some models, decreasing the number of features can increase performance and therefore should be done. However, in other situations, performance will decrease but run time will also decrease. The final decision on how many features to retain will therefore be a trade-off between accuracy and run time. "]}, {"block": 39, "type": "markdown", "linesLength": 1, "startIndex": 355, "lines": ["## Compare Trade-Offs"]}, {"block": 40, "type": "code", "linesLength": 16, "startIndex": 356, "lines": ["# Use time library for run time evaluation\n", "import time\n", "\n", "# All features training and testing time\n", "all_features_time = []\n", "\n", "# Do 10 iterations and take average for all features\n", "for _ in range(10):\n", "    start_time = time.time()\n", "    rf_exp.fit(train_features, train_labels)\n", "    all_features_predictions = rf_exp.predict(test_features)\n", "    end_time = time.time()\n", "    all_features_time.append(end_time - start_time)\n", "\n", "all_features_time = np.mean(all_features_time)\n", "print('All features total training and testing time:', round(all_features_time, 2), 'seconds.')"]}, {"block": 41, "type": "code", "linesLength": 13, "startIndex": 372, "lines": ["# Time training and testing for reduced feature set\n", "reduced_features_time = []\n", "\n", "# Do 10 iterations and take average\n", "for _ in range(10):\n", "    start_time = time.time()\n", "    rf_exp.fit(important_train_features, train_labels)\n", "    reduced_features_predictions = rf_exp.predict(important_test_features)\n", "    end_time = time.time()\n", "    reduced_features_time.append(end_time - start_time)\n", "\n", "reduced_features_time = np.mean(reduced_features_time)\n", "print('Reduced features total training and testing time:', round(reduced_features_time, 2), 'seconds.')"]}, {"block": 42, "type": "markdown", "linesLength": 1, "startIndex": 385, "lines": ["### Accuracy vs Run-Time"]}, {"block": 43, "type": "code", "linesLength": 8, "startIndex": 386, "lines": ["all_accuracy =  100 * (1- np.mean(abs(all_features_predictions - test_labels) / test_labels))\n", "reduced_accuracy = 100 * (1- np.mean(abs(reduced_features_predictions - test_labels) / test_labels))\n", "\n", "comparison = pd.DataFrame({'features': ['all (17)', 'reduced (5)'], \n", "                           'run_time': [round(all_features_time, 2), round(reduced_features_time, 2)],\n", "                           'accuracy': [round(all_accuracy, 2), round(reduced_accuracy, 2)]})\n", "\n", "comparison[['features', 'accuracy', 'run_time']]"]}, {"block": 44, "type": "code", "linesLength": 5, "startIndex": 394, "lines": ["relative_accuracy_decrease = 100 * (all_accuracy - reduced_accuracy) / all_accuracy\n", "print('Relative decrease in accuracy:', round(relative_accuracy_decrease, 3), '%.')\n", "\n", "relative_runtime_decrease = 100 * (all_features_time - reduced_features_time) / all_features_time\n", "print('Relative decrease in run time:', round(relative_runtime_decrease, 3), '%.')"]}, {"block": 45, "type": "markdown", "linesLength": 1, "startIndex": 399, "lines": ["# Concluding Graphs"]}, {"block": 46, "type": "code", "linesLength": 20, "startIndex": 400, "lines": ["# Find the original feature indices \n", "original_feature_indices = [feature_list.index(feature) for feature in\n", "                                      feature_list if feature not in\n", "                                      ['ws_1', 'prcp_1', 'snwd_1']]\n", "\n", "# Create a test set of the original features\n", "original_test_features = test_features[:, original_feature_indices]\n", "\n", "# Time to train on original data set (1 year)\n", "original_features_time = []\n", "\n", "# Do 10 iterations and take average for all features\n", "for _ in range(10):\n", "    start_time = time.time()\n", "    rf.fit(original_train_features, original_train_labels)\n", "    original_features_predictions = rf.predict(original_test_features)\n", "    end_time = time.time()\n", "    original_features_time.append(end_time - start_time)\n", "    \n", "original_features_time = np.mean(original_features_time)"]}, {"block": 47, "type": "code", "linesLength": 16, "startIndex": 420, "lines": ["# Calculate mean absolute error for each model\n", "original_mae = np.mean(abs(original_features_predictions - test_labels))\n", "exp_all_mae = np.mean(abs(all_features_predictions - test_labels))\n", "exp_reduced_mae = np.mean(abs(reduced_features_predictions - test_labels))\n", "\n", "# Calculate accuracy for model trained on 1 year of data\n", "original_accuracy = 100 * (1 - np.mean(abs(original_features_predictions - test_labels) / test_labels))\n", "\n", "# Create a dataframe for comparison\n", "model_comparison = pd.DataFrame({'model': ['original', 'exp_all', 'exp_reduced'], \n", "                                 'error (degrees)':  [original_mae, exp_all_mae, exp_reduced_mae],\n", "                                 'accuracy': [original_accuracy, all_accuracy, reduced_accuracy],\n", "                                 'run_time (s)': [original_features_time, all_features_time, reduced_features_time]})\n", "\n", "# Order the dataframe\n", "model_comparison = model_comparison[['model', 'error (degrees)', 'accuracy', 'run_time (s)']]"]}, {"block": 48, "type": "markdown", "linesLength": 3, "startIndex": 436, "lines": ["### Comparison of Three Models\n", "\n", "There are three models to compare: the model trained on a single year of data and the original number of features, the model trained on six years of data with the full set of features, and the model trained on six years of data with only the 6 most important features"]}, {"block": 49, "type": "code", "linesLength": 1, "startIndex": 439, "lines": ["model_comparison"]}, {"block": 50, "type": "code", "linesLength": 30, "startIndex": 440, "lines": ["# Make plots \n", "# Set up the plotting layout\n", "fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, figsize = (8,16), sharex = True)\n", "\n", "# Set up x-axis\n", "x_values = [0, 1, 2]\n", "labels = list(model_comparison['model'])\n", "plt.xticks(x_values, labels)\n", "\n", "# Set up fonts\n", "fontdict = {'fontsize': 18}\n", "fontdict_yaxis = {'fontsize': 14}\n", "\n", "# Error Comparison\n", "ax1.bar(x_values, model_comparison['error (degrees)'], color = ['b', 'r', 'g'], edgecolor = 'k', linewidth = 1.5)\n", "ax1.set_ylim(bottom = 3.5, top = 4.5)\n", "ax1.set_ylabel('Error (degrees) (F)', fontdict = fontdict_yaxis); \n", "ax1.set_title('Model Error Comparison', fontdict= fontdict)\n", "\n", "# Accuracy Comparison\n", "ax2.bar(x_values, model_comparison['accuracy'], color = ['b', 'r', 'g'], edgecolor = 'k', linewidth = 1.5)\n", "ax2.set_ylim(bottom = 92, top = 94)\n", "ax2.set_ylabel('Accuracy (%)', fontdict = fontdict_yaxis); \n", "ax2.set_title('Model Accuracy Comparison', fontdict= fontdict)\n", "\n", "# Run Time Comparison\n", "ax3.bar(x_values, model_comparison['run_time (s)'], color = ['b', 'r', 'g'], edgecolor = 'k', linewidth = 1.5)\n", "ax3.set_ylim(bottom = 2, top = 12)\n", "ax3.set_ylabel('Run Time (sec)', fontdict = fontdict_yaxis); \n", "ax3.set_title('Model Run-Time Comparison', fontdict= fontdict);\n"]}, {"block": 51, "type": "markdown", "linesLength": 1, "startIndex": 470, "lines": ["Check in on [my blog](https://medium.com/@williamkoehrsen) for more data science/machine learning articles. Additional parts of this machine learning exercise will be out shortly. I appreciate any comments and constructive feedback.  "]}]