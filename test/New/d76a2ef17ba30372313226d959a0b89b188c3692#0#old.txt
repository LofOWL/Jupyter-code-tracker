[{"block": 0, "type": "code", "linesLength": 9, "startIndex": 0, "lines": ["import pandas as pd\n", "import numpy as np\n", "\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline \n", "\n", "import seaborn as sns\n", "\n", "import pymc3 as pm"]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 9, "lines": ["# Load in Exercise Data"]}, {"block": 2, "type": "code", "linesLength": 5, "startIndex": 10, "lines": ["exercise = pd.read_csv('data/exercise.csv')\n", "calories = pd.read_csv('data/calories.csv')\n", "df = pd.merge(exercise, calories, on = 'User_ID')\n", "df['Intercept'] = 1\n", "df.head()"]}, {"block": 3, "type": "markdown", "linesLength": 1, "startIndex": 15, "lines": ["# Plot Relationship"]}, {"block": 4, "type": "code", "linesLength": 2, "startIndex": 16, "lines": ["plt.plot(df['Duration'], df['Calories'], 'bo');\n", "plt.xlabel('Duration (min'); plt.ylabel('Calories'); plt.title('Calories burned vs Duration of Exercise');"]}, {"block": 5, "type": "code", "linesLength": 3, "startIndex": 18, "lines": ["# Create the features and response\n", "X = df.loc[:, ['Intercept', 'Duration']]\n", "y = df.ix[:, 'Calories']"]}, {"block": 6, "type": "markdown", "linesLength": 1, "startIndex": 21, "lines": ["# Implementing Linear Regression by Hand"]}, {"block": 7, "type": "code", "linesLength": 6, "startIndex": 22, "lines": ["# Takes a matrix of features (with intercept as first column) \n", "# and response vector and calculates linear regression coefficients\n", "def linear_regression(X, y):\n", "    # Equation for linear regression coefficients\n", "    beta = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T), y)\n", "    return beta"]}, {"block": 8, "type": "code", "linesLength": 4, "startIndex": 28, "lines": ["# Run the by hand implementation\n", "by_hand_coefs = linear_regression(X, y)\n", "print('Intercept calculated by hand:', by_hand_coefs[0])\n", "print('Slope calculated by hand: ', by_hand_coefs[1])"]}, {"block": 9, "type": "markdown", "linesLength": 1, "startIndex": 32, "lines": ["# Verify with Sklearn Implementation"]}, {"block": 10, "type": "code", "linesLength": 7, "startIndex": 33, "lines": ["from sklearn.linear_model import LinearRegression\n", "\n", "# Create the model and fit on the data\n", "lr = LinearRegression()\n", "lr.fit(X.Duration.reshape(-1, 1), y)\n", "print('Intercept from library:', lr.intercept_)\n", "print('Slope from library:', lr.coef_)"]}, {"block": 11, "type": "code", "linesLength": 0, "startIndex": 40, "lines": []}, {"block": 12, "type": "markdown", "linesLength": 5, "startIndex": 40, "lines": ["# Bayesian Linear Regression\n", "\n", "## Using PyMC3 and N-UTurn Sampling (NUTS)\n", "\n", "Implement MCMC to find the posterior distribution of the model parameters. Rather than a single point estimate of the model weights, Bayesian linear regression will give us a posterior distribution for the model weights."]}, {"block": 13, "type": "code", "linesLength": 3, "startIndex": 45, "lines": ["# Dataframe with response included\n", "X_with_labels = X.copy()\n", "X_with_labels['Calories'] = y.values"]}, {"block": 14, "type": "code", "linesLength": 6, "startIndex": 48, "lines": ["with pm.Model() as linear_model:\n", "    # Use the formula syntax to specify equation\n", "    pm.GLM.from_formula('Calories ~ Duration', data = X_with_labels, family = pm.families.Normal())\n", "    \n", "    # Sample from the posterior 1500 times with 500 burn-in steps \n", "    linear_trace = pm.sample(draws=1500, tune=500, chains=2, njobs=-1)"]}, {"block": 15, "type": "markdown", "linesLength": 1, "startIndex": 54, "lines": ["# Bayesian Model Results"]}, {"block": 16, "type": "markdown", "linesLength": 1, "startIndex": 55, "lines": ["## Trace of All Model Parameters"]}, {"block": 17, "type": "code", "linesLength": 1, "startIndex": 56, "lines": ["pm.traceplot(linear_trace);"]}, {"block": 18, "type": "markdown", "linesLength": 1, "startIndex": 57, "lines": ["## Posterior Distribution of Model Parameteres"]}, {"block": 19, "type": "code", "linesLength": 1, "startIndex": 58, "lines": ["pm.plot_posterior(linear_trace);"]}, {"block": 20, "type": "markdown", "linesLength": 1, "startIndex": 59, "lines": ["## Confidence Intervals for Model Parameters"]}, {"block": 21, "type": "code", "linesLength": 1, "startIndex": 60, "lines": ["pm.forestplot(linear_trace);"]}, {"block": 22, "type": "markdown", "linesLength": 3, "startIndex": 61, "lines": ["# Predictions of Response Sampled from the Posterior\n", "\n", "We can now generate predictions of the linear regression line using the model results. The following plot shows 1000 different estimates of the regression line drawn from the posterior. The distribution of the lines gives an estimate of the uncertainty in the estimate. Bayesian Linear Regression has the benefit that it gives us a posterior __distribution__ rather than a __single point estimate__ in the frequentist ordinary least squares regression."]}, {"block": 23, "type": "code", "linesLength": 1, "startIndex": 64, "lines": ["linear_trace['Intercept'].mean()"]}, {"block": 24, "type": "code", "linesLength": 1, "startIndex": 65, "lines": ["linear_trace['Duration'].mean()"]}, {"block": 25, "type": "code", "linesLength": 2, "startIndex": 66, "lines": ["pm.plot_posterior_predictive_glm(linear_trace, samples = 1000, eval=np.linspace(0, 30, 100),\n", "                                lm = lambda x, sample: sample['Intercept'] + sample['Duration'] * x);"]}, {"block": 26, "type": "code", "linesLength": 1, "startIndex": 68, "lines": ["pm.df_summary(linear_trace)"]}, {"block": 27, "type": "code", "linesLength": 0, "startIndex": 69, "lines": []}]