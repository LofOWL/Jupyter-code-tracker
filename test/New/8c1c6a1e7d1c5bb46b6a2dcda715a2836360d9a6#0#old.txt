[{"block": 0, "type": "markdown", "linesLength": 2, "startIndex": 0, "lines": ["<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n", "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Problem Statement</a></span></li><li><span><a href=\"#Approach\" data-toc-modified-id=\"Approach-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Approach</a></span><ul class=\"toc-item\"><li><span><a href=\"#Library-Imports\" data-toc-modified-id=\"Library-Imports-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Library Imports</a></span></li></ul></li><li><span><a href=\"#Helper-Visualization-Functions\" data-toc-modified-id=\"Helper-Visualization-Functions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Helper Visualization Functions</a></span></li></ul></li><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataset</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Impute-Missing-Values\" data-toc-modified-id=\"Impute-Missing-Values-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Impute Missing Values</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training-and-Testing-Sets\" data-toc-modified-id=\"Training-and-Testing-Sets-2.0.1.1\"><span class=\"toc-item-num\">2.0.1.1&nbsp;&nbsp;</span>Training and Testing Sets</a></span></li></ul></li><li><span><a href=\"#Naive-Baseline\" data-toc-modified-id=\"Naive-Baseline-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Naive Baseline</a></span></li></ul></li></ul></li><li><span><a href=\"#Evaluate-Standard-Machine-Learning-Methods\" data-toc-modified-id=\"Evaluate-Standard-Machine-Learning-Methods-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Evaluate Standard Machine Learning Methods</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Logistic Regression</a></span></li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Random Forest</a></span></li><li><span><a href=\"#Support-Vector-Machine-Classifier\" data-toc-modified-id=\"Support-Vector-Machine-Classifier-3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span>Support Vector Machine Classifier</a></span></li></ul></li><li><span><a href=\"#Conclusions-from-Machine-Learning-Models\" data-toc-modified-id=\"Conclusions-from-Machine-Learning-Models-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Conclusions from Machine Learning Models</a></span></li></ul></li><li><span><a href=\"#Bayesian-Logistic-Regression\" data-toc-modified-id=\"Bayesian-Logistic-Regression-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Bayesian Logistic Regression</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Formula-for-Logistic-Regression\" data-toc-modified-id=\"Formula-for-Logistic-Regression-4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>Formula for Logistic Regression</a></span></li></ul></li><li><span><a href=\"#Implementing-the-Logistic-Model\" data-toc-modified-id=\"Implementing-the-Logistic-Model-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Implementing the Logistic Model</a></span></li></ul></li><li><span><a href=\"#Interpret-Results\" data-toc-modified-id=\"Interpret-Results-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Interpret Results</a></span><ul class=\"toc-item\"><li><span><a href=\"#Traceplot-of-Sampled-Variables\" data-toc-modified-id=\"Traceplot-of-Sampled-Variables-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Traceplot of Sampled Variables</a></span></li><li><span><a href=\"#Forest-Plot-of-Sampled-Variables\" data-toc-modified-id=\"Forest-Plot-of-Sampled-Variables-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Forest Plot of Sampled Variables</a></span></li><li><span><a href=\"#Posterior-Plot-of-the-Variables\" data-toc-modified-id=\"Posterior-Plot-of-the-Variables-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Posterior Plot of the Variables</a></span></li><li><span><a href=\"#Summary-Statistics\" data-toc-modified-id=\"Summary-Statistics-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Summary Statistics</a></span></li><li><span><a href=\"#Equation-from-Model\" data-toc-modified-id=\"Equation-from-Model-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Equation from Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Compare-to-Standard-Logistic-Regression\" data-toc-modified-id=\"Compare-to-Standard-Logistic-Regression-5.5.1\"><span class=\"toc-item-num\">5.5.1&nbsp;&nbsp;</span>Compare to Standard Logistic Regression</a></span></li></ul></li></ul></li><li><span><a href=\"#Model-Effects-of-Variables\" data-toc-modified-id=\"Model-Effects-of-Variables-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model Effects of Variables</a></span></li><li><span><a href=\"#Model-Metrics\" data-toc-modified-id=\"Model-Metrics-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Model Metrics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Calculating-Metrics\" data-toc-modified-id=\"Calculating-Metrics-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Calculating Metrics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Accuracy-and-F1-Score\" data-toc-modified-id=\"Accuracy-and-F1-Score-7.1.1\"><span class=\"toc-item-num\">7.1.1&nbsp;&nbsp;</span>Accuracy and F1 Score</a></span></li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-7.1.2\"><span class=\"toc-item-num\">7.1.2&nbsp;&nbsp;</span>Confusion Matrix</a></span></li><li><span><a href=\"#AUC-of-the-ROC\" data-toc-modified-id=\"AUC-of-the-ROC-7.1.3\"><span class=\"toc-item-num\">7.1.3&nbsp;&nbsp;</span>AUC of the ROC</a></span></li></ul></li><li><span><a href=\"#Comparison-to-Standard-Machine-Learning-Models\" data-toc-modified-id=\"Comparison-to-Standard-Machine-Learning-Models-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Comparison to Standard Machine Learning Models</a></span></li></ul></li><li><span><a href=\"#Model-Predictions\" data-toc-modified-id=\"Model-Predictions-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Model Predictions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Calculate-Predictions\" data-toc-modified-id=\"Calculate-Predictions-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Calculate Predictions</a></span></li><li><span><a href=\"#Examine-Specific-Patient-Predictions\" data-toc-modified-id=\"Examine-Specific-Patient-Predictions-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Examine Specific Patient Predictions</a></span></li></ul></li><li><span><a href=\"#Investigate-Wrong-Classifications\" data-toc-modified-id=\"Investigate-Wrong-Classifications-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Investigate Wrong Classifications</a></span></li><li><span><a href=\"#Logistic-Regression-Model-Using-Two-Variables\" data-toc-modified-id=\"Logistic-Regression-Model-Using-Two-Variables-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Logistic Regression Model Using Two Variables</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Posterior-Distribution\" data-toc-modified-id=\"Posterior-Distribution-10.0.1\"><span class=\"toc-item-num\">10.0.1&nbsp;&nbsp;</span>Posterior Distribution</a></span></li><li><span><a href=\"#Effects-of-Variables\" data-toc-modified-id=\"Effects-of-Variables-10.0.2\"><span class=\"toc-item-num\">10.0.2&nbsp;&nbsp;</span>Effects of Variables</a></span></li></ul></li><li><span><a href=\"#Evaluate-Performance\" data-toc-modified-id=\"Evaluate-Performance-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Evaluate Performance</a></span></li></ul></li><li><span><a href=\"#Visualizing-Decision-Boundary\" data-toc-modified-id=\"Visualizing-Decision-Boundary-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Visualizing Decision Boundary</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot-of-Data-by-Outcome\" data-toc-modified-id=\"Plot-of-Data-by-Outcome-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>Plot of Data by Outcome</a></span><ul class=\"toc-item\"><li><span><a href=\"#Limitations\" data-toc-modified-id=\"Limitations-11.1.1\"><span class=\"toc-item-num\">11.1.1&nbsp;&nbsp;</span>Limitations</a></span></li></ul></li></ul></li><li><span><a href=\"#Future-Work\" data-toc-modified-id=\"Future-Work-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Future Work</a></span></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Conclusions</a></span></li><li><span><a href=\"#Sources\" data-toc-modified-id=\"Sources-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Sources</a></span></li></ul></div>"]}, {"block": 1, "type": "markdown", "linesLength": 22, "startIndex": 2, "lines": ["# Introduction\n", "\n", "In this notebook, we will explore using Bayesian Logistic Regression in order to predict whether or not a patient has diabetes. The task is supervised - we are given labeled training data to fit our model - and it is a classification task because the lables are binary. Typically this is a task that is approached with supervised machine learning techniques such as logistic regression, support vector machines, or tree based methods. We will take a slightly different approach and use a Bayesian Framework to fit a logistic regression model and then intrepret the resulting model parameters.\n", "\n", "## Problem Statement\n", "\n", "Given a set of eight medical characteristics about a patients, predict whether or not the patient has diabetes. To accomplish this task, we will use a set of real-world data collected on females 21 years of age and over collected by a national health institution in the United States.  \n", "\n", "## Approach\n", "\n", "The goal is to implement the Bayesian form of Logistic Regression. This is done by setting up a model relating the inputs (features) to the outputs (response) and then sampling from the posteriors for the model parameters. In the case of logistic regression, the model parameters are the weights on the features. The end result is a distribution (trace) of all the model parameter values that are drawn from the posterior. The procedure will be implemented in the Python library `PyMC3`, which is used for Bayesian inference and has a number of Markov Chain Monte Carlo implementations for sampling from the posterior. The basic outline is as follows:\n", "\n", "1. Exploratory Data Analysis (in previous notebook)\n", "2. Split data into training and testing sets\n", "3. Establish naive benchmark \n", "4. Evaluate several standard machine learning approaches for performance\n", "5. Define the model formula and set up the model using the `pm.GLM.from_formula` function\n", "6. Sample from the posterior for the model parameters using the `pm.sample` function which will use the NUTS sampler\n", "7. Interpret the parameter traces\n", "8. Use the parameter traces to make predictions\n", "9. Compare the performance of the logistic regression model to the standard machine learning methods\n", "10. Draw conclusions and outline future work opportunities"]}, {"block": 2, "type": "markdown", "linesLength": 1, "startIndex": 24, "lines": ["### Library Imports"]}, {"block": 3, "type": "code", "linesLength": 3, "startIndex": 25, "lines": ["# Pandas and numpy for data manipulation\n", "import pandas as pd\n", "import numpy as np"]}, {"block": 4, "type": "code", "linesLength": 27, "startIndex": 28, "lines": ["# Matplotlib and seaborn for visualizations\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "from IPython.core.pylabtools import figsize\n", "import matplotlib.lines as mlines\n", "\n", "import seaborn as sns\n", "\n", "# Standard machine learning models\n", "from sklearn.linear_model import LogisticRegressionCV\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.svm import SVC\n", "\n", "# Scikit-learn utilities\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, roc_curve\n", "\n", "import itertools\n", "\n", "pd.options.mode.chained_assignment = None\n", "\n", "import matplotlib as mpl\n", "\n", "from warnings import filterwarnings\n", "filterwarnings('ignore', category=mpl.MatplotlibDeprecationWarning)"]}, {"block": 5, "type": "code", "linesLength": 2, "startIndex": 55, "lines": ["# PyMC3 for Bayesian Inference\n", "import pymc3 as pm"]}, {"block": 6, "type": "markdown", "linesLength": 1, "startIndex": 57, "lines": ["## Helper Visualization Functions"]}, {"block": 7, "type": "code", "linesLength": 13, "startIndex": 58, "lines": ["# Shows the trace with a vertical line at the mean of the trace\n", "def plot_trace(trace):\n", "    # Traceplot with vertical lines at the mean value\n", "    ax = pm.traceplot(trace, figsize=(12, len(trace.varnames)*1.5),\n", "                      lines={k: v['mean'] for k, v in pm.summary(trace).iterrows()})\n", "    \n", "    plt.rcParams['font.size'] = 12\n", "    # Labels with the mean value\n", "    for i, mn in enumerate(pm.summary(trace)['mean']):\n", "        ax[i, 0].annotate('{:0.2f}'.format(mn), xy = (mn, 0), xycoords = 'data', size = 8,\n", "                          xytext = (5, 10), textcoords = 'offset points', rotation = 90,\n", "                          va = 'bottom', fontsize = 'large', color = 'red')\n", "        "]}, {"block": 8, "type": "code", "linesLength": 41, "startIndex": 71, "lines": ["# Visualize a confusion matrix as a plot given the confusion matrix\n", "def plot_confusion_matrix(cm, classes = ['No Diabetes', 'Diabetes'],\n", "                          title='Diabetes Confusion matrix',\n", "                          cmap=plt.cm.Reds):\n", "    \n", "    # Display the matrix in text form\n", "    print('Confusion matrix')\n", "    print(cm)\n", "    figsize(8, 8)\n", "    \n", "    # Show the matrix using the imshow functionality\n", "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n", "    plt.title(title, size = 20)\n", "    \n", "    # Tick marks show classes\n", "    tick_marks = np.arange(len(classes))\n", "    plt.xticks(tick_marks, classes, rotation=45, size = 12)\n", "    plt.yticks(tick_marks, classes, rotation = 90, size = 12)\n", "\n", "    # Formatting for text labels on plot\n", "    fmt1 = 's'\n", "    fmt2 = 'd'\n", "    thresh = cm.max() / 2.\n", "    \n", "    # Four types of classifications\n", "    types = [['True Negative', 'False Positive'],\n", "             ['False Negative', 'True Positive']]\n", "    \n", "    # Add the actual numbers and the types onto the heatmap plot\n", "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n", "        plt.text(j, i - 0.05, format(types[i][j], fmt1),\n", "                 horizontalalignment=\"center\", size = 18,\n", "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n", "        \n", "        plt.text(j, i + 0.15, format(cm[i, j], fmt2),\n", "                 horizontalalignment=\"center\", size = 24,\n", "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n", "\n", "    plt.tight_layout()\n", "    plt.ylabel('True label', size = 16)\n", "    plt.xlabel('Predicted Label', size = 16)"]}, {"block": 9, "type": "code", "linesLength": 7, "startIndex": 112, "lines": ["# Calculate the accuracy and f1 score of a model\n", "def calc_metrics(predictions, y_test):\n", "    accuracy = np.mean(predictions == y_test)\n", "    f1_metric = f1_score(y_test, predictions)\n", "\n", "    print('Accuracy of Model: {:.2f}%'.format(100 * accuracy))\n", "    print('F1 Score of Model: {:.4f}'.format(f1_metric))"]}, {"block": 10, "type": "code", "linesLength": 17, "startIndex": 119, "lines": ["# Determine the roc curve and the auc and display\n", "def calc_roc(probs, y_test):\n", "  # Calculate the area under the roc curve\n", "  auc = roc_auc_score(y_test, probs)\n", "  # Calculate metrics for the roc curve\n", "  fpr, tpr, thresholds = roc_curve(y_test, probs)\n", "  \n", "  plt.style.use('bmh')\n", "  plt.figure(figsize = (8, 8))\n", "  \n", "  # Plot the roc curve\n", "  plt.plot(fpr, tpr, 'b')\n", "  plt.xlabel('False Positive Rate', size = 16)\n", "  plt.ylabel('True Positive Rate', size = 16)\n", "  plt.title('Receiver Operating Characteristic Curve, AUC = %0.4f' % auc, \n", "            size = 18)\n", "  "]}, {"block": 11, "type": "markdown", "linesLength": 5, "startIndex": 136, "lines": ["# Dataset\n", "\n", "We are using the [Pima Indians diabetes dataset](https://www.kaggle.com/uciml/pima-indians-diabetes-database). The objective with this dataset is to create a binary classification model that predicts whether or not an individual has diabetes based on several medical indicators. The target variable is given as `Outcome` and takes on a value of 1 if the patient has diabetes and 0 otherwise. This is an imbalanced class problem because there are significantly more patients without diabetes than with diabetes.\n", "\n", "We did some data exploration in a previous notebook, so here we will focus on learning a model. The first step is to impute the missing values, as was discovered to be necessary in the exploration. Then, we can establish the performance of a naive baseline model where we simply predict the most common class in the training data for all observations in the testing data. We will split the data into a training and testing set randomly, with 200 observations in the test set."]}, {"block": 12, "type": "code", "linesLength": 3, "startIndex": 141, "lines": ["# Read in data and display first 5 lines\n", "data = pd.read_csv('https://raw.githubusercontent.com/WillKoehrsen/eecs-491/master/assign/project/diabetes.csv')\n", "data.head()"]}, {"block": 13, "type": "markdown", "linesLength": 3, "startIndex": 144, "lines": ["### Impute Missing Values\n", "\n", "There are several columns that have 0 values which are not physically possible. To correct these values, we can impute the zeros with the median of the column."]}, {"block": 14, "type": "code", "linesLength": 5, "startIndex": 147, "lines": ["data['Glucose'] = data['Glucose'].replace({0: data['Glucose'].median()})\n", "data['BloodPressure'] = data['BloodPressure'].replace({0: data['BloodPressure'].median()})\n", "data['SkinThickness'] = data['SkinThickness'].replace({0: data['SkinThickness'].median()})\n", "data['Insulin'] = data['Insulin'].replace({0: data['Insulin'].median()})\n", "data['BMI'] = data['BMI'].replace({0: data['BMI'].median()})"]}, {"block": 15, "type": "markdown", "linesLength": 3, "startIndex": 152, "lines": ["#### Training and Testing Sets \n", "\n", "We will be using 200 randomly selected observations for the test set. This leaves 568 observations in the training data from which we will fit our model."]}, {"block": 16, "type": "code", "linesLength": 9, "startIndex": 155, "lines": ["# Extract the features and the labels\n", "features = data.drop(columns='Outcome')\n", "labels = data.Outcome\n", "\n", "# Split into training and testing set using 200 observations for testing\n", "X, X_test, y, y_test = train_test_split(features, labels, test_size=200, random_state = 50)\n", "\n", "print(X.shape)\n", "print(X_test.shape)"]}, {"block": 17, "type": "markdown", "linesLength": 1, "startIndex": 164, "lines": ["As we can see, there are 568 training observations and 200 testing observations. We will be using 8 explanatory variables (features) for this first model."]}, {"block": 18, "type": "markdown", "linesLength": 3, "startIndex": 165, "lines": ["### Naive Baseline\n", "\n", "In order to assess if machine learning is even applicable to our problem, we must create a baseline against which to compare our results. For classification, a simple naive baseline is to to predict the most common class in the training data for all testing observations. If our model cannot beat this performance, then machine learning may not be applicable to the task or we should consider a different modeling approach."]}, {"block": 19, "type": "code", "linesLength": 1, "startIndex": 168, "lines": ["y.value_counts()"]}, {"block": 20, "type": "code", "linesLength": 3, "startIndex": 169, "lines": ["# Make naive baseline and evaluate metrics\n", "baseline_pred = [0 for _ in range(len(y_test))]\n", "calc_metrics(baseline_pred, y_test)"]}, {"block": 21, "type": "markdown", "linesLength": 1, "startIndex": 172, "lines": ["If our model cannot beat 63.50% accuracy, then we will need to rethink our approach. Granted, the baseline is not very useful because it identifies no cases of diabetes (an outcome of 1). This can be seen in the confusion matrix."]}, {"block": 22, "type": "code", "linesLength": 2, "startIndex": 173, "lines": ["cm = confusion_matrix(y_test, baseline_pred)\n", "plot_confusion_matrix(cm)"]}, {"block": 23, "type": "markdown", "linesLength": 3, "startIndex": 175, "lines": ["# Evaluate Standard Machine Learning Methods\n", "\n", "We can implement three machine learning models in order to compare performance. I will not spend any time on tuning the hyperparameters, so it is likely that we could achieve better performance through hyperparameter optimization. However, here I am more concerned with just comparing the baseline performance of these models to Bayesian Logistic Regression."]}, {"block": 24, "type": "markdown", "linesLength": 1, "startIndex": 178, "lines": ["### Logistic Regression"]}, {"block": 25, "type": "code", "linesLength": 8, "startIndex": 179, "lines": ["# Make logistic regression model and fit\n", "lr = LogisticRegressionCV(Cs= 20, cv = 3, scoring = 'f1', \n", "                          penalty = 'l2', random_state = 42)\n", "lr.fit(X, y)\n", "\n", "# Make predictions and evaluate\n", "lr_pred = lr.predict(X_test)\n", "calc_metrics(lr_pred, y_test)"]}, {"block": 26, "type": "code", "linesLength": 2, "startIndex": 187, "lines": ["lr_pred_proba = lr.predict_proba(X_test)[:, 1]\n", "calc_roc(lr_pred_proba, y_test)"]}, {"block": 27, "type": "markdown", "linesLength": 1, "startIndex": 189, "lines": ["### Random Forest"]}, {"block": 28, "type": "code", "linesLength": 7, "startIndex": 190, "lines": ["# Create and fit the random forest\n", "rf = RandomForestClassifier(n_estimators=100, random_state = 42)\n", "rf.fit(X, y)\n", "\n", "# Evaluate predictions\n", "rf_pred = rf.predict(X_test)\n", "calc_metrics(rf_pred, y_test)"]}, {"block": 29, "type": "code", "linesLength": 3, "startIndex": 197, "lines": ["# ROC for random forest\n", "rf_pred_proba = rf.predict_proba(X_test)[:, 1]\n", "calc_roc(rf_pred_proba, y_test)"]}, {"block": 30, "type": "markdown", "linesLength": 1, "startIndex": 200, "lines": ["### Support Vector Machine Classifier"]}, {"block": 31, "type": "code", "linesLength": 13, "startIndex": 201, "lines": ["# Need to scale features for support vector classifier\n", "scaler = StandardScaler()\n", "X_scaled = scaler.fit_transform(X)\n", "X_test_scaled = scaler.transform(X_test)\n", "\n", "# Create support vector classifier and fit\n", "svc = SVC(C = 10, gamma = 0.001, probability=True,\n", "          random_state = 42)\n", "svc.fit(X_scaled, y)\n", "\n", "# Make predictions and evaluate\n", "svc_pred = svc.predict(X_test_scaled)\n", "calc_metrics(svc_pred, y_test)"]}, {"block": 32, "type": "code", "linesLength": 3, "startIndex": 214, "lines": ["# Predict probabilities and calculate roc and auc\n", "svc_pred_proba = svc.predict_proba(X_test_scaled)[:, 1]\n", "calc_roc(svc_pred_proba, y_test)"]}, {"block": 33, "type": "markdown", "linesLength": 3, "startIndex": 217, "lines": ["## Conclusions from Machine Learning Models\n", "\n", "These results show that machine learning is acceptable for this task because all of the models perform better than the baseline. From the f1 score and the Area Under the Receiver Operating Characteristic Curve, the random forest performs the best. We can compare these results to the Bayesian Model we will build next. "]}, {"block": 34, "type": "markdown", "linesLength": 38, "startIndex": 220, "lines": ["# Bayesian Logistic Regression\n", "\n", "The first step is to define our model. In the frequentist interpretation of Logistic Regression, the natural log odds of the output, called the logits, are assumed to be generated from a linear combination of the inputs (the features) plus an error term to account for random sampling noise or latent variables. The frequentist interpretation formula is as follows:\n", "\n", "$$\\text{logit} = \\beta_0 + \\beta_1(\\text{Pregnancies}) + \\beta_2(\\text{Glucose}) + \\beta_3(\\text{BloodPressure}) + \\beta_4(\\text{SkinThickness}) + \\beta_5(\\text{Insulin}) + \\beta_6(\\text{BMI}) + \\beta_7(\\text{DiabetesPedigreeFunction}) + \\beta_8(\\text{Age}) + \\epsilon$$\n", "\n", "The log odds can then be converted to a proability of the output:\n", "\n", "$$p = \\frac{1}{1 + e^\\text{-logit}}$$\n", "\n", "For our problem, we are interested in finding the probability a patient has diabetes given the medical evidence:\n", "\n", "$$p(\\text{Diabetes} | \\text{Features})$$\n", "\n", "The objective is to learn the \"best\" $\\beta$ values which generally means those that are the most likely given a set of inputs and outputs. Logistic Regression calculates the values of $\\beta$ using an iterative procedure of Maximum Likelihood Estimation.\n", "\n", "In contrast, the Bayesian Framework assumes that the likelihood of the data is drawn from a distribution. For classification with a binary outcome, this is a bernoulli distribution:\n", "\n", "$$p(Diabetes | Features) = \\prod^{n}_{i=1} p_{i}^{y} (1 - p_{i})^{1-y_{i}}$$\n", "\n", "Where $y_i = 1$ if the patient has diabetes and $y_i = 0$ if the patient does not have diabetes. $p_i$ is the probability defined above, $p_i = \\frac{1}{1 + e^\\text{-logit_i}}$ with the $\\text{logit}$ a linear combination of the model parameters. \n", "\n", "$$\\text{logit_i} = \\beta_0 + \\beta_1(\\text{Pregnancies}) + \\beta_2(\\text{Glucose}) + \\beta_3(\\text{BloodPressure}) + \\beta_4(\\text{SkinThickness}) + \\beta_5(\\text{Insulin}) + \\beta_6(\\text{BMI}) + \\beta_7(\\text{DiabetesPedigreeFunction}) + \\beta_8(\\text{Age})$$\n", "\n", "The objective is to find the posterior probability distribution of the model parameters given the inputs and outputs.\n", "\n", "$$P(\\beta | X , y) = \\frac{P(y| \\beta, X)P(\\beta|X)}{P(X, y)}$$\n", "\n", "where $P(y| \\beta, X)$ is the data likelihood, $P(\\beta|X)$ is the prior on the parameters, and $P(X, y)$ is a normalization constant so the probability of the parameters sums to one. In reality, calculating the exact posterior for the model parameters is intractable with continuous variables, and so we use Monte Carlo sampling methods. This means that we draw repeated samples from the posterior in order to approximate the posterior. Generally this is done with some form of a Markov Chain Monte Carlo algorithm, in which the next state is dependent only a bounded subset of the current state and past states (depending on the order of the chain). In the Bayesian Framework, the end result of linear regression is not a single point estimate for the \"best\" model parameters, but rather an entire distribution of potential model parameters representing our uncertainty about the true values. \n", "\n", "In the Bayesian method, not only are the log odds assumed to be generated from a distribution, but so are the model parameters. If we have an idea about the parameters, we can encode this in the priors, or we can let the data speak and choose non-informative priors. Usually this means a normal distribution with a wide standard devivation for $\\beta$. In the case of infinite data, the priors will be completely washed out by the data likelihood. There are two primary advantages to the Bayesian method:\n", "\n", "1. __Priors__: if we have information about the model parameters, we can inject it into the model\n", "2. __Posterior distributions__: the result is a distribution for the model parameters instead of a point estimate\n", "\n", "We can use the mean of the parameter distributions if we want to obtain a single \"most likely\" prediction by calculating the log odds and then converting to a probability, but we can also express a range of probabilities. As the amount of training data increases, the uncertainty in the model parameters should decrease. In the case of limited data, Bayesian methods can be a better way to quantify uncertainty because the possible distribution of parameters will be wide and the standard deviation of the log odd distribution will be large. Although frequentist logistic regression can include confidence intervals, Bayesian methods can provide a more inuitive display of uncertainty in the cases of limited data. \n", "\n", "The end result of \"fitting\" the model by drawing samples from the posterior will be a distribution for each $\\beta$ model weight. The distribution can be used to make predictions and to examine the effects on the outcome of changing the value of different input variables (sensitivity analysis). First, we will build up the model and sample from the posterior, and then we will thoroughly interpret the modeling results. Additionally, we can make predictions and compare the results to standard machine learning models, although the benefit of Bayesian Methods is not so much in the accuracy, as in the interpretability of the results of the modeling. "]}, {"block": 35, "type": "markdown", "linesLength": 3, "startIndex": 258, "lines": ["### Formula for Logistic Regression\n", "\n", "Now on to the details. The `PyMC3` library has a module called `GLM` which implements a function called `GLM.from_formula`. This accepts an R style formula and builds up a model for us. It adds random variables (by default a normal distribution with a large standard deviation) for each of the features, a random variable for the intercept, and a normal distribution by default for the data likelihood. In this case, we need to change the data likelihood from a normal distribution to a Bernoulli distribution because we are using a binary outcome variable. "]}, {"block": 36, "type": "code", "linesLength": 5, "startIndex": 261, "lines": ["# Build up a formula\n", "formula = [' %s + ' % variable for variable in X.columns]\n", "formula.insert(0, 'y ~ ')\n", "formula = ' '.join(''.join(formula).split(' ')[:-2])\n", "formula"]}, {"block": 37, "type": "markdown", "linesLength": 3, "startIndex": 266, "lines": ["This follows the R programming language formula syntax where `~` means \"is a function of\". Here we are stating that `y` is a function of eight variables in a linear combination. In this equation,  `y` represents the log odds of diabetes which we can convert to a probability using the following equation:\n", "\n", "$$P(Diabetes | features) = \\frac{1}{1 + e^{-y}}$$\n"]}, {"block": 38, "type": "markdown", "linesLength": 11, "startIndex": 269, "lines": ["## Implementing the Logistic Model\n", "\n", "The first model will use all eight features. We create the model using the formula defined above and then draw samples from the posterior using the No-UTurn Sampler (NUTS). This is a variation of the Metropolis-Hastings algorithm. Both of these algorithms fall under the general category of Markov Chain Monte Carlo methods because they sample random values from the posterior with the next value dependent only on the current state (or a bounded subset of previous states). \n", "\n", "The data likelihood is a Bernoulli Variable because it satisfies the following three conditions:\n", "\n", "1. Outcome is binary: 0 for does not have diabetes and 1 for has diabetes\n", "2. Trials are independent of each other.\n", "3. The probability of the outcome does not change with each trial. \n", "\n", "If we have some prior knowledge, we can encode that if the priors for the $\\beta$ coefficients, or we can use the default priors in PyMC3. The default priors for regressors (the explanatory variables) are normal with the distribution $N(0, 10^{6})$ and the default prior for the Intercept is a flat distribution. In the call to create our model, we will pass in the formula, our training data, and the distribution to use for the response variable, which is a Bernoulli distribution. "]}, {"block": 39, "type": "code", "linesLength": 3, "startIndex": 280, "lines": ["# Create version of the training data with labels\n", "X_with_labels = X.copy()\n", "X_with_labels['y'] = y"]}, {"block": 40, "type": "code", "linesLength": 11, "startIndex": 283, "lines": ["# Create the model in a context\n", "with pm.Model() as logistic_model:\n", "  \n", "    # Build the model using the formula and specify the data likelihood \n", "    pm.GLM.from_formula(formula, data = X_with_labels, family = pm.glm.families.Binomial())\n", "    \n", "    # Using the no-uturn sampler\n", "    sampler = pm.NUTS()\n", "    \n", "    # Sample from the posterior using NUTS\n", "    trace_log = pm.sample(draws=5000, step = sampler, chains=2, tune=1000)"]}, {"block": 41, "type": "markdown", "linesLength": 1, "startIndex": 294, "lines": ["The entire model consists of traces for the model parameters. We can interpret and investigate the results using these traces."]}, {"block": 42, "type": "markdown", "linesLength": 1, "startIndex": 295, "lines": ["# Interpret Results"]}, {"block": 43, "type": "markdown", "linesLength": 3, "startIndex": 296, "lines": ["## Traceplot of Sampled Variables\n", "\n", "This trace shows all of the samples drawn for all of the variables. On the left we can see the distribution of the values with a vertical line at the mean. On the right we see the entire sampling trace. Notice that there are 2 chains indicating the sampling was done twice, each time with 5000 draws and 1000 tuning steps. The tuning steps are discarded because it takes a while for the model to \"settle in\" and the draws converge to posterior with enough sampling, but the initial samples are likely far off. "]}, {"block": 44, "type": "code", "linesLength": 1, "startIndex": 299, "lines": ["plot_trace(trace_log); "]}, {"block": 45, "type": "markdown", "linesLength": 1, "startIndex": 300, "lines": ["The traceplot nicely summaries what is occurring behind the scenes with Markov Chain Monte Carlo. The left shows the final approximate posterior distribution for the model parameters, and the right shows the complete trace from the sampling procedure. "]}, {"block": 46, "type": "markdown", "linesLength": 3, "startIndex": 301, "lines": ["## Forest Plot of Sampled Variables\n", "\n", "This also shows the distribution of the sampled variables with the 95% credible interval (highest positive density). This is one of the benefits of Bayesian Inference: the model returns uncertainty measures, which represent that any model is really only a guess about the state of the world."]}, {"block": 47, "type": "code", "linesLength": 2, "startIndex": 304, "lines": ["figsize(10, 12)\n", "pm.forestplot(trace_log);"]}, {"block": 48, "type": "markdown", "linesLength": 1, "startIndex": 306, "lines": ["We can see that the uncertainty is greatest for the `DiabetesPedigreeFunction` and the `Intercept`. The model is less sure about these two model parameters than for the other parameters."]}, {"block": 49, "type": "markdown", "linesLength": 3, "startIndex": 307, "lines": ["## Posterior Plot of the Variables\n", "\n", "The final plot we show here is another distribution of the sampled variables. The distributions are normal as expected from the default priors."]}, {"block": 50, "type": "code", "linesLength": 1, "startIndex": 310, "lines": ["pm.plot_posterior(trace_log);"]}, {"block": 51, "type": "markdown", "linesLength": 1, "startIndex": 311, "lines": ["The 95% highest positive density shows the likely range of the model parameters. If we are asked to explain the model parameters, we could give the mean as a \"most likely\" point estimate, but also share the 95% credible interval to show that there is uncertainty in our model, just like there is uncertainty in any real-world situation."]}, {"block": 52, "type": "markdown", "linesLength": 1, "startIndex": 312, "lines": ["## Summary Statistics"]}, {"block": 53, "type": "code", "linesLength": 1, "startIndex": 313, "lines": ["pm.summary(trace_log)"]}, {"block": 54, "type": "markdown", "linesLength": 1, "startIndex": 314, "lines": ["These values are not as directly interpretable as in linear regression: for example, we cannot say that for every one unit increase in age, the probability increases by 0.014. The coefficient in a logistic regression represents the difference in teh log odds of the response. What we have to do to interpret this in an understandable manner is take the exponent of the coefficent, which gives us the odds ratio associated with holding all other variables constant and changing that one variable. We then compare this to 1 to determine the effect of a 1-unit increase in the variable holding all other variables constant."]}, {"block": 55, "type": "code", "linesLength": 5, "startIndex": 315, "lines": ["# Convert coefficients to effect on odds of diabetes\n", "stat_df = pm.summary(trace_log)\n", "stat_df['odds_ratio'] = np.exp(stat_df['mean'])\n", "stat_df['percentage_effect'] = 100 * (stat_df['odds_ratio'] - 1)\n", "stat_df"]}, {"block": 56, "type": "markdown", "linesLength": 1, "startIndex": 320, "lines": ["Now we can interprete the `percentage_effect` to determine how a unit change in one variable affects the probability of diabetes. For example, with a 1 unit increase in `Pregnancies`, the odds of diabetes increases by 12%. Similarly, for a one unit increase in the `DiabetesPedigreeFunction`, the odds of having diabetes increases by 119%. We can see that some variables have a negative affect on the probability of having diabetes, that is, as the variable (such as `Insulin`) increases, the odds of the patient having diabetes decreases."]}, {"block": 57, "type": "markdown", "linesLength": 3, "startIndex": 321, "lines": ["## Equation from Model\n", "\n", "Using the average value of the parameters, we can output the equation for the log odds. We can then compare this to the model parameters returned from standard linear regression."]}, {"block": 58, "type": "code", "linesLength": 20, "startIndex": 324, "lines": ["# Calculate the means and std of all the variables in the trace\n", "# Also print the equation for the log odds\n", "def evaluate_trace(trace, data, print_model = False):\n", "    means_dict = {}\n", "    std_dict = {}\n", "    \n", "    for var in trace.varnames:\n", "        means_dict[var] = np.mean(trace[var])\n", "        std_dict[var] = np.std(trace[var])\n", "    \n", "    model = 'logit = %0.4f + ' % np.mean(means_dict['Intercept'])\n", "    \n", "    for var in data.columns:\n", "        model += '%0.4f * %s + ' % (means_dict[var], var)\n", "    \n", "    model = ' '.join(model.split(' ')[:-2])\n", "    if print_model:\n", "        print('Final Equation: \\n{}'.format(model))\n", "    \n", "    return means_dict, std_dict"]}, {"block": 59, "type": "code", "linesLength": 1, "startIndex": 344, "lines": ["means_dict, std_dict = evaluate_trace(trace_log, X, print_model=True)"]}, {"block": 60, "type": "markdown", "linesLength": 1, "startIndex": 345, "lines": ["### Compare to Standard Logistic Regression"]}, {"block": 61, "type": "code", "linesLength": 1, "startIndex": 346, "lines": ["print('Intercept: {:0.4f}'.format(lr.intercept_[0]))"]}, {"block": 62, "type": "code", "linesLength": 2, "startIndex": 347, "lines": ["for feature, weight in zip(X.columns, lr.coef_[0]):\n", "  print('Feature: {:30} Weight: {:0.4f}'.format(feature, weight))"]}, {"block": 63, "type": "markdown", "linesLength": 1, "startIndex": 349, "lines": ["The model parameters are very similar between the two methods. As we increase the number of training examples, we would expect the Bayesian Logistic Regression values to converge on those from the Standard Linear Regression."]}, {"block": 64, "type": "markdown", "linesLength": 5, "startIndex": 350, "lines": ["# Model Effects of Variables\n", "\n", "We can now look at the effect of a single variable on the probability of a patient having diabetes. This is a form of sensitivity analysis, in which we hold all but one variables constant, change that single variable, and observe the response. This lets us get an idea of how one variable affects the patient outcome. In a medical setting, we could use this as a measure of the most important medical indicators to focus on. We could then tell the patient how much their risk of diabetes decreases based on lifestyle interventions. Machine learning should be not just about making accurate predictions, but deriving actionable insights that can then be used to improve real-world outcomes.\n", "\n", "Here we will use the function `pm.plot_posterior_predictive_glm` which takes in a linear model over which to evaluate a range of values. We will pass in a linear model made by holding all variables except the query variable at their median value. The function then evaluates the probability of diabetes across a range of values for the query variable and plots the results. From these plots, we can see how changing the value of a variable affects the probability of a patient having diabetes according to the model."]}, {"block": 65, "type": "code", "linesLength": 34, "startIndex": 355, "lines": ["# Model the effect of a single variable on the probability\n", "def model_effect(query_var, trace, X):\n", "   # All variables except the query variable are held constant\n", "    steady_vars = list(X.columns)\n", "    steady_vars.remove(query_var)\n", "    \n", "    # Linear model \n", "    def lm(x, samples):\n", "        denominator = samples['Intercept'] + samples[query_var] * x\n", "        \n", "        # Hold all other variables at the median value\n", "        for var in steady_vars:\n", "            denominator += samples[var] * X[var].median()\n", "        denominator = (1 + np.exp(-denominator))\n", "        probs = 1 / denominator \n", "        \n", "        return probs\n", "    \n", "    figsize(8, 8)\n", "    \n", "    # Range over which to evaluate the query variable\n", "    var_min = X[query_var].min()\n", "    var_max = X[query_var].max()\n", "    \n", "    # Plot the linear model across a range of the query var\n", "    pm.plot_posterior_predictive_glm(trace, eval=np.linspace(var_min, var_max, 100), \n", "                                    lm=lm, samples=100, color='blue', alpha = 0.4, lw = 2)\n", "    \n", "    # Label the image\n", "    plt.xlabel('%s' % query_var, size = 18)\n", "    plt.ylabel('P(Diabetes)', size = 18)\n", "    plt.title(\"%s Effect on Posterior Probability\" % query_var, size = 20)\n", "    plt.xticks(size = 14); plt.yticks(size = 14)\n", "    plt.show()"]}, {"block": 66, "type": "code", "linesLength": 1, "startIndex": 389, "lines": ["model_effect('Age', trace_log, X)"]}, {"block": 67, "type": "markdown", "linesLength": 1, "startIndex": 390, "lines": ["Age has a positive effect on the probability of diabetes on average. The spread of lines indicates the uncertainty in the estimate of the age effect in the model."]}, {"block": 68, "type": "code", "linesLength": 1, "startIndex": 391, "lines": ["model_effect('Glucose', trace_log, X)"]}, {"block": 69, "type": "markdown", "linesLength": 1, "startIndex": 392, "lines": ["Glucose has a much stronger positive effect on the probability of diabetes. This is an actionable insight because we could use plots like this to show the patient how changing their lifestyle will affect their health outcomes. There is more certainty in the Glucose estimate indicating that we could be more confident stating that high glucose is at least correlated with increased risk of diabetes."]}, {"block": 70, "type": "code", "linesLength": 1, "startIndex": 393, "lines": ["model_effect('DiabetesPedigreeFunction', trace_log, X)"]}, {"block": 71, "type": "code", "linesLength": 1, "startIndex": 394, "lines": ["model_effect('BloodPressure', trace_log, X)"]}, {"block": 72, "type": "markdown", "linesLength": 3, "startIndex": 395, "lines": ["# Model Metrics\n", "\n", "Now we can use the trace to make predictions. We can choose the mean value of the parameters to arrive at a single prediction in order to calculate metrics for comparison. We can also use all the values in the trace to come up with a range of estimations showing our uncertainty in any prediction. This reflects that any model will only be an approximate representation of the world."]}, {"block": 73, "type": "code", "linesLength": 24, "startIndex": 398, "lines": ["# Find a single probabilty estimate using the mean value of variables in a trace\n", "def find_probs(trace, data):\n", "    \n", "    # Find the means and std of the variables\n", "    means_dict, std_dict = evaluate_trace(trace, data)\n", "    \n", "    \n", "    probs = []\n", "    \n", "    mean_array = np.array(list(means_dict.values()))\n", "    \n", "    # Need an intercept term in the data\n", "    data['Intercept'] = 1\n", "    data = data[list(means_dict.keys())]\n", "    \n", "    # Calculate the probability for each observation in the data\n", "    for _, row in data.iterrows():\n", "        # First the log odds\n", "        logit = np.dot(row, mean_array)\n", "        # Convert the log odds to a probability\n", "        probability = 1 / (1 + np.exp(-logit))\n", "        probs.append(probability)\n", "        \n", "    return probs"]}, {"block": 74, "type": "markdown", "linesLength": 3, "startIndex": 422, "lines": ["## Calculating Metrics\n", "\n", "Here we calculate the metrics using the mean value of the parameters as a \"most likely\" estimate. Although the primary benefits of Bayesian models are not in their accuracy but interpretability, calculating the metrics and comparing to stanadard machine learning methods is still an interesting exercise and maybe our model will be better! We will threshold the predictions at 0.5 although we could change this to change the number of false positive and false negatives."]}, {"block": 75, "type": "markdown", "linesLength": 1, "startIndex": 425, "lines": ["### Accuracy and F1 Score"]}, {"block": 76, "type": "code", "linesLength": 6, "startIndex": 426, "lines": ["# Find the most likely estimate\n", "blr_probs = find_probs(trace_log, X_test)\n", "\n", "# Threshold the values at 0.5\n", "predictions = (np.array(blr_probs) > 0.5)\n", "calc_metrics(predictions, y_test)"]}, {"block": 77, "type": "markdown", "linesLength": 1, "startIndex": 432, "lines": ["### Confusion Matrix"]}, {"block": 78, "type": "code", "linesLength": 2, "startIndex": 433, "lines": ["cm = confusion_matrix(y_test, predictions)\n", "plot_confusion_matrix(cm, classes = ['Negative', 'Positive'])"]}, {"block": 79, "type": "markdown", "linesLength": 1, "startIndex": 435, "lines": ["### AUC of the ROC"]}, {"block": 80, "type": "code", "linesLength": 1, "startIndex": 436, "lines": ["calc_roc(blr_probs, y_test)"]}, {"block": 81, "type": "markdown", "linesLength": 3, "startIndex": 437, "lines": ["## Comparison to Standard Machine Learning Models\n", "\n", "The best method for comparing classifiers is by graphing the ROC on top of each other. Here we will graph the four ROC curves corresponding to the four models."]}, {"block": 82, "type": "code", "linesLength": 15, "startIndex": 440, "lines": ["labels = ['Logistic Regression CV', 'Random Forest', 'Support Vector Machine',\n", "         'Bayesian LR']\n", "\n", "plt.figure(figsize = (8, 8))\n", "\n", "for i, probs in enumerate([lr_pred_proba, rf_pred_proba, svc_pred_proba, blr_probs]):\n", "  fpr, tpr, thresholds = roc_curve(y_test, probs)\n", "  auc = roc_auc_score(y_test, probs)\n", "  plt.plot(fpr, tpr, label = '%s, AUC = %0.4f' % (labels[i], auc))\n", "  \n", "plt.xlabel('False Positive Rate', size = 16)\n", "plt.ylabel('True Positive Rate', size = 16)\n", "plt.legend(prop={'size': 16})\n", "plt.title('ROC for Models')\n", "plt.show();"]}, {"block": 83, "type": "markdown", "linesLength": 3, "startIndex": 455, "lines": ["Based on these results, the Bayesian Model performs slightly worse than the others (although the exact numbers change every run, this is the most common trend I observed). The Random Forest model is the clear winner, with the other models relatively similar. When evaluating these models, we would want to take into account the relative value of false positives and false negatives. For example, in this problem, although I do not have any medical expertise, I would think it is more important to limit the number of false negatives than the number of false positives. In other words, falsely diagnosing someone with diabetes when they do not have diabetes is probably less harmful than not diagnosing someone with diabetes when they have the disease. We could fine tune the model to decrease the number of false negatives by setting the threshold very low (the upper right corner of the graph). This would increase the number of true positives and decrease the number of false negatives. THe tradeoff is an increase in the number of false positives, and this would have to be a decision made by medical experts. \n", "\n", "Overall, the Bayesian Model performs satisfactorily in comparison with other machine learning methods including standard logistic regression. It clearly outperforms a naive baseline and could be tuned to limit the number of false positives / false negatives. Moreover, the real benefit of the Bayesian Logistic Regression model is in its interpretability. We can see the effects of changing a single variable and use these results to make lifestyle recommendations to a patient. Let's now look at another advantage of Bayesian modeling: the demonstration of our uncertainty. We will again make predictions on the test set, but this time, instead of limiting ourselves to a single point estimate, we will show the entire range of predictions for each patient."]}, {"block": 84, "type": "markdown", "linesLength": 1, "startIndex": 458, "lines": ["# Model Predictions"]}, {"block": 85, "type": "markdown", "linesLength": 1, "startIndex": 459, "lines": ["## Calculate Predictions"]}, {"block": 86, "type": "code", "linesLength": 22, "startIndex": 460, "lines": ["# Make n_samples predictions for each data point based on a trace\n", "def get_prediction_range(trace, data_point, n_samples=1000):\n", "    \n", "    var_dict = {}\n", "    \n", "    # Get a range of values for the model parameters\n", "    for var in trace.varnames:\n", "        var_dict[var]= trace[var][-n_samples:]\n", "    \n", "    var_list = list(var_dict.keys())\n", "    \n", "    # Need to add the intercept to the data\n", "    data_point['Intercept'] = 1\n", "    data_point = data_point[var_list]\n", "    \n", "    # Calculate the log odds with the parameters\n", "    logits = np.dot(data_point.values, np.array(list(var_dict.values())))\n", "    \n", "    # Convert to probabilities\n", "    probs = 1 / (1 + np.exp(-logits))\n", "    \n", "    return(probs)"]}, {"block": 87, "type": "markdown", "linesLength": 5, "startIndex": 482, "lines": ["## Examine Specific Patient Predictions\n", "\n", "Here we make predictions for a patient, showing a range of predictions. We do this by using 1000 samples of the model parameters from the posterior distribution instead of a single value. The result is a distribution of predictions which we can compare to the true value. \n", "\n", "To try and gain understanding into the prediction of the model, we can also plot the patient's medical information on top of the entire range of values seen in the test distribution. This lets us see where the patient falls in relation to other patients. Potentially this may allow us to see the mapping between the features of a patient and the model's predicted outcome. We can also focus on the predictions the model makes incorrectly to see what types of mistakes the model makes."]}, {"block": 88, "type": "code", "linesLength": 59, "startIndex": 487, "lines": ["# Make predictions for one observation given a model trace\n", "def investigate_obs(trace, obs, true_label, examine_pred = True):\n", "\n", "    # Range of predictions\n", "    probs = get_prediction_range(trace, data_point = obs, n_samples=1000)\n", "    \n", "    figsize(6, 6)\n", "    \n", "    # Print the information about the prediction\n", "    print('\\n\\nMedian estimated probability: {:0.2f}'.format(np.percentile(probs, 50)))\n", "    print('5% estimated probability:     {:0.2f}'.format(np.percentile(probs, 5)))\n", "    print('95% estimated probability:    {:0.2f}\\n\\n'.format(np.percentile(probs, 95)))\n", "    \n", "    # Density Plot of the Probabilities\n", "    kde2 = sns.kdeplot(probs, label = 'pdf')\n", "    \n", "    # Vertical lines at ranges of the credible interval\n", "    plt.vlines([np.percentile(probs, 5), np.percentile(probs, 95)], \n", "               ymin = 0, ymax = kde2.get_ybound()[1],  colors = ['red'], linestyles = '--',\n", "              label = '5% and 95% CI');\n", "    # Plot labels\n", "    plt.xlabel('P(Diabetes)', size = 18); plt.ylabel('Density', size =18);\n", "    \n", "    plt.title('True Class: %s' % ('Diabetes' if true_label == 1 else 'No Diabetes'), size = 18);\n", "    plt.legend(prop={'size': 12});\n", "    plt.show()\n", "    \n", "    \n", "    print('Observation Data\\n\\n', obs)\n", "    \n", "    figsize(12, 12)\n", "    \n", "    if examine_pred:\n", "    # Plot the range of each variable\n", "      for i, var in enumerate(X_test.columns):\n", "          test_obs_value = obs[var]\n", "\n", "          ax = plt.subplot(4, 2, i + 1)\n", "\n", "          # Density plot of the distribution of the variable\n", "          kde = sns.kdeplot(X_test[var], color = 'black')\n", "\n", "\n", "          # Plot a vertical line at the test observation value\n", "          plt.vlines(x = test_obs_value, ymin = 0, ymax = kde.get_ybound()[1], colors = ['blue'])\n", "\n", "          # Addin the value\n", "          ax.annotate('{:0.2f}'.format(test_obs_value), xy = (test_obs_value, 0), xycoords = 'data',\n", "                      xytext = (-15, 35), textcoords = 'offset points', rotation = 90,\n", "                      va = 'bottom', fontsize = 'large', color = 'blue')\n", "\n", "          ax.set_xlabel(var)\n", "          ax.set_ylabel('Density')\n", "          ax.legend_.remove()\n", "\n", "      # Format the plot\n", "      plt.suptitle('Observation Compared to Test Distribution', y = 1.02, size = 20)\n", "      plt.tight_layout()\n", "      plt.show()"]}, {"block": 89, "type": "code", "linesLength": 5, "startIndex": 546, "lines": ["# Drop the interecpt column if it is present\n", "if 'Intercept' in X_test.columns: X_test = X_test.drop(columns = 'Intercept')\n", "  \n", "test_index = 5\n", "investigate_obs(trace_log, X_test.iloc[test_index, :], y_test.values[test_index])"]}, {"block": 90, "type": "markdown", "linesLength": 1, "startIndex": 551, "lines": ["# Investigate Wrong Classifications"]}, {"block": 91, "type": "code", "linesLength": 2, "startIndex": 552, "lines": ["test_index = 366\n", "investigate_obs(trace_log, X_test.loc[test_index, :], y_test.loc[test_index])"]}, {"block": 92, "type": "markdown", "linesLength": 1, "startIndex": 554, "lines": ["Here we see an example where the model made the wrong classification. In this case, the patient has a low `DiabetesPedigreeFunction` and not an extremely high `Glucose` value which might explain why the model predicted no diabetes."]}, {"block": 93, "type": "code", "linesLength": 2, "startIndex": 555, "lines": ["test_index =  487\n", "investigate_obs(trace_log, X_test.loc[test_index, :], y_test.loc[test_index])"]}, {"block": 94, "type": "markdown", "linesLength": 1, "startIndex": 557, "lines": ["For this patient, our model is extremely confident that she has diabetes when she does not! We can see why our model made this mistake because the patient has an elevated `Glucose` measurement and a high `DiabetesPedigreeFunction`, both of which are positively related to the outcome of diabetes in our model. This serves to remind us that any model is only an approximation of the world, and there will always be edge cases that cannot be correctly classified. It would be interesting to compare these predictions to those from a doctor to see if the expert is able to make the correct diagnosis."]}, {"block": 95, "type": "markdown", "linesLength": 3, "startIndex": 558, "lines": ["# Logistic Regression Model Using Two Variables\n", "\n", "Here we will see how well the model can do using only two variables, `Glucose` and `BMI`. The goal of machine learning is to create a parimonious model, that is, the most accurate model that can still correclty map the inputs to the outputs. A model with fewer features will be easier to interpret, and if the performance is the same as a more complicated model, we would prefer the simpler version. We will follow the same procedure as with all the features."]}, {"block": 96, "type": "code", "linesLength": 8, "startIndex": 561, "lines": ["# Select only two features\n", "X_two = X[['Glucose', 'BMI']]\n", "X_test_two = X_test[['Glucose', 'BMI']]\n", "\n", "# New formula \n", "two_formula = 'y ~ Glucose + BMI'\n", "X_two_with_labels = X_two.copy()\n", "X_two_with_labels['y'] = y"]}, {"block": 97, "type": "code", "linesLength": 10, "startIndex": 569, "lines": ["# Model using only two features\n", "with pm.Model() as logistic_model_two:\n", "    # Create the model using the formula and a binomial distribution for target\n", "    pm.GLM.from_formula(two_formula, data = X_two_with_labels, family=pm.glm.families.Binomial())\n", "    \n", "    # No-UTurn Sampling\n", "    sampler = pm.NUTS()\n", "    \n", "    # Sample from the posterior using NUTS\n", "    trace_log_two = pm.sample(draws=5000, step = sampler, chains = 2, tune = 1000)"]}, {"block": 98, "type": "markdown", "linesLength": 1, "startIndex": 579, "lines": ["### Posterior Distribution"]}, {"block": 99, "type": "code", "linesLength": 1, "startIndex": 580, "lines": ["plot_trace(trace_log_two);"]}, {"block": 100, "type": "markdown", "linesLength": 1, "startIndex": 581, "lines": ["### Effects of Variables"]}, {"block": 101, "type": "code", "linesLength": 1, "startIndex": 582, "lines": ["model_effect('Glucose', trace_log_two, X_two)"]}, {"block": 102, "type": "code", "linesLength": 1, "startIndex": 583, "lines": ["model_effect('BMI', trace_log_two, X_two)"]}, {"block": 103, "type": "markdown", "linesLength": 1, "startIndex": 584, "lines": ["## Evaluate Performance"]}, {"block": 104, "type": "code", "linesLength": 4, "startIndex": 585, "lines": ["two_probs = find_probs(trace_log_two, X_test_two)\n", "\n", "two_predictions = (np.array(two_probs) > 0.5)\n", "calc_metrics(two_predictions, y_test)"]}, {"block": 105, "type": "code", "linesLength": 1, "startIndex": 589, "lines": ["calc_roc(two_probs, y_test)"]}, {"block": 106, "type": "markdown", "linesLength": 1, "startIndex": 590, "lines": ["The f1 score did decrease as well as the area under the receiver operating characteristic curve. In this case, we would probably want to retain more variable to build a more performant model. More work could be done in order to determine the optimal number of features to include. In the end, a model with only two features still outperforms the baseline on the task of diabetes patient diagnosis. "]}, {"block": 107, "type": "code", "linesLength": 3, "startIndex": 591, "lines": ["test_index =  487\n", "investigate_obs(trace_log_two, X_test_two.loc[test_index, :], \n", "                y_test.loc[test_index], examine_pred = False)"]}, {"block": 108, "type": "markdown", "linesLength": 3, "startIndex": 594, "lines": ["# Visualizing Decision Boundary\n", "\n", "As we only used two dimensions for these classifications, we can plot all of the points and then examine the decision boundary. We will look at the distribution of all test data points based on the two variables `Glucose` and `BMI` and then see where the model draws the separation plane. We will use the mean of the variables in order to make our predictions on the test set. By looking at this distribution, we can see why exactly our model is unable to correctly classify all test observations."]}, {"block": 109, "type": "code", "linesLength": 4, "startIndex": 597, "lines": ["# Extrac the mean of all variables\n", "var_dict = {}\n", "for var in trace_log_two.varnames:\n", "    var_dict[var] = np.mean(trace_log_two[var])"]}, {"block": 110, "type": "markdown", "linesLength": 1, "startIndex": 601, "lines": ["## Plot of Data by Outcome"]}, {"block": 111, "type": "code", "linesLength": 3, "startIndex": 602, "lines": ["X_test['color'] = ['red' if y_test.values[i] == 1 else 'blue' for i in range(len(y_test))]\n", "plt.scatter(X_test['Glucose'], X_test['BMI'], color = X_test['color'])\n", "plt.ylabel('BMI'); plt.xlabel('Glucose'); plt.title('BMI vs Glucose for Test Data');"]}, {"block": 112, "type": "markdown", "linesLength": 3, "startIndex": 605, "lines": ["Clearly this is a non-linearly separable problem! We would not expect any model to be able to perform the task perfectly based on only these two variables.\n", "\n", "Here we will use color to represent our predicted probability. We can make predictions for a grid of xs and ys points and then color the background of the plot based on the probabilities. This will provide us with another opportunity to see how choosing a different threshold may be more appropriate depending on our objectives and the weight we place on the false negatives / false positives balance."]}, {"block": 113, "type": "code", "linesLength": 7, "startIndex": 608, "lines": ["# Points over which to plot predictions\n", "xs = np.linspace(X_test_two['Glucose'].min() - 10,\n", "                 X_test_two['Glucose'].max() + 10,\n", "                 1000)\n", "ys = np.linspace(X_test_two['BMI'].min() - 5,\n", "                 X_test_two['BMI'].max() + 5,\n", "                 1000)"]}, {"block": 114, "type": "code", "linesLength": 4, "startIndex": 615, "lines": ["# Not quite sure how this code works but it creates a grid\n", "grid = pm.floatX(np.mgrid[xs.min():xs.max():1000j, ys.min():ys.max():1000j])\n", "grid_2d = grid.reshape(2, -1).T\n", "dummy_out = np.ones(grid.shape[1], dtype = np.int8)"]}, {"block": 115, "type": "code", "linesLength": 3, "startIndex": 619, "lines": ["# Calculate logits and probabilities\n", "logits = grid_2d[:, 0] * var_dict['Glucose'] + grid_2d[:, 1] * var_dict['BMI'] + var_dict['Intercept']\n", "probs = 1 / (1 + np.exp(-logits))"]}, {"block": 116, "type": "code", "linesLength": 19, "startIndex": 622, "lines": ["# Colormap and figure \n", "cmap = sns.diverging_palette(250, 10, s=99, l=20, as_cmap=True)\n", "fig, ax = plt.subplots(figsize=(16, 9))\n", "\n", "# Create a 2D contour plot of the probabilities\n", "contour = ax.contourf(grid[0], grid[1], probs.reshape(1000, 1000), cmap=cmap)\n", "\n", "# Scatterplot of the test data\n", "ax.scatter(X_test['Glucose'], X_test['BMI'], color = X_test['color'])\n", "\n", "# Add colorbar for interpretation of colors\n", "cbar = plt.colorbar(contour, ax=ax)\n", "_ = ax.set(xlim = (60, 200), ylim = (20, 50)) \n", "\n", "# Label plot\n", "ax.set_xlabel('Glucose', size = 20)\n", "ax.set_ylabel('BMI', size = 20)\n", "ax.set_title('Posterior Probabilities and Test Data', size = 24);\n", "cbar.ax.set_ylabel('Posterior Probability of class label = 1', size = 14);"]}, {"block": 117, "type": "markdown", "linesLength": 3, "startIndex": 641, "lines": ["### Limitations\n", "\n", "Logistic Regression is linear in the natural log odds and if we are using only the features themselves (and not polynomials of the features), our model can only learn a linear decision boundary. In this example, with only two variables, the boundary between the classes is not linear. If we wanted to perfectly separate the classes (which may not be possible) we either would have to use additional features and hope these make the problem linear in higher dimensions, or we could map the two features here onto higher order polynomials. For now, we will leave the model as is but this would be a great exercise! From this plot, we can also see that if we lower our classification threshold below 0.5, we can correclty identify more true positives. However, this will have the trade-off of resulting in more false positives as well. By adjusting the classification threshold, we can adapt for the desired balance between false positives and false negatives. At this point we would probably want to consult a domain expert, hopefully someone with medical expertise who could inform us what the optimal balance should be. "]}, {"block": 118, "type": "markdown", "linesLength": 5, "startIndex": 644, "lines": ["# Future Work\n", "\n", "This preliminary evaluation of a Bayesian Logistic Regression for predicting the presence of diabetes has opened up additional avenues for exploration. For example, this is a problem that could benefit from expert domain knowledge. We might want to consult with a medical expert who could inform us what variables to use in our model, what new data we could gather to help our problem, the ideal threshold to balance the false positives and the false negatives, and who could help interpret the model parameters. I think machine learning is most useful when it is paired with domain knowledge, rather than being treated as a purely data-driven black-box method. Knowing that our model can make correct predictions is great, but it would also be useful if we can explain why our model makes the predictions it does and then use that information to improve patient outcomes. \n", "\n", "On the modeling aspect of this project, we could try to engineer new features to improve perform, such as polynomial maps of the existing features and interaction terms. We saw that the problem is not linearly separable in two dimensions, but there might be a linear solution with the correct features. In general, to reduce the uncertainty in our model, we would need to have more datapoints, or let the sampling process run for longer. Given that the average model parameters from Bayesian Logistic Regression are very similar to those from standard maximum likelihood estimation, it seems that our model has convered. Checking for convergence would be another area in which more work can be done. The limitation of this problem is the amount of data, and it is unlikely we could further improve the model without a larger number of training data points."]}, {"block": 119, "type": "markdown", "linesLength": 9, "startIndex": 649, "lines": ["# Conclusions\n", "\n", "In this notebook we saw how to implement Bayesian Logistic Regression for a machine learning supervised classification task. We compared the results with several standard machine learning models and thoroughly explored the Bayesian model. We saw that there are two benefits to Bayesian approaches: encoding domain knowledge in the model through the **use of priors** and determining a **posterior distribution** for the model parameters. \n", "\n", "We were able to interpret the model results to determine the effect of altering one variable while holding the others steady. This analysis could be useful when informing patients about the effects of medical interventions and lifestyle changes on the probability of developing diabetes (with the help of a medical expert of course). We also used the average value of the parameters in the posterior distribute to calculate most likely estimates for the test observations. The metrics from these predictions stacked up well against standard machine learning models. We then made prediction ranges on new observations and examined a few data points that the model got wrong. This manual error analysis could be used to build a better model, or to determine edge cases that are best addressed by a doctor.\n", "\n", "Finally, we explored whether it was possible to create a useful model using only two input features. While the model did outperform the naive baseline, it was not as performant as the model that used all eight of the features. We plotted the data points in two-dimensions and saw that this is not a linearly separable  classification problem. \n", "\n", "Overall, this project provided a nice introduction to the Bayesian approach to linear modeling. While not applicable in all situations, in problems with limited data, Bayesian methods allow us to create an interpretable model with good performance while also demonstrating our uncertainty about the model. Making accurate predictions is useful by itself, but it is even more powerful when we can explain the predictions of our model and use the results to improve real-world outcomes."]}, {"block": 120, "type": "markdown", "linesLength": 13, "startIndex": 658, "lines": ["# Sources\n", "\n", "https://en.wikipedia.org/wiki/Logistic_regression\n", "\n", "https://en.wikipedia.org/wiki/Bernoulli_distribution\n", "\n", "http://docs.pymc.io/notebooks/GLM-logistic.html\n", "\n", "https://statisticalhorizons.com/linear-vs-logistic\n", "\n", "https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/\n", "\n", "http://www.machinegurning.com/rstats/regularised-logistic-regression/"]}, {"block": 121, "type": "code", "linesLength": 0, "startIndex": 671, "lines": []}]