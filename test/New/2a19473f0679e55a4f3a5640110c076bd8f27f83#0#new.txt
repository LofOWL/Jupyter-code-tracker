[{"block": 0, "type": "markdown", "linesLength": 21, "startIndex": 0, "lines": ["# Introduction: Testing Cyclical Encoding of Features for Machine Learning\n", "\n", "Cyclical encoding of time-series features is typically done with time/date features that repeat periodically such as the hour of the day or day of the year. However, it is not a foregone conclusion this technique actually improves the performance of complex machine learning algorithms, such as a random forest. In this notebook, we will evaluate the efficacy of cyclical encoding of time-series features for predicting building energy data with two different models. The hypothesis is that cyclical encoding will improve the performance of the simple model, Linear Regression, but will not improve the performance of the model with higher capacity, the Random Forest Regression.\n", "\n", "## Methods\n", "\n", "__Feature Sets__\n", "1. Standard = `[\"time_of_day\", \"day_of_year\", \"temperature\"]`\n", "2. Cyclical = `[\"sin_time_of_day\", \"cos_time_of_day\", \"sin_day_of_year\", \"cos_day_of_year\", \"temperature\"]`\n", "\n", "(We include temperature in both feature sets because when modeling time-series problems, we almost always have other features besides the date and time. Results may vary with the addition of features).\n", "\n", "__Models__\n", "1. Linear Regression = `LinearRegression(n_jobs=-1)`\n", "2. Random Forest Regression = `RandomForestRegressor(n_estimators=25, max_depth=15, n_jobs=-1, random_state=100)`\n", "\n", "(The hyperparameters for the random forest have proved to work well for use cases in our data science systems. Again, performance may vary with hyperparameter values).\n", "\n", "We'll use a relatively simple test set-up. After creating the standard and cyclically encoded sets of features, we'll evaluate each set with the two models. Validation will consist of testing on a week of data at a time with all previous data used for training. We go through the dataset one week at a time, training on the historical data, and predicting for that selected week. This mimics what we actually do evaluating models for forecasting energy use at [Cortex Building Intel](https://cortexintel.com). The weekly validation is representative of a real-world situation, in which we'll have past training data and want to make a prediction of future energy consumption. By validating on past data, we can get an estimate of how the model will perform in the future. \n", "\n", "For our use case, we are primarily concerned with the performance of each set of features. We'll use a single metric, __mean absolute percentage error__ to assess our predictions. The benefits of MAPE are: relative comparisons, interpretability, and simplicity. The downsides are that it cna be sensitive to outliers and it's asymmetric because lower true values will result in greater percentage error for the same absolute error. There are always trade-offs to make in machine learning! Primarily we want to assess the difference in MAPE between the standard features and the cyclical features. Defining the difference in errors as standard mape - cyclical mape, a positive value indicates the cyclical features outperform the standard features. \n"]}, {"block": 1, "type": "markdown", "linesLength": 3, "startIndex": 21, "lines": ["# Data \n", "\n", "We are using building energy data originally part of a DrivenData machine learning competition to forecast building energy use. You can download the original data from DrivenData here. I've cleaned up the building data and removed all features except for timestamp and temperature. The __objective__ is to predict the \"energy\" from the features."]}, {"block": 2, "type": "code", "linesLength": 6, "startIndex": 24, "lines": ["import pandas as pd\n", "import numpy as np\n", "import glob\n", "\n", "building_data_files = glob.glob('data/building*')\n", "print(f'There are {len(building_data_files)} buildings to test.')"]}, {"block": 3, "type": "markdown", "linesLength": 1, "startIndex": 30, "lines": ["Let's take a look at one example dataset."]}, {"block": 4, "type": "code", "linesLength": 2, "startIndex": 31, "lines": ["data = pd.read_csv(building_data_files[10], parse_dates=['timestamp'], index_col=0).set_index('timestamp')\n", "data.head()"]}, {"block": 5, "type": "markdown", "linesLength": 1, "startIndex": 33, "lines": ["Nothing too extraordinary here."]}, {"block": 6, "type": "code", "linesLength": 5, "startIndex": 34, "lines": ["import cufflinks as cf\n", "from plotly.offline import plot\n", "\n", "fig = data.iplot(y='energy', secondary_y='temperature', title='Temperature and Energy', asFigure=True)\n", "plot(fig)"]}, {"block": 7, "type": "markdown", "linesLength": 3, "startIndex": 39, "lines": ["# Feature Engineering\n", "\n", "To repeatedly create the same features, we'll make two scikit-learn transformers and then join them together in a pipeline. This is very simple code, but including it in a well-defined interface means we can use it over and over while confident we'll get the same operations applied to our data."]}, {"block": 8, "type": "markdown", "linesLength": 3, "startIndex": 42, "lines": ["## Transformers\n", "\n", "[Scikit-Learn transformers](http://scikit-learn.org/stable/data_transforms.html) give us a standard interface for applying operations to our data. Using transformers is efficient and allows you to build robust systems."]}, {"block": 9, "type": "code", "linesLength": 47, "startIndex": 45, "lines": ["from sklearn.base import BaseEstimator, TransformerMixin\n", "\n", "class DateTimeFeatures(BaseEstimator, TransformerMixin):\n", "    \"\"\"\n", "    Extract day of year and time of day features from a timestamp\n", "    \"\"\"\n", "    def __init__(self):\n", "        pass\n", "\n", "    def fit(self, X, y=None):\n", "        return self\n", "\n", "    def transform(self, X, y=None):\n", "        # Timestamps must be in index\n", "        field = X.index\n", "        X[\"time_of_day\"] = field.hour + field.minute / 60\n", "        X[\"day_of_year\"] = field.dayofyear\n", "        return X\n", "\n", "\n", "class CyclicalDateTimeFeatures(BaseEstimator, TransformerMixin):\n", "    \"\"\"\n", "    Make cyclically encoded day of year and time of day features\n", "    \"\"\"\n", "    def __init__(self):\n", "        pass\n", "\n", "    def fit(self, X, y=None):\n", "        return self\n", "\n", "    def transform(self, X, y=None):\n", "        # Apply formula for sin and cosine\n", "        X[\"sin_time_of_day\"], X[\"cos_time_of_day\"] = _cyclical_encoding(\n", "            X[\"time_of_day\"], period=24\n", "        )\n", "        X[\"sin_day_of_year\"], X[\"cos_day_of_year\"] = _cyclical_encoding(\n", "            X[\"day_of_year\"], period=366\n", "        )\n", "        return X\n", "\n", "def _cyclical_encoding(series, period):\n", "    \"\"\"\n", "    Cyclical encoding of a series with a specified period\n", "    \"\"\"\n", "    # Basic formula for sin/cosine equation\n", "    base = 2 * np.pi * series / period\n", "    return np.sin(base), np.cos(base)"]}, {"block": 10, "type": "markdown", "linesLength": 3, "startIndex": 92, "lines": ["## Pipeline\n", "\n", "The pipeline is one of the most important features in sklearn. I highly recommend [reading up on the Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) and using it throughout your data science projects."]}, {"block": 11, "type": "code", "linesLength": 10, "startIndex": 95, "lines": ["from sklearn.pipeline import Pipeline\n", "\n", "# Make a pipeline with the steps\n", "transforms = Pipeline(\n", "    steps=[\n", "        # Must create the date/time features before encoding\n", "        (\"date_time_features\", DateTimeFeatures()),\n", "        (\"cylical_date_time_features\", CyclicalDateTimeFeatures()),\n", "    ]\n", ")"]}, {"block": 12, "type": "markdown", "linesLength": 1, "startIndex": 105, "lines": ["Now we can use our pipeline on any data set."]}, {"block": 13, "type": "code", "linesLength": 2, "startIndex": 106, "lines": ["transformed_data = transforms.transform(data)\n", "transformed_data.head()"]}, {"block": 14, "type": "code", "linesLength": 3, "startIndex": 108, "lines": ["data = pd.read_csv(building_data_files[20], parse_dates=['timestamp'], index_col=0).set_index('timestamp')\n", "transformed_data = transforms.transform(data)\n", "transformed_data.head()"]}, {"block": 15, "type": "code", "linesLength": 2, "startIndex": 111, "lines": ["fig = data.iplot(y=['sin_day_of_year', 'cos_day_of_year'], title=\"Cyclically Encoded Day of Year\", asFigure=True)\n", "plot(fig, filename='cylical_encoding_day_of_year.html')"]}, {"block": 16, "type": "code", "linesLength": 2, "startIndex": 113, "lines": ["fig = data.loc['2014-01-01': '2014-01-08'].iplot(y=['sin_time_of_day', 'cos_time_of_day'], title=\"Cyclically Encoded Time of Day\", asFigure=True)\n", "plot(fig, 'cyclical_encoding_time_of_day.html')"]}, {"block": 17, "type": "markdown", "linesLength": 7, "startIndex": 115, "lines": ["# Modeling\n", "\n", "The modeling itself is relatively simple thanks to Scikit-Learn. We'll make a function that takes a set of data iterates through the sets of features (2) and the models (2). On each iteration, we apply the testing framework by stepping through the dataset one week at a time. We train on the all previous data and then test on the selected week recording the predictions along with the actual value.\n", "\n", "This means that for each dataset, we are running 4 weekly validation testing sessions (2 x 2). Feel free to change the models or test using different features. You can also change the validation structure although we've found this to be an accurate indication of how well we can expect the model to perform on real data (forecasting for the future). \n", "\n", "If you were using this system in production, the most important aspect would be [writing unit tests](https://realpython.com/python-testing/)! These [should cover](https://docs.python-guide.org/writing/tests/) both the pipeline and the modeling. You are going to make mistakes in your code, that is guaranteed. Unit tests can help you find these mistakes before they cause unexpected errors in a machine learning system."]}, {"block": 18, "type": "code", "linesLength": 79, "startIndex": 122, "lines": ["def run_weekly_validation(\n", "    models,\n", "    data,\n", "    feature_sets=[\n", "        [\"time_of_day\", \"day_of_year\", \"temperature\"],\n", "        [\n", "            \"sin_time_of_day\",\n", "            \"cos_time_of_day\",\n", "            \"sin_day_of_year\",\n", "            \"cos_day_of_year\",\n", "            \"temperature\",\n", "        ],\n", "    ],\n", "):\n", "    \"\"\"\n", "    Run the weekly validation modeling with models and feature sets on data\n", "    \n", "    :param models: list of sklearn models\n", "    :param data: building energy dataframe\n", "    :param feature_sets: list of lists of features\n", "    \n", "    :return results: dataframe of prediction results from each combination of model and feature set\n", "    \"\"\"\n", "\n", "    all_predictions = []\n", "\n", "    # Iterate through feature sets\n", "    for feature_set in feature_sets:\n", "        features = \"standard\" if \"sin_time_of_day\" not in feature_set else \"cyclical\"\n", "        \n", "        # Subset to features in data\n", "        X = data[feature_set + [\"energy\"]].copy()\n", "\n", "        # Iterate through models\n", "        for model in models:\n", "            \n", "            model_name = model.__class__.__name__\n", "            \n", "            # Iterate through weeks in the dataset\n", "            # Must group by string formatted week and year\n", "            for (week, year), X_test in X.groupby(\n", "                # [week, year] grouping\n", "                [X.index.strftime(\"%U\"), X.index.strftime(\"%Y\")]\n", "            ):\n", "                # Subset to training data; all data before test set\n", "                X_train = X[X.index < X_test.index.min()].copy()\n", "\n", "                # Can not train or test on zero observations\n", "                if len(X_train) == 0 or len(X_test) == 0:\n", "                    continue\n", "\n", "                # Targets\n", "                y_train = X_train.pop(\"energy\")\n", "                y_test = X_test.pop(\"energy\")\n", "\n", "                # Fit and predict\n", "                model.fit(X_train, y_train)\n", "                predictions = model.predict(X_test)\n", "\n", "                # Record predictions along with actual values, model, and feature set in a dataframe\n", "                predictions = pd.DataFrame(\n", "                    dict(\n", "                        predicted=predictions,\n", "                        actual=y_test,\n", "                        model=model_name,\n", "                        features=features,\n", "                    ),\n", "                    # Index is same as for testing data\n", "                    index=X_test.index,\n", "                )\n", "                all_predictions.append(predictions)\n", "\n", "    # Return dataframe of dataframes ordered by timestamp\n", "    return (\n", "        pd.concat(all_predictions)\n", "        .reset_index()\n", "        .sort_values([\"model\", \"features\", \"timestamp\"])\n", "        .set_index(\"timestamp\")\n", "    )"]}, {"block": 19, "type": "markdown", "linesLength": 1, "startIndex": 201, "lines": ["We'll run one example first to make sure the results are as expected (in a real system, you'd want to have unit tests for this function and the pipeline.)"]}, {"block": 20, "type": "markdown", "linesLength": 1, "startIndex": 202, "lines": ["### Example Validation"]}, {"block": 21, "type": "code", "linesLength": 10, "startIndex": 203, "lines": ["from sklearn.linear_model import LinearRegression\n", "from sklearn.ensemble import RandomForestRegressor\n", "\n", "# Create linear model and random forest model for regression\n", "models = [\n", "    LinearRegression(n_jobs=-1),\n", "    RandomForestRegressor(n_estimators=25, max_depth=15, n_jobs=-1, random_state=100),\n", "]\n", "\n", "validation = run_weekly_validation(models, data)"]}, {"block": 22, "type": "code", "linesLength": 1, "startIndex": 213, "lines": ["validation.head()"]}, {"block": 23, "type": "code", "linesLength": 1, "startIndex": 214, "lines": ["validation.describe()"]}, {"block": 24, "type": "markdown", "linesLength": 3, "startIndex": 215, "lines": ["## Validation for all Buildings\n", "\n", "Finally, we can run the validation for all buildings. This may take a while and I've already included the results in the `validation_results` directory."]}, {"block": 25, "type": "code", "linesLength": 27, "startIndex": 218, "lines": ["from tqdm import tqdm_notebook\n", "\n", "def run_all_buildings(building_data_files):\n", "    \"\"\"\n", "    Run weekly validation for all buildings\n", "    \n", "    :param building_data_files: list of filenames with building energy data csv files\n", "    \n", "    :return None: saves each file in `validation_results` for analysis\n", "    \"\"\"\n", "\n", "    # Run validation for all buildings\n", "    for building_file_name in tqdm_notebook(building_data_files, desc=\"Buildings\"):\n", "\n", "        building_data = pd.read_csv(\n", "            building_file_name, parse_dates=[\"timestamp\"]\n", "        ).set_index(\"timestamp\")\n", "        # Create sets of features\n", "        building_data = transforms.transform(building_data)\n", "\n", "        # Run the validation and save the results\n", "        building_validation = run_weekly_validation(models, building_data)\n", "\n", "        # Save off results for analysis\n", "        building_validation.to_csv(\n", "            f\"{building_file_name.replace('energy_data', 'validation_results').replace('data', 'validation_results')}\"\n", "        )"]}, {"block": 26, "type": "code", "linesLength": 1, "startIndex": 245, "lines": ["run_all_buildings(building_data_files)"]}, {"block": 27, "type": "markdown", "linesLength": 3, "startIndex": 246, "lines": ["# Analysis\n", "\n", "Let's make sure we have all the datasets for analysis."]}, {"block": 28, "type": "code", "linesLength": 2, "startIndex": 249, "lines": ["building_result_files = glob.glob('validation_results/*_validation_results.csv')\n", "len(building_result_files)"]}, {"block": 29, "type": "markdown", "linesLength": 1, "startIndex": 251, "lines": ["We'll write our error metric as a function. There are some times when the actual value is 0 which results in infinite percentage error. To account for this, we replace the infinite values in the percentage error with a missing value and then ignore missing values in the mean absolute error calculation. The end outcome is a single float with the mean absolute error from the model's predictions."]}, {"block": 30, "type": "code", "linesLength": 14, "startIndex": 252, "lines": ["def mean_absolute_percentage_error(actual, predicted):\n", "    \"\"\"\n", "    Calculate the mean absolute percentage error of two arrays\n", "\n", "    :param actual: true values as a pandas series\n", "    :param predicted: estimated values as a pandas series\n", "\n", "    :return mape: float of mean absolute percentage error (%)\n", "\n", "    NOTE: We avoid getting an inf for the mape by replacing infinite values in the\n", "    absolute_percentage_error with nan and then ignoring the missing values (np.nanmean)\n", "    \"\"\"\n", "    ape_array = 100 * np.abs(predicted - actual) / actual\n", "    return np.nanmean(ape_array.replace({np.inf: np.nan, -np.inf: np.nan}))"]}, {"block": 31, "type": "markdown", "linesLength": 1, "startIndex": 266, "lines": ["First, we'll test out a simple block of code to calculate the error metric for one building. This groups by the model and feature set and finds the mean absolute error of the predictions."]}, {"block": 32, "type": "code", "linesLength": 14, "startIndex": 267, "lines": ["building_result_file = building_result_files[20]\n", "\n", "validation_results = pd.read_csv(building_result_file)\n", "\n", "building_metrics = (\n", "    validation_results.groupby(\n", "        [validation_results[\"features\"], validation_results[\"model\"]]\n", "    )\n", "    .apply(lambda x: mean_absolute_percentage_error(x[\"actual\"], x[\"predicted\"]))\n", "    .to_frame()\n", "    .rename(columns={0: \"mean_absolute_percentage_error\"})\n", ")\n", "\n", "building_metrics"]}, {"block": 33, "type": "markdown", "linesLength": 1, "startIndex": 281, "lines": ["That works, so now we'll calculate the metrics for all buildings. The above code can go in a function."]}, {"block": 34, "type": "code", "linesLength": 27, "startIndex": 282, "lines": ["def calculate_metrics(building_result_file):\n", "    \"\"\"\n", "    Calculate mean_absolute_percentage_error for a building from a predictions file\n", "    \n", "    :param building_result_file: location of validation results data\n", "    \n", "    :return building_metrics: dataframe of metrics for building\n", "    \"\"\"\n", "\n", "    # Read in the results\n", "    validation_results = pd.read_csv(building_result_file)\n", "\n", "    # Groupby the feature set and model\n", "    building_metrics = (\n", "        validation_results.groupby(\n", "            [\"features\", \"model\"]\n", "        )\n", "        # Calculate the error metric\n", "        .apply(lambda x: mean_absolute_percentage_error(x[\"actual\"], x[\"predicted\"]))\n", "        # Format the dataframe\n", "        .to_frame()\n", "        .reset_index()\n", "        .rename(columns={0: \"mean_absolute_percentage_error\"})\n", "    )\n", "    # Add the building id\n", "    building_metrics[\"building_id\"] = building_result_file.split(\"_\")[2]\n", "    return building_metrics"]}, {"block": 35, "type": "markdown", "linesLength": 1, "startIndex": 309, "lines": ["Now, we apply that function to all building results."]}, {"block": 36, "type": "code", "linesLength": 5, "startIndex": 310, "lines": ["# Create a dataframe of all metrcs\n", "all_metrics = pd.concat(\n", "    [calculate_metrics(br_file) for br_file in building_result_files]\n", ")\n", "all_metrics.to_csv(\"validation_results/all_metrics.csv\")"]}, {"block": 37, "type": "code", "linesLength": 2, "startIndex": 315, "lines": ["all_metrics.head()\n", "all_metrics.describe()"]}, {"block": 38, "type": "markdown", "linesLength": 3, "startIndex": 317, "lines": ["# Results\n", "\n", "We can now answer our main question: does cyclical encoding of date/time features benefit machine learning models? If so, we expect to see a decrease in the model error with the cyclical features."]}, {"block": 39, "type": "code", "linesLength": 1, "startIndex": 320, "lines": ["all_metrics.groupby(['features', 'model']).describe()"]}, {"block": 40, "type": "markdown", "linesLength": 1, "startIndex": 321, "lines": ["To find the answer, we group by the building and model and calculate the difference in error between the sets of features."]}, {"block": 41, "type": "code", "linesLength": 25, "startIndex": 322, "lines": ["def difference_between_features(metrics):\n", "    \"\"\"\n", "    Calculate the difference in mean absolute error between two sets of features.\n", "    \"\"\"\n", "    return float(\n", "        metrics.loc[\n", "            metrics[\"features\"] == \"standard\", \"mean_absolute_percentage_error\"\n", "        ].values\n", "        - metrics.loc[\n", "            metrics[\"features\"] == \"cyclical\", \"mean_absolute_percentage_error\"\n", "        ].values\n", "    )\n", "\n", "# Groupby building and model\n", "feature_differences = (\n", "    all_metrics.groupby([\"building_id\", \"model\"])\n", "    # Calculate difference\n", "    .apply(lambda x: difference_between_features(x))\n", "    # Format as dataframe\n", "    .to_frame()\n", "    .reset_index()\n", "    .rename(columns={0: \"difference_in_error\"})\n", ")\n", "\n", "feature_differences.groupby('model').describe()"]}, {"block": 42, "type": "markdown", "linesLength": 5, "startIndex": 347, "lines": ["The initial hypothesis holds: __Cyclical encoding of features benefits a simple linear regression, but not a complex model such as Random Forest Regression.__\n", "\n", "# Visualizations\n", "\n", "We've answered our main question, but we can still make some visualizations. We'll keep using the interactive [plotly library](https://plot.ly/python/) with its newest addition, [plotly express](https://medium.com/@plotlygraphs/introducing-plotly-express-808df010143d)"]}, {"block": 43, "type": "code", "linesLength": 2, "startIndex": 352, "lines": ["from plotly.offline import plot\n", "import plotly_express as px"]}, {"block": 44, "type": "code", "linesLength": 27, "startIndex": 354, "lines": ["def graph_results(all_metrics, feature_differences):\n", "    # Make a bar plot of all the errors for each model and set of features\n", "    all_scores_fig = px.bar(\n", "        all_metrics,\n", "        x=\"building_id\",\n", "        y=\"mean_absolute_percentage_error\",\n", "        color=\"features\",\n", "        barmode=\"group\",\n", "        facet_row=\"model\",\n", "        title=\"Comparison of Models and Errors\",\n", "        template=\"plotly_white\",\n", "        height=1400,\n", "    )\n", "    # Make a bar plot of the difference in errors from cyclical encoding\n", "    diff_fig = px.bar(\n", "        feature_differences,\n", "        x=\"building_id\",\n", "        y=\"difference_in_error\",\n", "        facet_row=\"model\",\n", "        height=1400,\n", "        template=\"plotly_white\",\n", "        title=\"Performance Gain from Cyclical Encoding of Features\",\n", "    )\n", "    return all_scores_fig, diff_fig\n", "\n", "\n", "all_scores_fig, diff_fig = graph_results(all_metrics, feature_differences)"]}, {"block": 45, "type": "code", "linesLength": 2, "startIndex": 381, "lines": ["plot(all_scores_fig, filename=\"validation_results/all_scores.html\")\n", "plot(diff_fig, filename='validation_results/diff_in_scores.html')"]}, {"block": 46, "type": "markdown", "linesLength": 1, "startIndex": 383, "lines": ["The graphs clearly show our conclusion: the cyclically encoding has lowered the error for the linear regression but not for the random forest. Feel free to explore the data and change the testing setup, model hyperparameters, feature sets, data sets to see if this result holds."]}, {"block": 47, "type": "markdown", "linesLength": 5, "startIndex": 384, "lines": ["# Conclusion\n", "\n", "The main takeaway is that __cyclical feature encoding is not required for complex models with a high capacity.__ These models are able to learn the relationships between the day of year, time of day and the target without cyclical encoding. The feature transformation does not hurt the more complex models, but it adds unnecessary steps to a machine learning pipeline without adding benefits. \n", "\n", "Granted, this result may only hold for building energy data without many other features and I think it'd be great if others applied a similar testing procedure to different datasets. Currently, the only way to figure out if something is a good idea in machine learning is to get some data and try it out, a lesson we learned with this feature testing project."]}, {"block": 48, "type": "code", "linesLength": 0, "startIndex": 389, "lines": []}]