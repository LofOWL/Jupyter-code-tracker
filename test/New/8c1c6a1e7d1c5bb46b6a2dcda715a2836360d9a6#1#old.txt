[{"block": 0, "type": "markdown", "linesLength": 9, "startIndex": 0, "lines": ["# Introduction \n", "\n", "In this notebook we will perform an exploratory data analysis (EDA). An EDA is used to find trends/patterns or correlations in a dataset that are useful on their own or can be used to inform modeling decisions. EDA involves both exploratory figures as well as summary statistics to quantify patterns. In short, we use EDA to find out what we can learn from our data.\n", "\n", "## Dataset\n", "\n", "We are using the diabetes dataset which is [available on Kaggle](https://www.kaggle.com/uciml/pima-indians-diabetes-database). The data was originally collected by the National Institute of Diabetes and Digestive and Kidney Diseases from a set of females at least 21 years old and of Pima Indian Heritage. \n", "__The objective is to use the patient information to predict whether or no the patient has diabetes.__\n", "There are 8 features (explanatory variables) and 1 label (response variable). This data collected from acutal patients and represents a task which might commonly be undertaken by a human doctor interested in identifying the patients most at risk for diabetes in order to recommend preventative measures."]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 9, "lines": ["# Exploratory Data Analysis"]}, {"block": 2, "type": "code", "linesLength": 3, "startIndex": 10, "lines": ["# numpy and pandas for data manipulation\n", "import numpy as np\n", "import pandas as pd"]}, {"block": 3, "type": "markdown", "linesLength": 1, "startIndex": 13, "lines": ["### Read in Data and Calculate Summary Statistics"]}, {"block": 4, "type": "code", "linesLength": 2, "startIndex": 14, "lines": ["df = pd.read_csv('https://raw.githubusercontent.com/WillKoehrsen/eecs-491/master/assign/project/diabetes.csv')\n", "df.head()"]}, {"block": 5, "type": "code", "linesLength": 1, "startIndex": 16, "lines": ["df.describe()"]}, {"block": 6, "type": "markdown", "linesLength": 1, "startIndex": 17, "lines": ["There are 768 observations (patients) with 8 features and 1 label each. The minimum glucose, blood pressure, skin thickness, insulin, and BMI are all 0. This appears suspect because these are physical quantities that cannot be 0 (for a live person). Therefore, this has already told us that we will need to perform imputation on these five columns. The range of the other variables all appears reasonable. "]}, {"block": 7, "type": "markdown", "linesLength": 3, "startIndex": 18, "lines": ["## Visualizations\n", "\n", "We can start off with the Pairs Plot which plots all variables against one another pairwise. This is useful for finding correlations between variables and visualizing distributions. "]}, {"block": 8, "type": "code", "linesLength": 6, "startIndex": 21, "lines": ["# matploblib and seaborn for visualizations\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "import seaborn as sns\n", "sns.set_context('talk')"]}, {"block": 9, "type": "markdown", "linesLength": 1, "startIndex": 27, "lines": ["In this plot, we can color the distributions by the outcome to see if there are noticeable trends between the features and whether or not the patient has diabetes. First, we can show the plot without imputing the missing values. "]}, {"block": 10, "type": "code", "linesLength": 2, "startIndex": 28, "lines": ["plt.style.use('fivethirtyeight')\n", "sns.pairplot(df, hue = 'Outcome', vars = df.columns[:8], diag_kind = 'kde');"]}, {"block": 11, "type": "markdown", "linesLength": 1, "startIndex": 30, "lines": ["The only clear trend appears to be that a higher glucose is correlated with an outcome of 1, meaning the patient has diabetes. The age also appears as if it might be correlated with diabetes: younger patients appear to be at a lower risk for developing diabetes. "]}, {"block": 12, "type": "markdown", "linesLength": 3, "startIndex": 31, "lines": ["### Impute Missing Values\n", "\n", "Before we go any further, let's take care of those missing values. Again, there are zeros in the Glucose, BloodPressure, SkinThickness, Insulin, and BMI categories. None of these can actually be 0, so we will assume that the missing values are a lack of data. To fill in these missing values, we will replace them with the median value in the column. There are other, more complicated methods for filling in missing values, but in practice, median imputation generally performs well."]}, {"block": 13, "type": "code", "linesLength": 5, "startIndex": 34, "lines": ["df['Glucose'] = df['Glucose'].replace({0: df['Glucose'].median()})\n", "df['BloodPressure'] = df['BloodPressure'].replace({0: df['BloodPressure'].median()})\n", "df['SkinThickness'] = df['SkinThickness'].replace({0: df['SkinThickness'].median()})\n", "df['Insulin'] = df['Insulin'].replace({0: df['Insulin'].median()})\n", "df['BMI'] = df['BMI'].replace({0: df['BMI'].median()})"]}, {"block": 14, "type": "code", "linesLength": 1, "startIndex": 39, "lines": ["df.describe()"]}, {"block": 15, "type": "markdown", "linesLength": 1, "startIndex": 40, "lines": ["Now that there are no missing values, we can calculate correlation values to see how the features are related to the outcome. Correlation does not of course imply causation, but because we are building a linear model, the correlated features are likely useful for learning a mapping between the patient information and whether or not they have diabetes. In a problem with a greater number of features, we could use a correlation threshold for removing variables. In this case, we will probably want to keep all of the variables and let the model decide which are relevant. "]}, {"block": 16, "type": "markdown", "linesLength": 1, "startIndex": 41, "lines": ["## Correlations"]}, {"block": 17, "type": "code", "linesLength": 1, "startIndex": 42, "lines": ["df.corr()['Outcome']"]}, {"block": 18, "type": "markdown", "linesLength": 1, "startIndex": 43, "lines": ["Our initial interpretation of the plots was correct: the glucose is the highest correlated value with the outcome. None of the features are strongly correlated with the outcome and there are no negative correlations. TO show the correlations and the distributions, we can make another pairs plot, but this time use a custom function to map information onto the plot. We also change the lower triangle to show a 2-dimensional kernel density estimate rather than a repeat of the scatterplots."]}, {"block": 19, "type": "code", "linesLength": 25, "startIndex": 44, "lines": ["import scipy\n", "\n", "# Function to calculate correlation coefficient between two variables\n", "def corrfunc(x, y, **kwgs):\n", "    r = np.corrcoef(x, y)[0][1]\n", "    ax = plt.gca()\n", "    ax.annotate(\"r = {:.2f}\".format(r),\n", "                xy=(.1, .8), xycoords=ax.transAxes,\n", "               size = 24)\n", "\n", "# Create a PairGrid\n", "g = sns.PairGrid(data = df,  \n", "                 vars = ['Outcome', 'Glucose', 'BMI', 'Pregnancies', 'Age'])\n", "\n", "# Map a scatterplot to the upper triangle\n", "g.map_upper(plt.scatter)\n", "\n", "# Map a histogram to the diagonal\n", "g.map_diag(plt.hist)\n", "\n", "# Map a kde plot to the lower triangle\n", "g.map_lower(sns.kdeplot)\n", "\n", "# Map the correlation coefficient to the lower diagonal\n", "g.map_lower(corrfunc)"]}, {"block": 20, "type": "markdown", "linesLength": 5, "startIndex": 69, "lines": ["# Conclusions\n", "\n", "In this brief Exploratory Data Analysis we learned 2 primary aspects about the dataset that we can use in modeling. First, we need to impute the missing values in several columns because these are not physically possible. We can use median imputation as a simple and effective method for filling in the 0 values. We also learned that there are correlations between the features and the response although there are not strong. Moreover, all of the features have at least a slight positive correlation with the outcome (whether or no the patient has diabetes). There are no obvious feature engineering steps to take, and there is no need to reduce the number of dimensions because there are only 8 features. Also, techniques such as [principal component analysis](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) obscure the physical relevance of the features and we cannot interpret the model as a result. Overall, I want to let the model learn from all of the data, and so will keep all of the features. This way we can let the data speak and interpret the modeling outcomes. This was a brief but valuable exercise and I'll see you in the modeling notebook!  \n", "\n", "\n"]}, {"block": 21, "type": "code", "linesLength": 1, "startIndex": 74, "lines": [""]}]