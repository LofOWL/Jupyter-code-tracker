[{"block": 0, "type": "markdown", "linesLength": 3, "startIndex": 0, "lines": ["# Introduction Simpson's Paradox\n", "\n", "Simpson's paradox occurs when trends that are present when data is separated into groups reverse when the data is aggregated. In this notebook, we take a look at three simple examples of Simpson's Paradox both quantitatively and visually. "]}, {"block": 1, "type": "code", "linesLength": 5, "startIndex": 3, "lines": ["import pandas as pd\n", "import numpy as np\n", "\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline"]}, {"block": 2, "type": "markdown", "linesLength": 3, "startIndex": 8, "lines": ["# Medicine A vs Medicine B\n", "\n", "In this example, X is better for both males and females, but performs worse overall! This is due to different sample sizes of males and females receiving each treatment."]}, {"block": 3, "type": "code", "linesLength": 4, "startIndex": 11, "lines": ["data = pd.DataFrame({'X Better': [125, 22], 'X Treated': [610, 75], 'Y Better': [29, 60], 'Y Treated': [150, 240]}, index = [\"M\", \"F\"])\n", "data['X Percent'] = data['X Better'] / data['X Treated']\n", "data['Y Percent'] = data['Y Better'] / data['Y Treated']\n", "data"]}, {"block": 4, "type": "code", "linesLength": 4, "startIndex": 15, "lines": ["data.loc['combined', :4] = list(data.iloc[:, :4].sum())\n", "data.loc['combined', 'X Percent'] = data.loc['combined', 'X Better'] / data.loc['combined', 'X Treated']\n", "data.loc['combined', 'Y Percent'] = data.loc['combined', 'Y Better'] / data.loc['combined', 'Y Treated']\n", "data"]}, {"block": 5, "type": "markdown", "linesLength": 3, "startIndex": 19, "lines": ["# Restaurant Reviews\n", "\n", "In this example, Carlo's Restaurant is recommended by a higher percentage of both males and females, but is recommended by a lower overall percentage of respondents. This again occurs because of mismatched sample sizes. "]}, {"block": 6, "type": "code", "linesLength": 4, "startIndex": 22, "lines": ["data = pd.DataFrame({\"Recommend Sophia's\": [50, 200], \"Sophia's Total\": [150, 250], \"Recommend Carlo's\": [180, 36], \"Carlo's Total\": [360, 40]}, index = [\"M\", \"F\"])\n", "data[\"Sophia\\'s Percent\"] = data[\"Recommend Sophia's\"] / data[\"Sophia's Total\"]\n", "data[\"Carlo's Percent\"] = data[\"Recommend Carlo's\"] / data[\"Carlo's Total\"]\n", "data"]}, {"block": 7, "type": "code", "linesLength": 4, "startIndex": 26, "lines": ["data.loc['combined', :4] = list(data.iloc[:, :4].sum())\n", "data.loc['combined', \"Sophia's Percent\"] = data.loc['combined', \"Recommend Sophia's\"] / data.loc['combined', \"Sophia's Total\"]\n", "data.loc['combined', \"Carlo's Percent\"] = data.loc['combined', \"Recommend Carlo's\"] / data.loc['combined', \"Carlo's Total\"]\n", "data"]}, {"block": 8, "type": "markdown", "linesLength": 3, "startIndex": 30, "lines": ["# Correlation Reversal: Hours of Exercise vs Chance of Developing Disease\n", "\n", "This is another fictional example showing Simpson's Paradox in the context of a correlation reversal. When looking at the data individually, there is a negative correlation between hours of exercise and chance of developing a disease, but when aggregating the data, the correlation reverses! This is due to the presence of another cause, age, on the chance of developing a disease. In order to determine the effect of exercise on the probability of disease, we need to control for the age of patients. "]}, {"block": 9, "type": "code", "linesLength": 1, "startIndex": 33, "lines": ["from numpy.polynomial.polynomial import polyfit"]}, {"block": 10, "type": "code", "linesLength": 42, "startIndex": 34, "lines": ["n_samples = 100\n", "np.random.seed(42)\n", "ages = np.random.randint(20, 50, n_samples)\n", "hours = np.random.randint(1, 5, n_samples) + np.random.randn(n_samples)\n", "p = 12 + 0.5 * ages + -2.1 * hours + np.random.randn(n_samples) *  2\n", "under_50 = pd.DataFrame({'age': ages, 'Hours Exercised': hours, 'probability': p})\n", "\n", "n_samples = 100\n", "ages = np.random.randint(50, 85, n_samples)\n", "hours = np.random.randint(3, 8, n_samples) + np.random.randn(n_samples) * 0.5\n", "p = 40 + 0.32 * ages + -3.2 * hours + np.random.randn(n_samples) \n", "over_50 = pd.DataFrame({'age': ages, 'Hours Exercised': hours, 'probability': p})\n", "\n", "\n", "def plot_relationship(data, c, color, ax):\n", "    \"\"\"Plot a scatter plot with linear fit\"\"\"\n", "    x, y = np.array(data[c]), np.array(data['probability'])\n", "    # Linear fit (polynomial of degree 1)\n", "    b, m = polyfit(x, y, 1)\n", "    # Plot scatterplot\n", "    data.plot(x = c, y = 'probability', c = color, \n", "              style = 'o', legend = None, ax = ax, ms = 10)\n", "    # Plot linear fit\n", "    ax.plot(x, m * x + b, '-', color = 'k');\n", "    if color == '#d9d142':\n", "        plt.title(f'Probability vs {c.capitalize()} over 50')\n", "    elif color == '#04c5ff':\n", "        plt.title(f'Probability vs {c.capitalize()} under 50')\n", "    else:\n", "        plt.title(f'Probability vs {c.capitalize()} Combined')\n", "    corr_coef = np.corrcoef(x, y)[0][1]\n", "    ax = plt.gca()\n", "    plt.ylabel('Probability'); \n", "    plt.text(0.2, 0.75, r'$\\rho$ = ' + f'{round(corr_coef, 2)}', fontsize = 28, color = 'k', \n", "             transform=ax.transAxes)\n", "\n", "    \n", "plt.figure(figsize = (20, 8))\n", "ax = plt.subplot(1, 2, 1)\n", "plot_relationship(under_50, 'Hours Exercised', '#04c5ff', ax)\n", "ax = plt.subplot(1, 2, 2)\n", "plot_relationship(over_50, 'Hours Exercised', '#d9d142', ax)"]}, {"block": 11, "type": "code", "linesLength": 4, "startIndex": 76, "lines": ["plt.figure(figsize = (10, 8))\n", "combined = pd.concat([under_50, over_50], axis = 0)\n", "ax = plt.subplot(1, 1, 1)\n", "plot_relationship(combined, 'Hours Exercised', 'r', ax)"]}, {"block": 12, "type": "code", "linesLength": 5, "startIndex": 80, "lines": ["plt.figure(figsize = (20, 8))\n", "ax = plt.subplot(1, 2, 1)\n", "plot_relationship(under_50, 'age', '#04c5ff', ax)\n", "ax = plt.subplot(1, 2, 2)\n", "plot_relationship(over_50, 'age', '#d9d142', ax)"]}, {"block": 13, "type": "code", "linesLength": 3, "startIndex": 85, "lines": ["plt.figure(figsize = (10, 8))\n", "ax = plt.subplot(1, 1, 1)\n", "plot_relationship(combined, 'age', 'r', ax)"]}, {"block": 14, "type": "code", "linesLength": 2, "startIndex": 88, "lines": ["colors = [ '#04c5ff' for _ in range(n_samples)]\n", "colors.extend(['#d9d142' for _ in range(n_samples)])"]}, {"block": 15, "type": "code", "linesLength": 20, "startIndex": 90, "lines": ["plt.figure(figsize = (10, 8))\n", "\n", "plt.scatter(combined['Hours Exercised'],\n", "            combined['probability'], c = colors, label = None, s = 60);\n", "\n", "x_c, y_c = np.array(combined['Hours Exercised']), np.array(combined['probability'])\n", "b_c, m_c = polyfit(x_c, y_c, 1)\n", "\n", "x_u, y_u = np.array(under_50['Hours Exercised']), np.array(under_50['probability'])\n", "b_u, m_u = polyfit(x_u, y_u, 1)\n", "\n", "x_o, y_o = np.array(over_50['Hours Exercised']), np.array(over_50['probability'])\n", "b_o, m_o = polyfit(over_50['Hours Exercised'], over_50['probability'], 1)\n", "\n", "plt.plot(x_u, b_u + m_u * x_u, c =  '#04c5ff', label = 'Under 50 Fit');\n", "plt.plot(x_o, b_o + m_o * x_o, c = '#d9d142', label = 'Over 50 Fit');\n", "plt.plot(x_c, b_c + m_c * x_c, c = 'r', label = 'Combined Fit');\n", "plt.xlabel('Hours Exercised');\n", "plt.ylabel('Probability'); plt.title(\"Simpson's Paradox: Correlation Reversal\");\n", "plt.legend(prop = {'size': 14});"]}, {"block": 16, "type": "code", "linesLength": 20, "startIndex": 110, "lines": ["plt.figure(figsize = (10, 8))\n", "\n", "plt.scatter(combined['age'],\n", "            combined['probability'], c = colors, label = None, s = 60);\n", "\n", "x_c, y_c = np.array(combined['age']), np.array(combined['probability'])\n", "b_c, m_c = polyfit(x_c, y_c, 1)\n", "\n", "x_u, y_u = np.array(under_50['age']), np.array(under_50['probability'])\n", "b_u, m_u = polyfit(x_u, y_u, 1)\n", "\n", "x_o, y_o = np.array(over_50['age']), np.array(over_50['probability'])\n", "b_o, m_o = polyfit(over_50['age'], over_50['probability'], 1)\n", "\n", "plt.plot(x_u, b_u + m_u * x_u, c =  '#04c5ff', label = 'Under 50 Fit');\n", "plt.plot(x_o, b_o + m_o * x_o, c = '#d9d142', label = 'Over 50 Fit');\n", "plt.plot(x_c, b_c + m_c * x_c, c = 'r', label = 'Combined Fit');\n", "plt.xlabel('Age');\n", "plt.ylabel('Probability'); plt.title(\"Simpson's Paradox: Additional Cause\");\n", "plt.legend(prop = {'size': 14});"]}, {"block": 17, "type": "code", "linesLength": 18, "startIndex": 130, "lines": ["from mpl_toolkits.mplot3d import Axes3D\n", "\n", "fig = plt.figure(figsize = (10, 8))\n", "ax = fig.add_subplot(111, projection='3d')\n", "\n", "\n", "markers = ['.' for _ in range(100)]\n", "markers.extend('o' for _ in range(100))\n", "\n", "x_c = np.array(combined['Hours Exercised'])\n", "y_c = np.array(combined['age'])\n", "z_c = np.array(combined['probability'])\n", "ax.scatter(x_c, y_c, z_c, c=colors, s = 40, marker='o', alpha = 1.0)\n", "\n", "ax.set_xlabel('Hours Exercised')\n", "ax.set_ylabel('Age')\n", "ax.set_zlabel('Probability');\n", "ax.set_title('Visualization of Multiple Causes', y = 1.05);"]}, {"block": 18, "type": "markdown", "linesLength": 3, "startIndex": 148, "lines": ["# Conclusions\n", "\n", "Simpson's Paradox is an intriguing statistical phenomenon that demonstrates the importance of thinking causally by asking why - determining the data generation process. Once we know how the data is generated, we can look for the other factors we are not being shown and resolve the paradox by either keeping the data separated, or aggregating the data."]}, {"block": 19, "type": "code", "linesLength": 0, "startIndex": 151, "lines": []}]