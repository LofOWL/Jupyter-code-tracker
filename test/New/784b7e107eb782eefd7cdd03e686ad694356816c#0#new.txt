[{"block": 0, "type": "markdown", "linesLength": 3, "startIndex": 0, "lines": ["# Introduction: Data Analysis of Medium Articles\n", "\n", "In this notebook, we'll do some basic data analysis of my medium articles. This is meant as a fun exercise that can also teach us a little about dealing with `HTML` files and making effective visuals."]}, {"block": 1, "type": "code", "linesLength": 17, "startIndex": 3, "lines": ["# Data science imports\n", "import pandas as pd\n", "import numpy as np\n", "\n", "# Options for pandas\n", "pd.options.display.max_columns = 20\n", "\n", "# Display all cell outputs\n", "from IPython.core.interactiveshell import InteractiveShell\n", "InteractiveShell.ast_node_interactivity = 'all'\n", "\n", "# Interactive plotting\n", "import plotly.plotly as py\n", "import plotly.graph_objs as go\n", "from plotly.offline import iplot\n", "import cufflinks\n", "cufflinks.go_offline()"]}, {"block": 2, "type": "markdown", "linesLength": 3, "startIndex": 20, "lines": ["# Parsing HTML with BeautifulSoup\n", "\n", "BeautifulSoup is the go-to method in Python for parsing HTML. To make a structured object (soup) out of HTML, we simply need to pass in the raw HTML. "]}, {"block": 3, "type": "code", "linesLength": 4, "startIndex": 23, "lines": ["from bs4 import BeautifulSoup\n", "\n", "soup = BeautifulSoup(open('data/published.html', 'r').read())\n", "soup.text[:100]"]}, {"block": 4, "type": "markdown", "linesLength": 3, "startIndex": 27, "lines": ["## Find All Articles\n", "\n", "Finding what we want from the soup is as easy as searching for different elements by the HTML tag or different attributes such as the `class`. Here we'll find all the entries on this page (which includes actual articles as well as responses). "]}, {"block": 5, "type": "code", "linesLength": 2, "startIndex": 30, "lines": ["entries = soup.find_all(attrs = {'class': 'bq y br af bs ag db dc dd c de df dg'})\n", "print(f'Found {len(entries)} entries.')"]}, {"block": 6, "type": "code", "linesLength": 1, "startIndex": 32, "lines": ["entries[-1]"]}, {"block": 7, "type": "markdown", "linesLength": 1, "startIndex": 33, "lines": ["(The [above article](https://medium.com/@williamkoehrsen/screw-the-environment-but-consider-your-wallet-a4f7cd3d3161) is one of my favorites!)"]}, {"block": 8, "type": "markdown", "linesLength": 1, "startIndex": 34, "lines": ["Later, we'll be able to sort out the articles because their pages have an `h1` header tag. For now, let's scrape the page for the title text and the reading times of all entries."]}, {"block": 9, "type": "code", "linesLength": 2, "startIndex": 35, "lines": ["titles = [e.text for e in entries]\n", "titles[-1]"]}, {"block": 10, "type": "markdown", "linesLength": 3, "startIndex": 37, "lines": ["## Find Reading Times\n", "\n", "The reading times can be extracted using a regular expression. We'll match the text to a regular expression."]}, {"block": 11, "type": "code", "linesLength": 3, "startIndex": 40, "lines": ["import re\n", "pattern = re.compile('[0-9]{1,} min read')\n", "pattern.findall(string = '15 min read')"]}, {"block": 12, "type": "code", "linesLength": 2, "startIndex": 43, "lines": ["read_times = soup.find_all(text = pattern)\n", "print(f'Found {len(read_times)} read times.')"]}, {"block": 13, "type": "markdown", "linesLength": 1, "startIndex": 45, "lines": ["Now for the best part: the total reading time of all articles. "]}, {"block": 14, "type": "code", "linesLength": 3, "startIndex": 46, "lines": ["read_times = [int(x.split(' ')[0]) for x in read_times]\n", "total_read_time = sum(read_times)\n", "print(f'Total Read Time of Entries: {total_read_time} minutes.')"]}, {"block": 15, "type": "markdown", "linesLength": 1, "startIndex": 49, "lines": ["This is somewhat inflated because some of these are responses. Later, we'll be able to sort out the responses."]}, {"block": 16, "type": "code", "linesLength": 5, "startIndex": 50, "lines": ["data = go.Histogram(x = read_times, xbins=dict(size=1), marker=dict(line=dict(color='black', width=1.2)))\n", "layout = go.Layout(title='Histogram of Read Times', \n", "                   yaxis=dict(title='Count'),\n", "                   xaxis=dict(title='Reading Time (minutes)'))\n", "figure = go.Figure(data=[data], layout=layout)"]}, {"block": 17, "type": "code", "linesLength": 1, "startIndex": 55, "lines": ["iplot(figure)"]}, {"block": 18, "type": "markdown", "linesLength": 3, "startIndex": 56, "lines": ["## Retrieving Articles\n", "\n", "Now that we have the basic metadata, we want to get the articles themselves. For this, we'll find the links associated with each entry. "]}, {"block": 19, "type": "code", "linesLength": 3, "startIndex": 59, "lines": ["entry_links = [entry.a.get_attribute_list('href')[0] for entry in entries]\n", "entry_links[-1]\n", "len(entry_links)"]}, {"block": 20, "type": "markdown", "linesLength": 1, "startIndex": 62, "lines": ["We can use the great `requests` library to retrieve all of the information on each of these linked pages."]}, {"block": 21, "type": "code", "linesLength": 7, "startIndex": 63, "lines": ["import requests\n", "\n", "entries = []\n", "for a in entry_links[:10]:\n", "    entries.append(requests.get(a).content)\n", "    \n", "entries[0][:10]"]}, {"block": 22, "type": "markdown", "linesLength": 1, "startIndex": 70, "lines": ["## Parsing Articles"]}, {"block": 23, "type": "markdown", "linesLength": 1, "startIndex": 71, "lines": ["We now have each entry as an HTML page. We can take the same approach as we did with the overall page to scrape whatever information we want. For example, we can find all the paragraphs on the page with the following:"]}, {"block": 24, "type": "code", "linesLength": 3, "startIndex": 72, "lines": ["entry_soup = BeautifulSoup(entries[1])\n", "entry_text = [p.text for p in entry_soup.find_all('p')]\n", "entry_text[0]"]}, {"block": 25, "type": "markdown", "linesLength": 1, "startIndex": 75, "lines": ["We can convert this to a single string with `join`. A rough word count can be found by then splitting this on spaces."]}, {"block": 26, "type": "code", "linesLength": 3, "startIndex": 76, "lines": ["entry_text = ' '.join(entry_text)\n", "word_count = len(entry_text.split(' '))\n", "print(f'There are {word_count} words in this article.')"]}, {"block": 27, "type": "markdown", "linesLength": 1, "startIndex": 79, "lines": ["We can also get the reading time (again) with a little parsing."]}, {"block": 28, "type": "code", "linesLength": 4, "startIndex": 80, "lines": ["read_time = entry_soup.find_all(attrs={'class': 'readingTime'})\n", "read_time[0].get('title')\n", "\n", "read_mins = int(read_time[0].get('title').split(' ')[0])"]}, {"block": 29, "type": "markdown", "linesLength": 3, "startIndex": 84, "lines": ["### Number of Claps\n", "\n", "The number of claps is hidden within an obscure class, but we can get it using this:"]}, {"block": 30, "type": "code", "linesLength": 4, "startIndex": 87, "lines": ["clap_pattern = re.compile('[0-9]{1,} claps')\n", "claps = entry_soup.find_all(text = clap_pattern)\n", "clap_number = int(claps[0].split(' ')[0])\n", "claps"]}, {"block": 31, "type": "markdown", "linesLength": 3, "startIndex": 91, "lines": ["## Determining Responses vs Articles\n", "\n", "The easiest way to figure out if an entry is a response or an article is to search for the `h1` header."]}, {"block": 32, "type": "code", "linesLength": 6, "startIndex": 94, "lines": ["if entry_soup.h1 is not None:\n", "    title = entry_soup.h1.text\n", "else:\n", "    title = 'response'\n", "    \n", "print(f'Found title: \"{title}\"')"]}, {"block": 33, "type": "markdown", "linesLength": 1, "startIndex": 100, "lines": ["To store all this information, let's use one of my favorite Python data types, a dictionary."]}, {"block": 34, "type": "code", "linesLength": 6, "startIndex": 101, "lines": ["from collections import defaultdict\n", "entry_dict = defaultdict(dict)\n", "entry_dict[title]['text'] = entry_text\n", "entry_dict[title]['word_count'] = word_count\n", "entry_dict[title]['clap_count'] = clap_number\n", "entry_dict[title]['read_time'] = read_mins"]}, {"block": 35, "type": "code", "linesLength": 1, "startIndex": 107, "lines": ["entry_dict.keys()"]}, {"block": 36, "type": "markdown", "linesLength": 3, "startIndex": 108, "lines": ["## Finding Published Time\n", "\n", "Another piece of information we may want is the published time of the article. This is simple enough to get."]}, {"block": 37, "type": "code", "linesLength": 2, "startIndex": 111, "lines": ["t = entry_soup.find_all('time')[0]\n", "t.get('datetime')"]}, {"block": 38, "type": "markdown", "linesLength": 1, "startIndex": 113, "lines": ["This is the time in UTC. We can use Pandas to convert to the local time of publication."]}, {"block": 39, "type": "code", "linesLength": 1, "startIndex": 114, "lines": ["pd.to_datetime(t.get('datetime'), utc=True).tz_convert('America/New_York')"]}, {"block": 40, "type": "markdown", "linesLength": 1, "startIndex": 115, "lines": ["(I didn't publish all of my articles in the eastern time zone, most were published in the midwest. However, I'm not sure how to get geographic information from these pages, so for now we'll just localize using Eastern time)."]}, {"block": 41, "type": "markdown", "linesLength": 3, "startIndex": 116, "lines": ["## Number of Responses\n", "\n", "As another piece of information, let's find the number of responses to the article."]}, {"block": 42, "type": "code", "linesLength": 4, "startIndex": 119, "lines": ["responses = entry_soup.find_all(attrs={'class': 'button button--chromeless u-baseColor--buttonNormal u-marginRight12'})\n", "responses\n", "\n", "n_responses = int(responses[0].text)"]}, {"block": 43, "type": "markdown", "linesLength": 3, "startIndex": 123, "lines": ["## Tags for Article\n", "\n", "The final piece of information we'll retrieve about the article is the tags associated with it. These are also easy to grab."]}, {"block": 44, "type": "code", "linesLength": 3, "startIndex": 126, "lines": ["tags = entry_soup.find_all(attrs={'class': 'tags tags--postTags tags--borderless'})\n", "tags = [li.text for li in tags[0].find_all('li')]\n", "tags"]}, {"block": 45, "type": "code", "linesLength": 2, "startIndex": 129, "lines": ["entry_dict[title]['num_responses'] = n_responses\n", "entry_dict[title]['tags'] = tags"]}, {"block": 46, "type": "markdown", "linesLength": 1, "startIndex": 131, "lines": ["At this point we have quick a lot of information about each entry. In the case where the entry is a response, we won't be able to gather as much data, but we are mostly focused on the articles anyway."]}, {"block": 47, "type": "code", "linesLength": 2, "startIndex": 132, "lines": ["import pprint\n", "pprint.pprint({key: value for key, value in entry_dict[title].items() if key != 'text'})"]}, {"block": 48, "type": "markdown", "linesLength": 3, "startIndex": 134, "lines": ["# Function for Parsing Articles\n", "\n", "We'll put all of the above steps into a single function that can grab the information from all the entries if passed a list of entry links. The return will be a dictionary containing the complete data about each entry."]}, {"block": 49, "type": "code", "linesLength": 72, "startIndex": 137, "lines": ["def get_entries(entry_links):\n", "    \"\"\"\n", "    Retrieve data of all entries in a list of links.\n", "    \n", "    :param entry_links: list of strings for links to entries\n", "    \n", "    :return entries: dictionary with information about each entry\n", "    \"\"\"\n", "    \n", "    entries = defaultdict(dict)\n", "    n_words = 0\n", "    \n", "    response_count = 0\n", "    # Iterate through all links\n", "    for i, link in enumerate(entry_links):\n", "        \n", "        # Tracking progress\n", "        if (i + 1) % 5 == 0:\n", "            print(f'{100 * i / len(entry_links):.2f}% complete. Total words = {n_words}.', end = '\\r')\n", "        \n", "        # Retrieve the article and create a soup\n", "        entry = requests.get(link).content\n", "        entry_soup = BeautifulSoup(entry)\n", "        \n", "        # Find the title header (determines if an article or a response)\n", "        if entry_soup.h1 is not None:\n", "            title = entry_soup.h1.text\n", "        else:\n", "            title = f'response-{response_count}'\n", "            response_count += 1\n", "            \n", "        # Text as single long string\n", "        entry_text = [p.text for p in entry_soup.find_all('p')]\n", "        entry_text = ' '.join(entry_text)\n", "        \n", "        # Word count\n", "        word_count = len(entry_text.split(' '))\n", "        n_words += word_count\n", "        \n", "        # Reading time in minutes\n", "        read_time = entry_soup.find_all(attrs={'class': 'readingTime'})\n", "        read_mins = int(read_time[0].get('title').split(' ')[0])\n", "        \n", "        # Number of claps\n", "        clap_pattern = re.compile('^[0-9]{1,} claps')\n", "        claps = entry_soup.find_all(text = clap_pattern)\n", "        \n", "        if len(claps) > 0:\n", "            clap_number = int(claps[0].split(' ')[0])\n", "        else:\n", "            clap_number = 0\n", "            \n", "        # Publication time\n", "        t = entry_soup.find_all('time')[0]\n", "        t = pd.to_datetime(t.get('datetime'), utc=True).tz_convert('America/New_York')\n", "        \n", "        # Post tags\n", "        tags = entry_soup.find_all(attrs={'class': 'tags tags--postTags tags--borderless'})\n", "        tags = [li.text for li in tags[0].find_all('li')]\n", "        \n", "        # Store in dictionary with title as key\n", "        entry_dict[title]['text'] = entry_text\n", "        entry_dict[title]['word_count'] = word_count\n", "        entry_dict[title]['read_time'] = read_mins\n", "        entry_dict[title]['claps'] = clap_number\n", "        entry_dict[title]['time_published'] = t\n", "        entry_dict[title]['tags'] = tags\n", "        \n", "    print(f'Found {len(entries) - responses} articles and {len(responses)} responses.')\n", "    print(f'Total words: {n_words}.')\n", "    \n", "    return articles"]}, {"block": 50, "type": "code", "linesLength": 3, "startIndex": 209, "lines": ["entry_soup = BeautifulSoup(requests.get('https://medium.com/@williamkoehrsen/one-of-the-best-parts-about-featuretools-is-that-its-not-constrained-by-human-limits-on-creativity-daa0b40406d6').content)\n", "clap_pattern = re.compile('^[0-9]{1,} claps')\n", "entry_soup.find_all(text=clap_pattern)"]}, {"block": 51, "type": "code", "linesLength": 1, "startIndex": 212, "lines": ["entry_dict = get_entries(entry_links=entry_links)"]}, {"block": 52, "type": "markdown", "linesLength": 3, "startIndex": 213, "lines": ["## Dataframe of Results\n", "\n", "Converting this dictionary to a dataframe is relatively simple. We just need to call `DataFrame` and then tranpose the columns (each of which is an entry) to be the rows. This puts the dataframe in the familiar wide format which is best for plotting. We can also make a column indicating if the entry was a response or an article."]}, {"block": 53, "type": "code", "linesLength": 3, "startIndex": 216, "lines": ["data = pd.DataFrame(all_articles_dict).transpose().reset_index().rename(columns={'index': 'title'})\n", "data['response'] = ['response' if x == True else 'article' for x in data['title'].str.contains('response')]\n", "data.head()"]}, {"block": 54, "type": "code", "linesLength": 2, "startIndex": 219, "lines": ["data['n_words'].iplot(kind='hist', xTitle='num words', yTitle='Count', \n", "                      title='Histogram of Number of Words')"]}, {"block": 55, "type": "code", "linesLength": 2, "startIndex": 221, "lines": ["data.iplot(x='date', y='n_words', opacity=0.4, mode='markers', xTitle='Date', yTitle='num_words',\n", "           title='Number of Words vs Date', text = 'title')"]}, {"block": 56, "type": "code", "linesLength": 4, "startIndex": 223, "lines": ["\n", "data.iplot(x='date', y='n_words', opacity=0.8, mode='markers', xTitle='Date', yTitle='num_words',\n", "           title='Number of Words vs Date', categories='response', \n", "           text = 'title')"]}, {"block": 57, "type": "markdown", "linesLength": 1, "startIndex": 227, "lines": ["# Articles Over Time"]}, {"block": 58, "type": "code", "linesLength": 14, "startIndex": 228, "lines": ["def get_links(soup):\n", "    \"\"\"Retrieve all links within webpage to articles\"\"\"\n", "    titles = soup.find_all(attrs = {'class': 'bq y br af bs ag db dc dd c de df dg'})\n", "    print(f'Found {len(titles)} articles.')\n", "    \n", "    pattern = re.compile('[0-9] min read')\n", "    pattern.findall(string = '1 min read')\n", "    read_times = soup.find_all(text = pattern)\n", "    total_read_time = sum([int(x.split(' ')[0]) for x in read_times])\n", "    \n", "    print(f'Total Read Time of Articles: {total_read_time} minutes.')\n", "    article_links = [title.a.get_attribute_list('href')[0] for title in titles]\n", "    \n", "    return article_links"]}, {"block": 59, "type": "code", "linesLength": 2, "startIndex": 242, "lines": ["unlisted_soup = BeautifulSoup(open('data/unlisted.html', 'r').read())\n", "unlisted_soup.text[:100]"]}, {"block": 60, "type": "code", "linesLength": 1, "startIndex": 244, "lines": ["unlisted_links = get_links(unlisted_soup)"]}, {"block": 61, "type": "code", "linesLength": 1, "startIndex": 245, "lines": ["unlisted_articles = get_articles(unlisted_links)"]}, {"block": 62, "type": "code", "linesLength": 0, "startIndex": 246, "lines": []}]