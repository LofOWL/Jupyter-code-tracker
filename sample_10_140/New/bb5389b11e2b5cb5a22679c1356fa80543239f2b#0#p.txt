[{"block": 0, "type": "markdown", "linesLength": 3, "startIndex": 0, "lines": ["<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n", "\n", "<i>Licensed under the MIT License.</i>"]}, {"block": 1, "type": "markdown", "linesLength": 4, "startIndex": 3, "lines": ["# TF-IDF Content-Based Recommendation on the COVID-19 Open Research Dataset\n", "This demonstrates a simple implementation of Term Frequency Inverse Document Frequency (TF-IDF) content-based recommendation on the [COVID-19 Open Research Dataset](https://azure.microsoft.com/en-us/services/open-datasets/catalog/covid-19-open-research/), hosted through Azure Open Datasets.\n", "\n", "In this notebook, we will create a recommender which will return the top k recommended articles similar to any article of interest (query item) in the COVID-19 Open Reserach Dataset."]}, {"block": 2, "type": "code", "linesLength": 10, "startIndex": 7, "lines": ["# Set the environment path to find Recommenders\n", "import sys\n", "sys.path.append(\"../../\")\n", "\n", "# Import functions\n", "from reco_utils.dataset.covid_utils import *\n", "from reco_utils.recommender.tfidf.tfidf_utils import *\n", "\n", "# Print version\n", "print(\"System version: {}\".format(sys.version))"]}, {"block": 3, "type": "markdown", "linesLength": 1, "startIndex": 17, "lines": ["Set the default parameters for accessing the dataset."]}, {"block": 4, "type": "code", "linesLength": 5, "startIndex": 18, "lines": ["# Parameters for the COVID-19 dataset in Azure Open Datasets\n", "azure_storage_account_name='azureopendatastorage'\n", "azure_storage_sas_token='sv=2019-02-02&ss=bfqt&srt=sco&sp=rlcup&se=2025-04-14T00:21:16Z&st=2020-04-13T16:21:16Z&spr=https&sig=JgwLYbdGruHxRYTpr5dxfJqobKbhGap8WUtKFadcivQ%3D'\n", "container_name='covid19temp'\n", "metadata_filename='metadata.csv'"]}, {"block": 5, "type": "markdown", "linesLength": 2, "startIndex": 23, "lines": ["### 1. Download the dataset\n", "Let's begin by downloading the metadata file for the dataset. This file contains metadata about each of the scientific articles included in the full dataset."]}, {"block": 6, "type": "code", "linesLength": 3, "startIndex": 25, "lines": ["# Get metadata (may take around 1-2 min)\n", "blob_service = get_blob_service(azure_storage_account_name, azure_storage_sas_token, container_name)\n", "metadata = download_metadata(blob_service, container_name, metadata_filename)"]}, {"block": 7, "type": "markdown", "linesLength": 2, "startIndex": 28, "lines": ["### 2. Extract articles in the public domain\n", "The dataset contains articles using a variety of licenses. We will only be using articles that fall under the public domain ([cc0](https://creativecommons.org/publicdomain/zero/1.0/))."]}, {"block": 8, "type": "code", "linesLength": 2, "startIndex": 30, "lines": ["# View distribution of license types in the dataset\n", "metadata['license'].value_counts().plot(kind='bar', title='License')"]}, {"block": 9, "type": "code", "linesLength": 5, "startIndex": 32, "lines": ["# Extract metadata on public domain articles only\n", "metadata_public = extract_public_domain(metadata)\n", "\n", "# Clean dataframe\n", "metadata_public = clean_dataframe(metadata_public)"]}, {"block": 10, "type": "markdown", "linesLength": 1, "startIndex": 37, "lines": ["Let's look at the top few rows of this dataframe which contains metadata on public domain articles."]}, {"block": 11, "type": "code", "linesLength": 4, "startIndex": 38, "lines": ["# Preview metadata for public domain articles\n", "print('Number of articles in dataset: ' + str(len(metadata)))\n", "print('Number of articles in dataset that fall under the public domain (cc0): ' + str(len(metadata_public)))\n", "metadata_public.head()"]}, {"block": 12, "type": "markdown", "linesLength": 2, "startIndex": 42, "lines": ["### 3. Retrieve full article text\n", "Now that we have the metadata for the public domain articles as its own dataframe, let's retrieve the full text for each public domain scientific article."]}, {"block": 13, "type": "code", "linesLength": 2, "startIndex": 44, "lines": ["# Extract text from all public domain articles (may take 2-3 min)\n", "all_text = get_public_domain_text(metadata_public, blob_service, container_name)"]}, {"block": 14, "type": "markdown", "linesLength": 1, "startIndex": 46, "lines": ["Notice that **all_text** is the same as **metadata_public** but now has an additional column called **full_text** which contains the full text for each respective article."]}, {"block": 15, "type": "code", "linesLength": 2, "startIndex": 47, "lines": ["# Preview\n", "all_text.head()"]}, {"block": 16, "type": "markdown", "linesLength": 4, "startIndex": 49, "lines": ["### 4. Prepare text for use in the TF-IDF recommender\n", "The raw text retrieved for each article requires basic cleaning prior to being used in the TF-IDF model.\n", "\n", "Let's look at the full_text from the first article in our dataframe as an example."]}, {"block": 17, "type": "code", "linesLength": 2, "startIndex": 53, "lines": ["# Preview the full scientific text from one example\n", "print(all_text['full_text'][0])"]}, {"block": 18, "type": "markdown", "linesLength": 3, "startIndex": 55, "lines": ["As seen above, there are some special characters (such as \u2022 \u25b2 \u25a0 \u2265 \u00b0) and punctuation which should be removed prior to using the text as input. Casing (capitalization) is preserved for [BERT-based tokenization methods](https://huggingface.co/transformers/model_doc/bert.html), but is removed for simple or no tokenization.\n", "\n", "Let's join together the **abstract** and **full_text** columns and clean them for future use in the TF-IDF model."]}, {"block": 19, "type": "code", "linesLength": 3, "startIndex": 58, "lines": ["# Assign columns to clean and combine\n", "cols_to_clean = ['abstract','full_text']\n", "df_clean = clean_dataframe_for_rec(all_text, cols_to_clean, for_BERT=True)"]}, {"block": 20, "type": "code", "linesLength": 2, "startIndex": 61, "lines": ["# Preview the dataframe with the cleaned text\n", "df_clean.head()"]}, {"block": 21, "type": "code", "linesLength": 2, "startIndex": 63, "lines": ["# Preview the cleaned version of the previous example\n", "df_clean['cleaned_text'][0]"]}, {"block": 22, "type": "markdown", "linesLength": 11, "startIndex": 65, "lines": ["### 5. Rank articles by similarity using TF-IDF\n", "For each public domain article, generate a sorted list of all the other public domain articles in the dataset. This list is sorted by each article's similarity to the article of interest, calculated using cosine similarity on the respective TF-IDF vectors.\n", "\n", "Select one of the following tokenization methods to use in the model:\n", "\n", "| tokenization_method | Description                                                                                                                      |\n", "|:--------------------|:---------------------------------------------------------------------------------------------------------------------------------|\n", "| 'none'              | No tokenization is applied. Each word is considered a token.                                                                     |\n", "| 'nltk'              | Simple stemming is applied using NLTK.                                                                                           |\n", "| 'bert'              | HuggingFace BERT word tokenization ('bert-base-cased') is applied.                                                               |\n", "| 'scibert'           | SciBERT word tokenization ('allenai/scibert_scivocab_cased') is applied.<br>This is recommended for scientific journal articles. |"]}, {"block": 23, "type": "code", "linesLength": 13, "startIndex": 76, "lines": ["# TF-IDF with tokenization_method of 'nltk', 'bert', 'scibert', or 'none'\n", "results = recommend_with_tfidf(df_clean,\n", "                               text_col='cleaned_text',\n", "                               id_col='cord_uid',\n", "                               title_col='title',\n", "                               tokenization_method='scibert')\n", "\n", "# Organize results as an easy-to-read table (may take about 30 sec)\n", "num_of_recs_to_keep = 5\n", "rec_table = organize_results_as_tabular(df_clean,\n", "                                        results,\n", "                                        id_col='cord_uid',\n", "                                        k=num_of_recs_to_keep)"]}, {"block": 24, "type": "markdown", "linesLength": 3, "startIndex": 89, "lines": ["In our recommendation table, each row representats a single recommendation.\n", "\n", "**cord_uid** and **title** correspond to the article that is being used to make recommendations from. **rec_rank** contains the recommdation's rank (e.g., rank of 1 means top recommendation). **rec_score** is the cosine similarity score between the query article and the recommended article. **rec_cord_uid** and **rec_title** correspond to the recommended article."]}, {"block": 25, "type": "code", "linesLength": 2, "startIndex": 92, "lines": ["# Preview the recommendations\n", "rec_table"]}, {"block": 26, "type": "markdown", "linesLength": 2, "startIndex": 94, "lines": ["### 6. Display top recommendations for article of interest\n", "Now that we have the recommendation table containing IDs for both query and recommended articles, we can easily return the full metadata for the top k recommendations for any given article."]}, {"block": 27, "type": "code", "linesLength": 1, "startIndex": 96, "lines": ["display_top_recommendations(rec_table, metadata_public, query_id='ej795nks', id_col='cord_uid')"]}, {"block": 28, "type": "markdown", "linesLength": 1, "startIndex": 97, "lines": ["In this notebook, we have demonstrated how to create a TF-IDF recommender to recommend the top k (in this case 5) articles similar in content to an article of interest (in this example, article with cord_uid='ej795nks')."]}]