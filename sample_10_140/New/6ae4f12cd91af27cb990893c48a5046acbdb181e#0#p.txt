[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["## utils.mem"]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 1, "lines": ["Utility functions for memory management (mainly GPU)."]}, {"block": 2, "type": "code", "linesLength": 2, "startIndex": 2, "lines": ["from fastai.gen_doc.nbdoc import *\n", "from fastai.utils.mem import * "]}, {"block": 3, "type": "code", "linesLength": 1, "startIndex": 4, "lines": ["show_doc(gpu_mem_get)"]}, {"block": 4, "type": "markdown", "linesLength": 5, "startIndex": 5, "lines": ["[`gpu_mem_get`](/utils.mem.html#gpu_mem_get)\n", "\n", "* for gpu returns `GPUMemory(total, used, free)`\n", "* for cpu returns `GPUMemory(0, 0, 0)`\n", "* for invalid gpu id returns `GPUMemory(0, 0, 0)`"]}, {"block": 5, "type": "code", "linesLength": 1, "startIndex": 10, "lines": ["show_doc(gpu_mem_get_all)"]}, {"block": 6, "type": "markdown", "linesLength": 3, "startIndex": 11, "lines": ["[`gpu_mem_get_all`](/utils.mem.html#gpu_mem_get_all)\n", "* for gpu returns `[ GPUMemory(total_0, used_0, free_0), GPUMemory(total_1, used_1, free_1), .... ]`\n", "* for cpu returns `[]`\n"]}, {"block": 7, "type": "code", "linesLength": 1, "startIndex": 14, "lines": ["show_doc(gpu_mem_get_free_no_cache)"]}, {"block": 8, "type": "markdown", "linesLength": 1, "startIndex": 15, "lines": ["[`gpu_mem_get_free_no_cache`](/utils.mem.html#gpu_mem_get_free_no_cache)"]}, {"block": 9, "type": "code", "linesLength": 1, "startIndex": 16, "lines": ["show_doc(gpu_mem_get_used_no_cache)"]}, {"block": 10, "type": "markdown", "linesLength": 1, "startIndex": 17, "lines": ["[`gpu_mem_get_used_no_cache`](/utils.mem.html#gpu_mem_get_used_no_cache)"]}, {"block": 11, "type": "code", "linesLength": 1, "startIndex": 18, "lines": ["show_doc(gpu_mem_get_used_fast)"]}, {"block": 12, "type": "markdown", "linesLength": 1, "startIndex": 19, "lines": ["[`gpu_mem_get_used_fast`](/utils.mem.html#gpu_mem_get_used_fast)"]}, {"block": 13, "type": "code", "linesLength": 1, "startIndex": 20, "lines": ["show_doc(gpu_with_max_free_mem)"]}, {"block": 14, "type": "markdown", "linesLength": 3, "startIndex": 21, "lines": ["[`gpu_with_max_free_mem`](/utils.mem.html#gpu_with_max_free_mem):\n", "* for gpu returns: `gpu_with_max_free_ram_id, its_free_ram`\n", "* for cpu returns: `None, 0`\n"]}, {"block": 15, "type": "code", "linesLength": 1, "startIndex": 24, "lines": ["show_doc(preload_pytorch)"]}, {"block": 16, "type": "markdown", "linesLength": 1, "startIndex": 25, "lines": ["[`preload_pytorch`](/utils.mem.html#preload_pytorch) is helpful when GPU memory is being measured, since the first time any operation on `cuda` is performed by pytorch, usually about 0.5GB gets used by CUDA context."]}, {"block": 17, "type": "code", "linesLength": 1, "startIndex": 26, "lines": ["show_doc(GPUMemory)"]}, {"block": 18, "type": "markdown", "linesLength": 1, "startIndex": 27, "lines": ["[`GPUMemory`](/utils.mem.html#GPUMemory) is a namedtuple that is returned by functions like [`gpu_mem_get`](/utils.mem.html#gpu_mem_get) and [`gpu_mem_get_all`](/utils.mem.html#gpu_mem_get_all)."]}, {"block": 19, "type": "code", "linesLength": 1, "startIndex": 28, "lines": ["show_doc(b2mb)"]}, {"block": 20, "type": "markdown", "linesLength": 1, "startIndex": 29, "lines": ["[`b2mb`](/utils.mem.html#b2mb) is a helper utility that just does `int(bytes/2**20)`"]}, {"block": 21, "type": "markdown", "linesLength": 1, "startIndex": 30, "lines": ["## Memory Tracing Utils"]}, {"block": 22, "type": "code", "linesLength": 1, "startIndex": 31, "lines": ["show_doc(GPUMemTrace)"]}, {"block": 23, "type": "markdown", "linesLength": 27, "startIndex": 32, "lines": ["Usage examples:\n", "```\n", "memtrace = GPUMemTrace()\n", "memtrace.start() start tracing\n", "\n", "some_code()\n", "memtrace.report() print intermediary cumulative report\n", "used, peak =  memtrace.data() same but as data\n", "\n", "some_code()\n", "memtrace.report('2nd run') print intermediary cumulative report\n", "used, peak =  memtrace.data()\n", "\n", "for i in range(10):\n", "    memtrace.reset()\n", "    code()\n", "    memtrace.report(f'i={i}') report for just the last code run since reset\n", "\n", "# combine report+reset\n", "memtrace.reset()\n", "for i in range(10):\n", "    code()\n", "    memtrace.report_n_reset(f'i={i}') report for just the last code run since reset\n", "\n", "memtrace.stop() # stop the monitor thread\n", "\n", "```"]}, {"block": 24, "type": "markdown", "linesLength": 6, "startIndex": 59, "lines": ["## Workarounds to the leaky ipython traceback on exception\n", "\n", "ipython has a feature where it stores tb with all the `locals()` tied in, which\n", "prevents `gc.collect()` from freeing those variables and leading to a leakage.\n", "\n", "Therefore we cleanse the tb before handing it over to ipython. The 2 ways of doing it are by either using the [`gpu_mem_restore`](/utils.mem.html#gpu_mem_restore) decorator or the [`gpu_mem_restore_ctx`](/utils.mem.html#gpu_mem_restore_ctx) context manager which are described next:"]}, {"block": 25, "type": "code", "linesLength": 1, "startIndex": 65, "lines": ["show_doc(gpu_mem_restore)"]}, {"block": 26, "type": "markdown", "linesLength": 12, "startIndex": 66, "lines": ["[`gpu_mem_restore`](/utils.mem.html#gpu_mem_restore) is a decorator to be used with any functions that interact with CUDA (top-level is fine)\n", "\n", "* under non-ipython environment it doesn't do anything.\n", "* under ipython currently it strips tb by default only for the \"CUDA out of memory\" exception.\n", "\n", "The env var `FASTAI_TB_CLEAR_FRAMES` changes this behavior when run under ipython,\n", "depending on its value: \n", "\n", "* \"0\": never  strip tb (makes it possible to always use `%debug` magic, but with leaks)\n", "* \"1\": always strip tb (never need to worry about leaks, but `%debug` won't work)\n", "\n", "e.g. `os.environ['FASTAI_TB_CLEAR_FRAMES']=\"0\"` will set it to 0.\n"]}, {"block": 27, "type": "code", "linesLength": 1, "startIndex": 78, "lines": ["show_doc(gpu_mem_restore_ctx)"]}, {"block": 28, "type": "markdown", "linesLength": 6, "startIndex": 79, "lines": ["if function decorator is not a good option, you can use a context manager instead. For example:\n", "```\n", "with gpu_mem_restore_ctx():\n", "   learn.fit_one_cycle(1,1e-2)\n", "```\n", "This particular one will clear tb on any exception."]}, {"block": 29, "type": "markdown", "linesLength": 1, "startIndex": 85, "lines": ["## Undocumented Methods - Methods moved below this line will intentionally be hidden"]}]