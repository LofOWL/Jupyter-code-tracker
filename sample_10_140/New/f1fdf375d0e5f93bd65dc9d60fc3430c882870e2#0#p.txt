[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["# TrainingPhase and General scheduler"]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 1, "lines": ["Creates a scheduler that lets you train a model with following different [`TrainingPhase`](/callbacks.general_sched.html#TrainingPhase)."]}, {"block": 2, "type": "code", "linesLength": 4, "startIndex": 2, "lines": ["from fastai.gen_doc.nbdoc import *\n", "from fastai.callbacks.general_sched import * \n", "from fastai import *\n", "from fastai.vision import *"]}, {"block": 3, "type": "code", "linesLength": 1, "startIndex": 6, "lines": ["show_doc(TrainingPhase, doc_string=False)"]}, {"block": 4, "type": "markdown", "linesLength": 1, "startIndex": 7, "lines": ["Create a phase for training a model during `length` iterations, following a schedule given by `lrs` and `lr_anneal`, `moms` and `mom_anneal`. More specifically, the phase will make the learning rate (or momentum) vary from the first value of `lrs` (or `moms`) to the second, following `lr_anneal` (or `mom_anneal`). If an annealing function is speficied but `lrs` or `moms` is a float, it will decay to 0. If no annealing function is specified, the default is a linear annealing if `lrs` (or `moms`) is a tuple, a constant parameter if it's a float. "]}, {"block": 5, "type": "code", "linesLength": 2, "startIndex": 8, "lines": ["jekyll_note(\"\"\"If you want to use discriminative learning rates, you can pass an numpy array of learning rate (or a tuple\n", "of them for start and stop).\"\"\")"]}, {"block": 6, "type": "code", "linesLength": 1, "startIndex": 10, "lines": ["show_doc(GeneralScheduler)"]}, {"block": 7, "type": "markdown", "linesLength": 1, "startIndex": 11, "lines": ["### Callback methods"]}, {"block": 8, "type": "markdown", "linesLength": 1, "startIndex": 12, "lines": ["You don't call these yourself - they're called by fastai's [`Callback`](/callback.html#Callback) system automatically to enable the class's functionality."]}, {"block": 9, "type": "code", "linesLength": 1, "startIndex": 13, "lines": ["show_doc(GeneralScheduler.on_batch_end, doc_string=False)"]}, {"block": 10, "type": "markdown", "linesLength": 1, "startIndex": 14, "lines": ["Takes a step in the current phase and prepare the hyperparameters for the next batch."]}, {"block": 11, "type": "code", "linesLength": 1, "startIndex": 15, "lines": ["show_doc(GeneralScheduler.on_train_begin, doc_string=False)"]}, {"block": 12, "type": "markdown", "linesLength": 1, "startIndex": 16, "lines": ["Initiates the hyperparameters to the start values of the first phase. "]}, {"block": 13, "type": "markdown", "linesLength": 1, "startIndex": 17, "lines": ["Let's make an example by using this to code [SGD with warm restarts](https://arxiv.org/abs/1608.03983)."]}, {"block": 14, "type": "code", "linesLength": 9, "startIndex": 18, "lines": ["def fit_sgd_warm(learn, n_cycles, lr, mom, cycle_len, cycle_mult):\n", "    n = len(learn.data.train_dl)\n", "    phases = [TrainingPhase(n * (cycle_len * cycle_mult**i), lr, mom, lr_anneal=annealing_cos) for i in range(n_cycles)]\n", "    sched = GeneralScheduler(learn, phases)\n", "    learn.callbacks.append(sched)\n", "    if cycle_mult != 1:\n", "        total_epochs = int(cycle_len * (1 - (cycle_mult)**n_cycles)/(1-cycle_mult)) \n", "    else: total_epochs = n_cycles * cycle_len\n", "    learn.fit(total_epochs)"]}, {"block": 15, "type": "code", "linesLength": 4, "startIndex": 27, "lines": ["path = untar_data(URLs.MNIST_SAMPLE)\n", "data = ImageDataBunch.from_folder(path)\n", "learn = Learner(data, simple_cnn((3,16,16,2)), metrics=accuracy)\n", "fit_sgd_warm(learn, 3, 1e-3, 0.9, 1, 2)"]}, {"block": 16, "type": "code", "linesLength": 1, "startIndex": 31, "lines": ["learn.recorder.plot_lr()"]}, {"block": 17, "type": "markdown", "linesLength": 1, "startIndex": 32, "lines": ["## Undocumented Methods - Methods moved below this line will intentionally be hidden"]}]