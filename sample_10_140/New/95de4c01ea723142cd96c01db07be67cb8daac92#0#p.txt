[{"block": 0, "type": "markdown", "linesLength": 9, "startIndex": 0, "lines": ["## PmagPy Online: Jupyter Notebooks, the PmagPy Software Package and the Magnetics Information Consortium (MagIC) Database\n", "\n", "Lisa Tauxe$^1$, Rupert Minnett$^2$, Nick Jarboe$^1$, Catherine Constable$^1$, Anthony Koppers$^2$, Lori Jonestrask$^1$, Nick Swanson-Hysell$^3$\n", "\n", "$^1$Scripps Institution of Oceanography, United States of America;  $^2$   Oregon State University; $^3$ University of California, Berkely; ltauxe@ucsd.edu\n", "\n", "The Magnetics Information Consortium (MagIC), hosted at http://earthref.org/MagIC is a database that serves as a Findable, Accessible, Interoperable, Reusable (FAIR) archive for paleomagnetic and rock magnetic data. It has a flexible, comprehensive data model that can accomodate most kinds of paleomagnetic data. The **PmagPy** software package is a cross-platform and open-source set of tools written in Python for the analysis of paleomagnetic data that serves as one interface to MagIC, accommodating various levels of user expertise. It is available through github.com/PmagPy. Because PmagPy requires installation of python and the software package, there is a speed bump for many practitioners on beginning to use the software. In order to make the software and MagIC more accessible to the broad spectrum of scientists interested in paleo and rock magnetism, we have prepared a set of Jupyter notebooks, hosted on [jupyterhub.earthref.org](https://jupyterhub.earthref.org) which serve a set of purposes. 1) There is a complete course in Python for Earth Scientists, 2) a set of notebooks that introduce PmagPy (drawing the software package from the github repository) and illustrate how it can be used to create data products and figures for typical papers, and 3) show how to prepare data from the laboratory to upload into the MagIC database. The latter will satisfy expectations from NSF for data archiving and for example the AGU publication data archiving requirements.\n", "\n", "\n"]}, {"block": 1, "type": "markdown", "linesLength": 11, "startIndex": 9, "lines": ["### Getting started\n", "\n", "- Go to the jupyter-hub website at https://jupyterhub.earthref.org/ to run this online.  You will have to log in to the earthref website with your ORCID, but then you will have a workspace to use this and the other PmagPy jupyter notebooks.\n", "\n", "- Alternatively, you can  install Python and the  PmagPy software package on your computer (see [https://earthref.org/PmagPy/cookbook](https://earthref.org/PmagPy/cookbook) for instructions).   Follow  the instructions for  \"Full PmagPy install and update\" through section 1.4 (Quickstart with PmagPy notebooks).  This notebook is in  the collection of PmagPy notebooks. \n", "\n", "- Click on the cell below and then click on 'Run' from the menu above to import the desired functionality\n", "\n", "- To avoid overwriting this notebook if you make changes, select File => Make a Copy.  You can rename it as you like. Be sure to click on the save file icon (little diskette) or under File => Save and checkpoint.  \n", "\n", "- To understand what a particular Python or **PmagPy** function expects as input and delivers, use the Python _help_ function, e.g., help(print)."]}, {"block": 2, "type": "code", "linesLength": 1, "startIndex": 20, "lines": ["help(print)"]}, {"block": 3, "type": "code", "linesLength": 26, "startIndex": 21, "lines": ["# Import PmagPy modules\n", "import pmagpy.pmag as pmag\n", "import pmagpy.pmagplotlib as pmagplotlib\n", "import pmagpy.ipmag as ipmag\n", "\n", "# Import plotting modules\n", "has_cartopy, Cartopy = pmag.import_cartopy() # import mapping module, if it is available\n", "import matplotlib.pyplot as plt # our plotting buddy\n", "# This allows you to make matplotlib plots inside the notebook.  \n", "%matplotlib inline \n", "\n", "# Import more useful modules\n", "import numpy as np # the fabulous NumPy package\n", "import pandas as pd # and  Pandas for data wrangling\n", "import os # some useful operating system functions\n", "from importlib import reload # for reloading module if they get changed after initial import\n", "from IPython.display import Image\n", "import imageio # for making animations\n", "\n", "# make a directory for use with this notebook\n", "dirs=os.listdir() # get a list of directories in this one\n", "if 'MagIC_online' not in dirs:\n", "    os.mkdir(\"MagIC_online\")\n", "    print ('MagIC_online directory created')\n", "else:\n", "    print ('MagIC_online directory already exists')"]}, {"block": 4, "type": "markdown", "linesLength": 15, "startIndex": 47, "lines": ["### Overview of   MagIC\n", "\n", " \n", "The Magnetics Information Consortium (MagIC), hosted at http://earthref.org/MagIC is a database that serves as a Findable, Accessible, Interoperable, Reusable (FAIR) archive for paleomagnetic and rock magnetic data. Its datamodel is fully described here: [https://www2.earthref.org/MagIC/data-models/3.0](https://www2.earthref.org/MagIC/data-models/3.0). Each contribution is associated with a publication via the DOI.  There are nine data tables:\n", "\n", "- contribution: metadata of the associated publication.\n", "- locations: metadata for locations, which are groups of sites (e.g., stratigraphic section, region, etc.)\n", "- sites: metadata and derived data at the site level (units with a common expectation)\n", "- samples: metadata and derived data at the sample level.\n", "- specimens: metadata and derived data at the specimen level.\n", "- criteria: criteria by which data are deemed acceptable\n", "- ages: ages and metadata for sites/samples/specimens\n", "- images: associated images and plots.  \n", "\n", "Here we will show examples of how to download and import MagIC data and make some useful plots."]}, {"block": 5, "type": "markdown", "linesLength": 19, "startIndex": 62, "lines": ["### Overview of   PmagPy\n", "\n", "The functionality of **PmagPy** is demonstrated within three other notebooks in the **PmagPy** repository:\n", "\n", "- [PmagPy_calculations.ipynb](PmagPy_calculations.ipynb):  demonstrates many of the PmagPy calculation functions such as those that rotate directions, return statistical parameters, and simulate data from specified distributions. \n", "- [PmagPy_plots_analysis.ipynb](PmagPy_plots_analysis.ipynb): demonstrates PmagPy functions that can be used to visual data as well as those that conduct statistical tests that have associated visualizations.\n", "- [PmagPy_MagIC.ipynb](PmagPy_MagIC.ipynb): demonstrates how PmagPy can be used to read and write data to and from the MagIC database format including conversion from many individual lab measurement file formats.\n", "\n", "Please see also our YouTube channel with more presentations from the 2020 MagIC workshop here: \n", "[https://www.youtube.com/playlist?list=PLirL2unikKCgUkHQ3m8nT29tMCJNBj4kj](https://www.youtube.com/playlist?list=PLirL2unikKCgUkHQ3m8nT29tMCJNBj4kj)\n", "\n", "The current notebook will highlight the link between **PmagPy** and the Findable Accessible Interoperable Reusabe (FAIR) database maintained by the Magnetics Information Consortium (MagIC) at [https://earthref.org/MagIC](https://eathref.org/MagIC).  \n", "\n", "- [Importing data from MagIC](#Importing-data-directly-from-MagIC)\n", "\n", "- [Importing MagIC formatted data from desktop](#Importing-MagIC-format-data-from-desktop)\n", "\n", "\n", "\n"]}, {"block": 6, "type": "markdown", "linesLength": 13, "startIndex": 81, "lines": ["### Importing data directly from MagIC\n", "\n", "- Hunt around the earthref.org/MagIC/search page for a data set you would like to look at. We will use the data of Behar et al., 2019,  DOI: 10.1029/2019GC008479 for this example.  \n", "- Once you have the DOI or the MagIC ID number (which in this case is 16676, there are two ways to import the contribution file: 1) with the MagIC ID (id=16676) using **ipmag.download_magic_from_id()** and one with the DOI using **ipmag.download_magic_from_doi()**. \n", "- create the data directory to store the data in.\n", "- import the magic contribution file, move it to your directory (in this example, MagIC\\_import), and unpack it with ipmag.download_magic().  \n", "- Use PmagPy functions to make the following plots:\n", "    - use _ipmag.eqarea_magic()_ to make an equal area plot\n", "    - use _ipmag.vgpmap_magic()_ to make a map of VGPs\n", "    - use _ipmag.reversal_test_bootstrap()_ for a bootstrap reversals test\n", "    - use _pmagplotlib.plot_map()_ to make a site map\n", "    \n", "- Execute (run) each of the cells below in turn.  "]}, {"block": 7, "type": "code", "linesLength": 7, "startIndex": 94, "lines": ["# set up the directory structure for this example, if not already present:\n", "dirs=os.listdir('MagIC_online') # get a list of directories in this one\n", "if 'MagIC_import' not in dirs:\n", "    os.mkdir(\"MagIC_online/MagIC_import\")\n", "    print ('MagIC_import directory created')\n", "else:\n", "    print ('MagIC_import directory already exists')"]}, {"block": 8, "type": "markdown", "linesLength": 3, "startIndex": 101, "lines": ["Let's try the contribution ID way first:  \n", "\n", "First we need to learn how **ipmag.download_magic_from_id()** works. One way is with the python **help()** function: "]}, {"block": 9, "type": "code", "linesLength": 1, "startIndex": 104, "lines": ["help(ipmag.download_magic_from_id)"]}, {"block": 10, "type": "code", "linesLength": 6, "startIndex": 105, "lines": ["dir_path='MagIC_online/MagIC_import' # set the path to the correct working directory\n", "magic_id='16676' # set the magic ID number\n", "magic_contribution='magic_contribution_'+magic_id+'.txt' # set the file name string\n", "ipmag.download_magic_from_id(magic_id) # download the contribution from MagIC\n", "os.rename(magic_contribution, dir_path+'/'+magic_contribution) # move the contribution to the directory\n", "ipmag.download_magic(magic_contribution,dir_path=dir_path,print_progress=False) # unpack the file"]}, {"block": 11, "type": "markdown", "linesLength": 1, "startIndex": 111, "lines": ["Now let's try to do this with the API for DOIs:"]}, {"block": 12, "type": "code", "linesLength": 6, "startIndex": 112, "lines": ["dir_path='MagIC_online/MagIC_import' # set the path to the correct working directory\n", "reference_doi='10.1029/2019GC008479'\n", "magic_contribution='magic_contribution.txt'\n", "ipmag.download_magic_from_doi(reference_doi)\n", "os.rename(magic_contribution, dir_path+'/'+magic_contribution)\n", "ipmag.download_magic(magic_contribution,dir_path=dir_path,print_progress=False)"]}, {"block": 13, "type": "markdown", "linesLength": 1, "startIndex": 118, "lines": ["Now we can get to the fun stuff of making plots.  "]}, {"block": 14, "type": "markdown", "linesLength": 2, "startIndex": 119, "lines": ["### Equal area net example\n", "- use ipmag.eqarea_magic()"]}, {"block": 15, "type": "code", "linesLength": 2, "startIndex": 121, "lines": ["# first get help on how to use it:\n", "help(ipmag.eqarea_magic)"]}, {"block": 16, "type": "code", "linesLength": 2, "startIndex": 123, "lines": ["# now we do it for real:\n", "ipmag.eqarea_magic(dir_path=dir_path,save_plots=False)"]}, {"block": 17, "type": "markdown", "linesLength": 2, "startIndex": 125, "lines": ["### Map of VGPs\n", "- use ipmag.vgpmap_magic() to plot the VGPs from the same data"]}, {"block": 18, "type": "code", "linesLength": 2, "startIndex": 127, "lines": ["# get help message for vgpmap_magic\n", "help(ipmag.vgpmap_magic)"]}, {"block": 19, "type": "code", "linesLength": 1, "startIndex": 129, "lines": ["ipmag.vgpmap_magic(dir_path=dir_path,size=50,flip=True,save_plots=False,lat_0=60,rsym='b^',rsize=50)"]}, {"block": 20, "type": "markdown", "linesLength": 2, "startIndex": 130, "lines": ["### Bootstrap reversals test\n", "- use ipmag.reversal_test_bootstrap() to do the reversals test"]}, {"block": 21, "type": "code", "linesLength": 1, "startIndex": 132, "lines": ["help(ipmag.reversal_test_bootstrap)"]}, {"block": 22, "type": "code", "linesLength": 7, "startIndex": 133, "lines": ["# read in the data into a Pandas DataFrame\n", "sites_df=pd.read_csv(dir_path+'/sites.txt',sep='\\t',header=1)\n", "# pick out the declinations and inclinations\n", "decs=sites_df.dir_dec.values\n", "incs=sites_df.dir_inc.values\n", "# call the function\n", "ipmag.reversal_test_bootstrap(dec=decs,inc=incs,plot_stereo=False)"]}, {"block": 23, "type": "markdown", "linesLength": 2, "startIndex": 140, "lines": ["### Make a site map\n", "- use pmagplotlib.plot_map() to make a site map"]}, {"block": 24, "type": "code", "linesLength": 1, "startIndex": 142, "lines": ["help(pmagplotlib.plot_map)"]}, {"block": 25, "type": "markdown", "linesLength": 1, "startIndex": 143, "lines": ["NB: the most recent PmagPy version fixes the scale issue - but it is SLOW at high resolution... so set Opts\\['res'\\] to 'c' for crude for a quick look.  if you want to be dazzled - set it to 'h' but be prepared to wait for a while...  'i' for intermediate is probably good enough for most purposes (50m resolution)"]}, {"block": 26, "type": "code", "linesLength": 27, "startIndex": 144, "lines": ["# read in the data file:\n", "site_df=pd.read_csv(dir_path+'/sites.txt',sep='\\t',header=1)\n", "# pick out the longitudes and latitudes\n", "lons=site_df['lon'].values\n", "lats=site_df['lat'].values\n", "# set some options\n", "Opts={}\n", "Opts['sym']='r*' # sets the symbol to white dots\n", "Opts['symsize']=100 # sets symbol size to 3 pts\n", "Opts['proj']='lcc' # Lambert Conformal projection\n", "Opts['pltgrid']=True\n", "Opts['lat_0']=33\n", "Opts['lon_0']=35\n", "Opts['latmin']=29\n", "Opts['latmax']=35\n", "Opts['lonmin']=32\n", "Opts['lonmax']=37\n", "Opts['gridspace']=1\n", "Opts['details']={}\n", "Opts['details']['coasts']=True\n", "Opts['details']['ocean']=True\n", "Opts['details']['countries']=True\n", "Opts['global']=False\n", "Opts['res']='i'\n", "plt.figure(1,(10,10)) # optional - make a map\n", "\n", "pmagplotlib.plot_map(1, lats, lons, Opts)\n"]}, {"block": 27, "type": "markdown", "linesLength": 18, "startIndex": 171, "lines": ["## Importing MagIC format data from desktop\n", "\n", "- make a directory called MagIC\\_upload as in the first example. \n", "- download the data from Tauxe et al. (2015; DOI: 10.1016/J.EPSL.2014.12.034; MagIC id:16749) to your desktop by clicking on the 'Download' button next to the contribution. \n", "\n", "- upload the file (magic\\_contribution\\_16749.txt) to the jupyterhub site by:\n", "    - click on Open in the File menu.\n", "    - click on Upload and choose the datafile\n", "    - move the datafile to the MagIC\\_upload directory as in the first example. \n", "\n", "- Unpack it with ipmag.download_magic()\n", "- make a figure with these elements for the interval 40 m to 160 m:\n", "    - magstrat time scale plot from 2 to 7 Ma\n", "    - inclinations (dir\\_inc) from the 20mT step in the measurements table  against composite_depth as blue dots\n", "    - inclinations (dir\\_inc) from the specimens table against composite depth as red triangles. \n", "    - put on dotted lines for the GAD inclination\n", "- use ipmag.ani_depthplot to plot the anisotropy data against depth in the Hole.  \n", "- extract and plot the external\\_results data from the sites.txt file. "]}, {"block": 28, "type": "code", "linesLength": 13, "startIndex": 189, "lines": ["# set up the directory structure for this example, if not already present:\n", "dirs=os.listdir('MagIC_online') # get a list of directories in this one\n", "if 'MagIC_upload' not in dirs:\n", "    os.mkdir(\"MagIC_online/MagIC_upload\")\n", "    print ('MagIC_upload directory created')\n", "else:\n", "    print ('MagIC_upload directory already exists')\n", "magic_contribution='magic_contribution_16761.txt' # set the file name string  \n", "# move uploaded file to working directory\n", "files=os.listdir()\n", "print (files)\n", "if magic_contribution in files:\n", "    os.rename(magic_contribution, dir_path+'/'+magic_contribution) # move the contribution to the directory"]}, {"block": 29, "type": "markdown", "linesLength": 2, "startIndex": 202, "lines": ["### Download and unpack the data\n", "\n"]}, {"block": 30, "type": "code", "linesLength": 4, "startIndex": 204, "lines": ["dir_path='MagIC_online/MagIC_upload' # set the path to your working directory\n", "depth_min, depth_max= 40, 160 # set the core depth bounds as required\n", "# First get the file from MagIC into your working directory:\n", "ipmag.download_magic(magic_contribution,dir_path=dir_path,print_progress=False) # unpack the file"]}, {"block": 31, "type": "markdown", "linesLength": 7, "startIndex": 208, "lines": ["## Magstrat figure\n", "- read in the data file as a Pandas DataFrame with pd.read_csv().  \n", "    - All MagIC .txt files are tab delimited.  This is indicated with a sep='\\t' keywork.  \n", "    - The column headers in the second row, hence (because Python counts from zero), header=1\n", "- the depth of a particular specimen/site in MagIC is stored in the sites.txt table.  You will have to merge the data from that table into the specimens/measurements tables.  To do that you need to do a few things:\n", "    - you need a common key.  Because the specimen/sample/site names are the same for an IODP record, make a column in the specimen/measurements dataframes labled 'site' that is the same as the specimen.  \n", "    - merge the two dataframes (sites and specimens/measurements) with pd.merge()"]}, {"block": 32, "type": "code", "linesLength": 28, "startIndex": 215, "lines": ["depth_min, depth_max= 40, 160 # set the core depth bounds as required\n", "# read in the required data tables:\n", "meas_df=pd.read_csv(dir_path+'/measurements.txt',sep='\\t',header=1)\n", "site_df=pd.read_csv(dir_path+'/sites.txt',sep='\\t',header=1)\n", "spec_df=pd.read_csv(dir_path+'/specimens.txt',sep='\\t',header=1)\n", "ages_df=pd.read_csv(dir_path+'/ages.txt',sep='\\t',header=1)\n", "# filter the ages table for method codes that indicate paleomagnetic reversals:\n", "ages_df=ages_df[ages_df['method_codes'].str.contains('PMAG')]\n", "# filter the measurements for the 20 mT (.02 T) step\n", "meas_df.dropna(subset=['treat_ac_field'],inplace=True)\n", "meas_20mT=meas_df[meas_df['treat_ac_field']==0.02] \n", "# make the site key in the measurements and specimens dataframes\n", "meas_20mT['site']=meas_20mT['specimen']\n", "spec_df['site']=spec_df['specimen']\n", "# we only want the core depth out of the sites dataframe, so we can pare it down like this:\n", "depth_df=site_df[['site','core_depth']]\n", "# merge the specimen, depth dataframes\n", "spec_df=pd.merge(spec_df,depth_df,on='site')\n", "# merge the measurements, depth dataframes\n", "meas_20mT=pd.merge(meas_20mT,depth_df,on='site')\n", "# filter for the desired depth range: \n", "spec_df=spec_df[(spec_df['core_depth']>depth_min)&(spec_df['core_depth']<depth_max)]\n", "meas_20mT=meas_20mT[(meas_20mT['core_depth']>depth_min)&(meas_20mT['core_depth']<depth_max)]\n", "# note that the age table has only height (not depth), so these numbers are the opposite\n", "ages_df=ages_df[(ages_df['tiepoint_height']<-depth_min)&(ages_df['tiepoint_height']>-depth_max)]\n", "# get the site latitude (there is only one)\n", "lat=site_df['lat'].unique()[0]\n", "\n"]}, {"block": 33, "type": "code", "linesLength": 16, "startIndex": 243, "lines": ["fig=plt.figure(1,(9,12)) # make the figure\n", "ax1=fig.add_subplot(131) # make the first of three subplots\n", "pmagplotlib.plot_ts(ax1,2,7,timescale='gts12') # plot on the time scale\n", "ax2=fig.add_subplot(132) # make the second of three subplots\n", "plt.plot(meas_20mT.dir_inc,meas_20mT.core_depth,'bo',markeredgecolor='black',alpha=.5)\n", "plt.plot(spec_df.dir_inc,spec_df.core_depth,'r^',markeredgecolor='black')\n", "\n", "plt.ylim(depth_max,depth_min)\n", "# calculate the geocentric axial dipole field for the site latitude\n", "gad=pmag.pinc(lat) # tan (I) = 2 tan (lat)\n", "# put it on the plot as a green dashed line\n", "plt.axvline(gad,color='green',linestyle='dashed',linewidth=2)\n", "plt.axvline(-gad,color='green',linestyle='dashed',linewidth=2)\n", "plt.title('Inclinations')\n", "pmagplotlib.label_tiepoints(ax2,100,ages_df.tiepoint.values,-1*ages_df.tiepoint_height.values,lines=True)\n", "#"]}, {"block": 34, "type": "markdown", "linesLength": 2, "startIndex": 259, "lines": ["### \"Christmas tree\" of anisotropy\n", "- use ipmag.ani\\_depthplot()"]}, {"block": 35, "type": "code", "linesLength": 1, "startIndex": 261, "lines": ["help(ipmag.ani_depthplot)"]}, {"block": 36, "type": "code", "linesLength": 1, "startIndex": 262, "lines": ["ipmag.ani_depthplot(dir_path=dir_path,dmin=40,dmax=160);"]}, {"block": 37, "type": "markdown", "linesLength": 3, "startIndex": 263, "lines": ["### Plotting External Results\n", "- ipmag.ani_depthplot() reads in a specimen file with the column aniso_s filled in and calculates the eigenvalues for you. in this exercise, you learn to calculate anisotropy eigenvalues from the aniso_s column in the specimens table yourself,\n", "-  plot anisotropy eigenvalues and the natural gamma radiation values from U1361A between 40 and 160 meters below sea floor"]}, {"block": 38, "type": "code", "linesLength": 47, "startIndex": 266, "lines": ["depth_min, depth_max= 40, 160 # set the core depth bounds as required\n", "# read in the data files and filter for desired columns\n", "site_df=pd.read_csv(dir_path+'/sites.txt',sep='\\t',header=1)\n", "site_df=site_df[['site','core_depth','external_results']]\n", "\n", "anis_df=pd.read_csv(dir_path+'/specimens.txt',sep='\\t',header=1)\n", "anis_df['site']=anis_df['specimen']\n", "anis_df.dropna(subset=['aniso_v1'],inplace=True)\n", "# merge with sites and filter for the depth\n", "anis_df=pd.merge(anis_df,site_df,on='site')\n", "anis_df=anis_df[(anis_df.core_depth>depth_min)&(anis_df.core_depth<depth_max)]\n", "\n", "\n", "# unpack the eigenparameters from aniso_v1,aniso_v2 and aniso_v3 and pick out the eigenvalues\n", "anis_df['tau1']=anis_df['aniso_v1'].str.split(':',expand=True)[0].astype('float').values\n", "anis_df['tau2']=anis_df['aniso_v2'].str.split(':',expand=True)[0].astype('float').values\n", "anis_df['tau3']=anis_df['aniso_v3'].str.split(':',expand=True)[0].astype('float').values\n", "\n", "# unpack external results data\n", "site_df['ngr']=site_df['external_results'].str.split(':',expand=True)[1].astype('float').values\n", "\n", "# make the plots\n", "\n", "\n", "fig=plt.figure(1,(6,12)) # make the figure\n", "ax1=fig.add_subplot(121) # make the first of two subplots\n", "ax2=fig.add_subplot(122) # make the second of two subplots\n", "\n", "# plot the eigenvalues with the usual symbols\n", "ax1.plot(anis_df['tau1'],anis_df['core_depth'],'rs') # red square\n", "ax1.plot(anis_df['tau2'],anis_df['core_depth'],'b^') # blue triangle\n", "ax1.plot(anis_df['tau3'],anis_df['core_depth'],'ko') # black circle\n", "ax1.set_ylim(depth_max,depth_min) # set the y axis limits\n", "ax1.set_xlabel('Eigenvalues')\n", "\n", "# plot the ngr data as a black line\n", "ax2.plot(site_df['ngr'],site_df['core_depth'],'k-')\n", "ax2.set_ylim(depth_max,depth_min) # set the y axis limits\n", "# shade in the high NGR regions - these are the clay dominated layers with higher anisotropy\n", "y2=np.ones(len(site_df['ngr']))*site_df['ngr'].median()\n", "plt.fill_betweenx(site_df['core_depth'],site_df['ngr'], y2,\\\n", "                  where = site_df['ngr']>=y2, facecolor='grey')\n", "ax2.axvline(site_df['ngr'].median()) # draw a vertical line up the median values\n", "ax2.set_xlabel('NGR'); # label the X axis\n", "\n", "\n", "\n"]}, {"block": 39, "type": "code", "linesLength": 0, "startIndex": 313, "lines": []}]