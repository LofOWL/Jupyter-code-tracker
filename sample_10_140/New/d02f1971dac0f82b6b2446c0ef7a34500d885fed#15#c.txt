[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["## Memory management utils"]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 1, "lines": ["Utility functions for memory management. Currently primarily for GPU."]}, {"block": 2, "type": "code", "linesLength": 2, "startIndex": 2, "lines": ["from fastai.gen_doc.nbdoc import *\n", "from fastai.utils.mem import * "]}, {"block": 3, "type": "code", "linesLength": 1, "startIndex": 4, "lines": ["show_doc(gpu_mem_get)"]}, {"block": 4, "type": "markdown", "linesLength": 5, "startIndex": 5, "lines": ["[`gpu_mem_get`](/utils.mem.html#gpu_mem_get)\n", "\n", "* for gpu returns `GPUMemory(total, free, used)`\n", "* for cpu returns `GPUMemory(0, 0, 0)`\n", "* for invalid gpu id returns `GPUMemory(0, 0, 0)`"]}, {"block": 5, "type": "code", "linesLength": 1, "startIndex": 10, "lines": ["show_doc(gpu_mem_get_all)"]}, {"block": 6, "type": "markdown", "linesLength": 3, "startIndex": 11, "lines": ["[`gpu_mem_get_all`](/utils.mem.html#gpu_mem_get_all)\n", "* for gpu returns `[ GPUMemory(total_0, free_0, used_0), GPUMemory(total_1, free_1, used_1), .... ]`\n", "* for cpu returns `[]`\n"]}, {"block": 7, "type": "code", "linesLength": 1, "startIndex": 14, "lines": ["show_doc(gpu_mem_get_free)"]}, {"block": 8, "type": "markdown", "linesLength": 0, "startIndex": 15, "lines": []}, {"block": 9, "type": "code", "linesLength": 1, "startIndex": 15, "lines": ["show_doc(gpu_mem_get_free_no_cache)"]}, {"block": 10, "type": "markdown", "linesLength": 0, "startIndex": 16, "lines": []}, {"block": 11, "type": "code", "linesLength": 1, "startIndex": 16, "lines": ["show_doc(gpu_mem_get_used)"]}, {"block": 12, "type": "markdown", "linesLength": 0, "startIndex": 17, "lines": []}, {"block": 13, "type": "code", "linesLength": 1, "startIndex": 17, "lines": ["show_doc(gpu_mem_get_used_no_cache)"]}, {"block": 14, "type": "markdown", "linesLength": 1, "startIndex": 18, "lines": ["[`gpu_mem_get_used_no_cache`](/utils.mem.html#gpu_mem_get_used_no_cache)"]}, {"block": 15, "type": "code", "linesLength": 1, "startIndex": 19, "lines": ["show_doc(gpu_mem_get_used_fast)"]}, {"block": 16, "type": "markdown", "linesLength": 1, "startIndex": 20, "lines": ["[`gpu_mem_get_used_fast`](/utils.mem.html#gpu_mem_get_used_fast)"]}, {"block": 17, "type": "code", "linesLength": 1, "startIndex": 21, "lines": ["show_doc(gpu_with_max_free_mem)"]}, {"block": 18, "type": "markdown", "linesLength": 3, "startIndex": 22, "lines": ["[`gpu_with_max_free_mem`](/utils.mem.html#gpu_with_max_free_mem):\n", "* for gpu returns: `gpu_with_max_free_ram_id, its_free_ram`\n", "* for cpu returns: `None, 0`\n"]}, {"block": 19, "type": "code", "linesLength": 1, "startIndex": 25, "lines": ["show_doc(preload_pytorch)"]}, {"block": 20, "type": "markdown", "linesLength": 1, "startIndex": 26, "lines": ["[`preload_pytorch`](/utils.mem.html#preload_pytorch) is helpful when GPU memory is being measured, since the first time any operation on `cuda` is performed by pytorch, usually about 0.5GB gets used by CUDA context."]}, {"block": 21, "type": "code", "linesLength": 1, "startIndex": 27, "lines": ["show_doc(GPUMemory, title_level=4)"]}, {"block": 22, "type": "markdown", "linesLength": 1, "startIndex": 28, "lines": ["[`GPUMemory`](/utils.mem.html#GPUMemory) is a namedtuple that is returned by functions like [`gpu_mem_get`](/utils.mem.html#gpu_mem_get) and [`gpu_mem_get_all`](/utils.mem.html#gpu_mem_get_all)."]}, {"block": 23, "type": "code", "linesLength": 1, "startIndex": 29, "lines": ["show_doc(b2mb)"]}, {"block": 24, "type": "markdown", "linesLength": 1, "startIndex": 30, "lines": ["[`b2mb`](/utils.mem.html#b2mb) is a helper utility that just does `int(bytes/2**20)`"]}, {"block": 25, "type": "markdown", "linesLength": 1, "startIndex": 31, "lines": ["## Memory Tracing Utils"]}, {"block": 26, "type": "code", "linesLength": 1, "startIndex": 32, "lines": ["show_doc(GPUMemTrace, title_level=4)"]}, {"block": 27, "type": "markdown", "linesLength": 166, "startIndex": 33, "lines": ["**Arguments**:\n", "\n", "* `silent`: a shortcut to make `report` and `report_n_reset` silent w/o needing to remove those calls - this can be done from the constructor, or alternatively you can call `silent` method anywhere to do the same.\n", "* `ctx`: default context note in reports\n", "* `on_exit_report`:  auto-report on ctx manager exit (default `True`)\n", "\n", "**Definitions**:\n", "\n", "* **Delta Used** is the difference between current used memory and used memory at the start of the counter.\n", "\n", "* **Delta Peaked** is the memory overhead if any. It's calculated in two steps:\n", "   1. The base measurement is the difference between the peak memory and the used memory at the start of the counter.\n", "   2. Then if delta used is positive it gets subtracted from the base value.\n", "   \n", "   It indicates the size of the blip.\n", "\n", "   **Warning**: currently the peak memory usage tracking is implemented using a python thread, which is very unreliable, since there is no guarantee the thread will get a chance at running at the moment the peak memory is occuring (or it might not get a chance to run at all). Therefore we need pytorch to implement multiple concurrent and resettable [`torch.cuda.max_memory_allocated`](https://pytorch.org/docs/stable/cuda.html#torch.cuda.max_memory_allocated) counters. Please vote for this [feature request](https://github.com/pytorch/pytorch/issues/16266).\n", "\n", "**Usage Examples**:\n", "\n", "Setup:\n", "```\n", "from fastai.utils.mem import GPUMemTrace\n", "def some_code(): pass\n", "mtrace = GPUMemTrace()\n", "```\n", "\n", "Example 1: basic measurements via `report` (prints) and via [`data`](/tabular.data.html#tabular.data) (returns) accessors\n", "```\n", "some_code()\n", "mtrace.report()\n", "delta_used, delta_peaked = mtrace.data()\n", "\n", "some_code()\n", "mtrace.report('2nd run of some_code()')\n", "delta_used, delta_peaked = mtrace.data()\n", "```\n", "`report`'s optional `subctx` argument can be helpful if you have many `report` calls and you want to understand which is which in the outputs.\n", "\n", "Example 2: measure in a loop, resetting the counter before each run\n", "```\n", "for i in range(10):\n", "    mtrace.reset()\n", "    some_code()\n", "    mtrace.report(f'i={i}')\n", "```\n", "`reset` resets all the counters.\n", "\n", "Example 3: like example 2, but having `report` automatically reset the counters\n", "```\n", "mtrace.reset()\n", "for i in range(10):\n", "    some_code()\n", "    mtrace.report_n_reset(f'i={i}')\n", "```\n", "\n", "The tracing starts immediately upon the [`GPUMemTrace`](/utils.mem.html#GPUMemTrace) object creation, and stops when that object is deleted. But it can also be `stop`ed, `start`ed manually as well.\n", "```\n", "mtrace.start()\n", "mtrace.stop()\n", "```\n", "`stop` is in particular useful if you want to **freeze** the [`GPUMemTrace`](/utils.mem.html#GPUMemTrace) object and to be able to query its data on `stop` some time down the road.\n", "\n", "\n", "**Reporting**:\n", "\n", "In reports you can print a main context passed via the constructor:\n", "\n", "```\n", "mtrace = GPUMemTrace(ctx=\"foobar\")\n", "mtrace.report()\n", "```\n", "prints:\n", "```\n", "\u25b3Used Peaked MB:      0      0  (foobar)\n", "```\n", "\n", "and then add subcontext notes as needed:\n", "\n", "```\n", "mtrace = GPUMemTrace(ctx=\"foobar\")\n", "mtrace.report('1st try')\n", "mtrace.report('2nd try')\n", "\n", "```\n", "prints:\n", "```\n", "\u25b3Used Peaked MB:      0      0  (foobar: 1st try)\n", "\u25b3Used Peaked MB:      0      0  (foobar: 2nd try)\n", "```\n", "\n", "Both context and sub-context are optional, and are very useful if you sprinkle [`GPUMemTrace`](/utils.mem.html#GPUMemTrace) in different places around the code.\n", "\n", "You can silence report calls w/o needing to remove them via constructor or `silent`:\n", "\n", "```\n", "mtrace = GPUMemTrace(silent=True)\n", "mtrace.report() # nothing will be printed\n", "mtrace.silent(silent=False)\n", "mtrace.report() # printing resumed\n", "mtrace.silent(silent=True)\n", "mtrace.report() # nothing will be printed\n", "```\n", "\n", "**Context Manager**:\n", "\n", "[`GPUMemTrace`](/utils.mem.html#GPUMemTrace) can also be used as a context manager:\n", "\n", "Report the used and peaked deltas automatically:\n", "\n", "```\n", "with GPUMemTrace(): some_code()\n", "```\n", "\n", "If you wish to add context:\n", "\n", "```\n", "with GPUMemTrace(ctx='some context'): some_code()\n", "```\n", "\n", "The context manager uses subcontext `exit` to indicate that the report comes after the context exited.\n", "\n", "The reporting is done automatically, which is especially useful in functions due to return call:\n", "\n", "```\n", "def some_func():\n", "    with GPUMemTrace(ctx='some_func'):\n", "        # some code\n", "        return 1\n", "some_func()\n", "```\n", "prints:\n", "```\n", "\u25b3Used Peaked MB:      0      0 (some_func: exit)\n", "```\n", "so you still get a perfect report despite the `return` call here. `ctx` is useful for specifying the *context* in case you have many of those calls through your code and you want to know which is which.\n", "\n", "And, of course, instead of doing the above, you can use [`gpu_mem_trace`](/utils.mem.html#gpu_mem_trace) decorator to do it automatically, including using the function or method name as the context. Therefore, the example below does the same without modifying the function.\n", "\n", "```\n", "@gpu_mem_trace\n", "def some_func():\n", "    # some code\n", "    return 1\n", "some_func()\n", "```\n", "\n", "If you don't wish the automatic reporting, just pass `on_exit_report=False` in the constructor:\n", "\n", "```\n", "with GPUMemTrace(ctx='some_func', on_exit_report=False) as mtrace:\n", "    some_code()\n", "mtrace.report(\"measured in ctx\")\n", "```\n", "\n", "or the same w/o the context note:\n", "```\n", "with GPUMemTrace(on_exit_report=False) as mtrace: some_code()\n", "print(mtrace) # or mtrace.report()\n", "```\n", "\n", "And, of course, you can get the numerical data (in rounded MBs):\n", "```\n", "with GPUMemTrace() as mtrace: some_code()\n", "delta_used, delta_peaked = mtrace.data()\n", "```"]}, {"block": 28, "type": "code", "linesLength": 1, "startIndex": 199, "lines": ["show_doc(gpu_mem_trace)"]}, {"block": 29, "type": "markdown", "linesLength": 16, "startIndex": 200, "lines": ["This allows you to decorate any function or method with:\n", "\n", "```\n", "@gpu_mem_trace\n", "def my_function(): pass\n", "# run:\n", "my_function()\n", "```\n", "and it will automatically print the report including the function name as a context:\n", "```\n", "\u25b3Used Peaked MB:      0      0 (my_function: exit)\n", "```\n", "In the case of methods it'll print a fully qualified method, e.g.:\n", "```\n", "\u25b3Used Peaked MB:      0      0 (Class.function: exit)\n", "```\n"]}, {"block": 30, "type": "markdown", "linesLength": 1, "startIndex": 216, "lines": ["## Undocumented Methods - Methods moved below this line will intentionally be hidden"]}, {"block": 31, "type": "code", "linesLength": 1, "startIndex": 217, "lines": ["show_doc(GPUMemTrace.report)"]}, {"block": 32, "type": "markdown", "linesLength": 0, "startIndex": 218, "lines": []}, {"block": 33, "type": "code", "linesLength": 1, "startIndex": 218, "lines": ["show_doc(GPUMemTrace.silent)"]}, {"block": 34, "type": "markdown", "linesLength": 0, "startIndex": 219, "lines": []}, {"block": 35, "type": "code", "linesLength": 1, "startIndex": 219, "lines": ["show_doc(GPUMemTrace.start)"]}, {"block": 36, "type": "markdown", "linesLength": 0, "startIndex": 220, "lines": []}, {"block": 37, "type": "code", "linesLength": 1, "startIndex": 220, "lines": ["show_doc(GPUMemTrace.reset)"]}, {"block": 38, "type": "markdown", "linesLength": 0, "startIndex": 221, "lines": []}, {"block": 39, "type": "code", "linesLength": 1, "startIndex": 221, "lines": ["show_doc(GPUMemTrace.peak_monitor_stop)"]}, {"block": 40, "type": "markdown", "linesLength": 0, "startIndex": 222, "lines": []}, {"block": 41, "type": "code", "linesLength": 1, "startIndex": 222, "lines": ["show_doc(GPUMemTrace.stop)"]}, {"block": 42, "type": "markdown", "linesLength": 0, "startIndex": 223, "lines": []}, {"block": 43, "type": "code", "linesLength": 1, "startIndex": 223, "lines": ["show_doc(GPUMemTrace.report_n_reset)"]}, {"block": 44, "type": "markdown", "linesLength": 0, "startIndex": 224, "lines": []}, {"block": 45, "type": "code", "linesLength": 1, "startIndex": 224, "lines": ["show_doc(GPUMemTrace.peak_monitor_func)"]}, {"block": 46, "type": "markdown", "linesLength": 0, "startIndex": 225, "lines": []}, {"block": 47, "type": "code", "linesLength": 1, "startIndex": 225, "lines": ["show_doc(GPUMemTrace.data_set)"]}, {"block": 48, "type": "markdown", "linesLength": 0, "startIndex": 226, "lines": []}, {"block": 49, "type": "code", "linesLength": 1, "startIndex": 226, "lines": ["show_doc(GPUMemTrace.data)"]}, {"block": 50, "type": "markdown", "linesLength": 0, "startIndex": 227, "lines": []}, {"block": 51, "type": "code", "linesLength": 1, "startIndex": 227, "lines": ["show_doc(GPUMemTrace.peak_monitor_start)"]}, {"block": 52, "type": "markdown", "linesLength": 0, "startIndex": 228, "lines": []}, {"block": 53, "type": "markdown", "linesLength": 1, "startIndex": 228, "lines": ["## New Methods - Please document or move to the undocumented section"]}]