[{"block": 0, "type": "markdown", "linesLength": 3, "startIndex": 0, "lines": ["<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n", "\n", "<i>Licensed under the MIT License.</i>"]}, {"block": 1, "type": "markdown", "linesLength": 7, "startIndex": 3, "lines": ["# Wide and Deep Model for Movie Recommendation\n", "<br>\n", "\n", "\n", "This notebook shows how to build and test [**wide-and-deep model**](https://arxiv.org/abs/1606.07792)--linear combination of the linear and DNN models--using [TensorFlow high-level Estimator API](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedRegressor).\n", "\n", "For more details about hyperparameter tuning via Azure Machine Learning service, see [AML_Hyperparameter_Tuning notebook](../04_model_select_and_optimize/aml_hyperparameter_tuning.ipynb)."]}, {"block": 2, "type": "markdown", "linesLength": 2, "startIndex": 10, "lines": ["### Prerequisite\n", "* tensorflow (version 1.8 or higher) - GPU version is preferable"]}, {"block": 3, "type": "code", "linesLength": 19, "startIndex": 12, "lines": ["import sys\n", "sys.path.append(\"../../\")\n", "\n", "import os\n", "import shutil\n", "\n", "import tensorflow as tf\n", "import pandas as pd\n", "import numpy as np\n", "import sklearn.preprocessing\n", "\n", "from reco_utils.common import tf_utils\n", "from reco_utils.dataset import movielens\n", "from reco_utils.dataset.pandas_df_utils import user_item_pairs\n", "from reco_utils.dataset.python_splitters import python_random_split\n", "from reco_utils.evaluation.python_evaluation import (\n", "    rmse, mae, rsquared, exp_var,\n", "    map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n", ")"]}, {"block": 4, "type": "code", "linesLength": 6, "startIndex": 31, "lines": ["from tensorflow.python.client import device_lib\n", "\n", "print(\"Tensorflow Version:\", tf.__version__)\n", "\n", "devices = device_lib.list_local_devices()\n", "[x.name for x in devices]"]}, {"block": 5, "type": "markdown", "linesLength": 5, "startIndex": 37, "lines": ["### Data loading\n", "\n", "Download [MovieLens](https://grouplens.org/datasets/movielens/) data and split train / test set.\n", "We use genres as item features.\n", "We don't use timestamp since we don't want our model to fit to time information instead of movie."]}, {"block": 6, "type": "code", "linesLength": 10, "startIndex": 42, "lines": ["# top k items to recommend\n", "TOP_K = 10\n", "\n", "# Select Movielens data size: 100k, 1m, 10m, or 20m\n", "MOVIELENS_DATA_SIZE = '1m'\n", "\n", "USER_COL = 'UserId'\n", "ITEM_COL = 'MovieId'\n", "RATING_COL = 'Rating'\n", "ITEM_FEAT_COL = 'Genres'"]}, {"block": 7, "type": "code", "linesLength": 6, "startIndex": 52, "lines": ["data = movielens.load_pandas_df(\n", "    size=MOVIELENS_DATA_SIZE,\n", "    header=[USER_COL, ITEM_COL, RATING_COL],\n", "    genres_col='_Genres'\n", ")\n", "data.head()"]}, {"block": 8, "type": "code", "linesLength": 6, "startIndex": 58, "lines": ["# Encode 'genres' into int array (multi-hot representation) to use as item features\n", "genres_encoder = sklearn.preprocessing.MultiLabelBinarizer()\n", "data[ITEM_FEAT_COL] = genres_encoder.fit_transform(\n", "    data['_Genres'].apply(lambda s: s.split(\"|\"))\n", ").tolist()\n", "print(\"Genres:\", genres_encoder.classes_)"]}, {"block": 9, "type": "code", "linesLength": 5, "startIndex": 64, "lines": ["items = data.drop_duplicates(ITEM_COL)[[ITEM_COL, '_Genres', ITEM_FEAT_COL]].reset_index(drop=True)\n", "print(items.head())\n", "\n", "# We don't use _Genres column\n", "items.drop('_Genres', axis=1, inplace=True)"]}, {"block": 10, "type": "code", "linesLength": 2, "startIndex": 69, "lines": ["users = data.drop_duplicates(USER_COL)[[USER_COL]].reset_index(drop=True)\n", "users.head()"]}, {"block": 11, "type": "code", "linesLength": 5, "startIndex": 71, "lines": ["train, test = python_random_split(\n", "    data.drop('_Genres', axis=1),\n", "    ratio=0.75,\n", "    seed=123\n", ")"]}, {"block": 12, "type": "markdown", "linesLength": 3, "startIndex": 76, "lines": ["### Modeling\n", "\n", "'--model-type', 'wide_deep', '--epochs', '50', '--batch-size', '128', '--dnn-batch-norm', 'True', '--dnn-hidden-units', '256,64,256', '--dnn-item-embedding-dim', '4', '--dnn-optimizer', 'Adam', '--dnn-optimizer-lr', '0.02039596884309', '--dnn-user-embedding-dim', '128', '--dropout', '0.289914344143064', '--l1-reg', '0.0101626002122412', '--linear-optimizer', 'Ftrl', '--linear-optimizer-lr', '0.0369276616034676'"]}, {"block": 13, "type": "code", "linesLength": 22, "startIndex": 79, "lines": ["\"\"\" Hyper parameters\n", "\"\"\"\n", "BATCH_SIZE = 128\n", "NUM_EPOCHS = 50\n", "\n", "LINEAR_OPTIMIZER = tf.train.FtrlOptimizer(\n", "    learning_rate=0.03,\n", "    l1_regularization_strength=0.01\n", ")\n", "DNN_OPTIMIZER = tf.train.AdamOptimizer(\n", "    learning_rate=0.02\n", ")\n", "DNN_HIDDEN_UNITS = [256,64,256]\n", "DNN_DROPOUT = 0.2\n", "DNN_BATCH_NORM = True\n", "\n", "# Rule of thumb for embedding_dimensions =  number_of_categories ** 0.25\n", "DNN_USER_DIM = int(len(users) ** 0.25)\n", "DNN_ITEM_DIM = int(len(items) ** 0.25)\n", "\n", "print(\"Embedding {} users to {}-dim vector\".format(len(users), DNN_USER_DIM))\n", "print(\"Embedding {} items to {}-dim vector\".format(len(items), DNN_ITEM_DIM))"]}, {"block": 14, "type": "markdown", "linesLength": 9, "startIndex": 101, "lines": ["### Feature embedding\n", "\n", "Wide and deep model utilizes two different types of feature set: 1) a wide set of cross-producted features to capture how the co-occurrence of a query-item feature pair correlates with the target label or rating, and 2) a deep, lower-dimensional embedding vectors for every query and item.\n", "\n", "Genres as item feature\n", "\n", "Not using Timestamp for two reason:\n", "1. doesn't make sense to predict ratings\n", "2. For top-k recommendation scenario, we don't have TS for the items user haven't watched. Model will bias to learn those items.\n"]}, {"block": 15, "type": "code", "linesLength": 30, "startIndex": 110, "lines": ["user_id = tf.feature_column.categorical_column_with_vocabulary_list(USER_COL, users[USER_COL].values)\n", "item_id = tf.feature_column.categorical_column_with_vocabulary_list(ITEM_COL, items[ITEM_COL].values)\n", "\n", "wide_columns = [\n", "    tf.feature_column.crossed_column([user_id, item_id], hash_bucket_size=1000)\n", "]\n", "\n", "deep_columns = [\n", "    # User embedding\n", "    tf.feature_column.embedding_column(\n", "        categorical_column=user_id,\n", "        dimension=DNN_USER_DIM,\n", "        max_norm=DNN_USER_DIM ** .5\n", "    ),\n", "    # Item embedding\n", "    tf.feature_column.embedding_column(\n", "        categorical_column=item_id,\n", "        dimension=DNN_ITEM_DIM,\n", "        max_norm=DNN_ITEM_DIM ** .5\n", "    ),\n", "    # Item feature\n", "    tf.feature_column.numeric_column(\n", "        ITEM_FEAT_COL,\n", "        shape=len(genres_encoder.classes_),\n", "        dtype=tf.float32\n", "    )\n", "]\n", "\n", "for c in wide_columns + deep_columns:\n", "    print(str(c)[:100], \"...\")"]}, {"block": 16, "type": "markdown", "linesLength": 1, "startIndex": 140, "lines": ["Model."]}, {"block": 17, "type": "code", "linesLength": 26, "startIndex": 141, "lines": ["MODEL_DIR = os.path.join('.', 'model_checkpoints')\n", "\n", "try:\n", "    # Clean-up previous dir if exists\n", "    shutil.rmtree(MODEL_DIR)\n", "except (PermissionError, FileNotFoundError):\n", "    pass\n", "\n", "# Log less-frequently\n", "run_config = tf.estimator.RunConfig()\n", "run_config = run_config.replace(log_step_count_steps=1000)\n", "\n", "# We use regressor for rating prediction\n", "model = tf.estimator.DNNLinearCombinedRegressor(\n", "    model_dir=MODEL_DIR,\n", "    config=run_config,\n", "    # wide model args\n", "    linear_feature_columns=wide_columns,\n", "    linear_optimizer=LINEAR_OPTIMIZER,\n", "    # deep model args\n", "    dnn_feature_columns=deep_columns,\n", "    dnn_hidden_units=DNN_HIDDEN_UNITS,\n", "    dnn_optimizer=DNN_OPTIMIZER,\n", "    dnn_dropout=DNN_DROPOUT,\n", "    batch_norm=DNN_BATCH_NORM\n", ")"]}, {"block": 18, "type": "markdown", "linesLength": 1, "startIndex": 167, "lines": ["Can add additional metrics."]}, {"block": 19, "type": "code", "linesLength": 8, "startIndex": 168, "lines": ["metrics_fn = (lambda labels, predictions: {\n", "    'mae': tf.metrics.mean_absolute_error(\n", "        tf.cast(labels, tf.float32),\n", "        predictions['predictions']\n", "    )\n", "})\n", "\n", "model = tf.contrib.estimator.add_metrics(model, metrics_fn)"]}, {"block": 20, "type": "markdown", "linesLength": 11, "startIndex": 176, "lines": ["### Training and Evaluation\n", "\n", "If you want to do hyperparam tuning, split set into 3: training, evaluation, testing\n", "Here, we use a known parameter, so we just use training / test set.\n", "\n", "We evaluate every 100 iter, MAP and MAE.\n", "Note loss == MSE so we can easily get RMSE by sqrt.\n", "\n", "Prepare a recommendation pool for recommend k-item (ranking) scenario \n", "1. get all user-item pairs with remove seen-items (optional. Movie recommendation scenario)\n", "3. add genres"]}, {"block": 21, "type": "markdown", "linesLength": 1, "startIndex": 187, "lines": ["Can use ndcg hook, map hook, etc..."]}, {"block": 22, "type": "code", "linesLength": 11, "startIndex": 188, "lines": ["# User-item pairs for ranking (recommend k-item) evaluation. We use this set for eval-log-hook too.\n", "ranking_pool = user_item_pairs(\n", "    user_df=users,\n", "    item_df=items,\n", "    user_col=USER_COL,\n", "    item_col=ITEM_COL,\n", "    user_item_filter_df=train,\n", "    shuffle=True\n", ")\n", "\n", "ranking_pool.head()"]}, {"block": 23, "type": "code", "linesLength": 29, "startIndex": 199, "lines": ["eval_kwargs = {\n", "    'col_user': USER_COL,\n", "    'col_item': ITEM_COL,\n", "    'col_rating': RATING_COL,\n", "    'col_prediction': 'prediction',\n", "    'k': TOP_K\n", "}\n", "\n", "precision_eval_hook = tf_utils.TrainLogHook(\n", "    model_dir=MODEL_DIR,\n", "    model=model,\n", "    true_df=test,\n", "    y_col=RATING_COL,\n", "    eval_df=ranking_pool,\n", "    every_n_iter=10000,\n", "    eval_fn=precision_at_k,\n", "    **eval_kwargs\n", ")\n", "\n", "model.train(\n", "    input_fn=tf_utils.pandas_input_fn(\n", "        df=train,\n", "        y_col=RATING_COL,\n", "        batch_size=BATCH_SIZE,\n", "        num_epochs=NUM_EPOCHS,\n", "        shuffle=True\n", "    ),\n", "    hooks=[precision_eval_hook]\n", ")"]}, {"block": 24, "type": "markdown", "linesLength": 3, "startIndex": 228, "lines": ["### Testing\n", "\n", "We predict the ratings by using the wide-deep model we trained. Finally, we also generate top-k movie recommentation for each user and test the performance."]}, {"block": 25, "type": "markdown", "linesLength": 1, "startIndex": 231, "lines": ["1. Item rating prediction"]}, {"block": 26, "type": "code", "linesLength": 11, "startIndex": 232, "lines": ["cols = {\n", "    'col_user': USER_COL,\n", "    'col_item': ITEM_COL,\n", "    'col_rating': RATING_COL,\n", "    'col_prediction': 'prediction'\n", "}\n", "\n", "predictions = list(model.predict(input_fn=tf_utils.pandas_input_fn(df=test)))\n", "prediction_df = test.drop(RATING_COL, axis=1)\n", "prediction_df['prediction'] = [p['predictions'][0] for p in predictions]\n", "prediction_df['prediction'].describe()"]}, {"block": 27, "type": "code", "linesLength": 9, "startIndex": 243, "lines": ["eval_rmse = rmse(test, prediction_df, **cols)\n", "eval_mae = mae(test, prediction_df, **cols)\n", "eval_rsquared = rsquared(test, prediction_df, **cols)\n", "eval_exp_var = exp_var(test, prediction_df, **cols)\n", "\n", "print(\"RMSE:\\t\\t%f\" % eval_rmse,\n", "      \"MAE:\\t\\t%f\" % eval_mae,\n", "      \"rsquared:\\t%f\" % eval_rsquared,\n", "      \"exp var:\\t%f\" % eval_exp_var, sep='\\n')"]}, {"block": 28, "type": "code", "linesLength": 9, "startIndex": 252, "lines": ["eval_results = model.evaluate(\n", "    input_fn=tf_utils.pandas_input_fn(\n", "        df=test,\n", "        y_col=RATING_COL\n", "    ),\n", "    steps=None\n", ")\n", "\n", "print(eval_results)"]}, {"block": 29, "type": "markdown", "linesLength": 1, "startIndex": 261, "lines": ["2. Recommend k items"]}, {"block": 30, "type": "code", "linesLength": 14, "startIndex": 262, "lines": ["predictions = list(model.predict(input_fn=tf_utils.pandas_input_fn(df=ranking_pool)))\n", "prediction_df = ranking_pool.copy()\n", "prediction_df['prediction'] = [p['predictions'][0] for p in predictions]\n", "\n", "# TODO for now, fix TOP_K\n", "eval_map = map_at_k(test, prediction_df, k=TOP_K, **cols)\n", "eval_ndcg = ndcg_at_k(test, prediction_df, k=TOP_K, **cols)\n", "eval_precision = precision_at_k(test, prediction_df, k=TOP_K, **cols)\n", "eval_recall = recall_at_k(test, prediction_df, k=TOP_K, **cols)\n", "\n", "print(\"MAP:\\t%f\" % eval_map,\n", "      \"NDCG:\\t%f\" % eval_ndcg,\n", "      \"Precision@K:\\t%f\" % eval_precision,\n", "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"]}, {"block": 31, "type": "markdown", "linesLength": 6, "startIndex": 276, "lines": ["### Tensorboard\n", "\n", "To see Tensorboard, run `tensorboard --logdir=wide_deep_model` (Note, our MODEL_DIR = wide_deep_model)\n", "`localhost:6006`\n", "\n", "TODO: add screenshot"]}, {"block": 32, "type": "markdown", "linesLength": 1, "startIndex": 282, "lines": ["### Export Model"]}, {"block": 33, "type": "code", "linesLength": 34, "startIndex": 283, "lines": ["EXPORT_DIR_BASE = os.path.join('.', 'saved_model')\n", "os.makedirs(EXPORT_DIR, exist_ok=True)\n", "\n", "train_rcvr_fn = tf.contrib.estimator.build_supervised_input_receiver_fn_from_input_fn(\n", "    tf_utils.pandas_input_fn(\n", "        df=train,\n", "        y_col=RATING_COL,\n", "        batch_size=BATCH_SIZE,\n", "        num_epochs=NUM_EPOCHS,\n", "        shuffle=True\n", "    )\n", ")\n", "eval_rcvr_fn = tf.contrib.estimator.build_supervised_input_receiver_fn_from_input_fn(\n", "    tf_utils.pandas_input_fn(\n", "        df=test,\n", "        y_col=RATING_COL\n", "    )\n", ")\n", "serve_rcvr_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n", "    tf.feature_column.make_parse_example_spec(wide_columns+deep_columns)\n", ")\n", "rcvr_fn_map = {\n", "    tf.estimator.ModeKeys.TRAIN: train_rcvr_fn,\n", "    tf.estimator.ModeKeys.EVAL: eval_rcvr_fn,\n", "    tf.estimator.ModeKeys.PREDICT: serve_rcvr_fn\n", "}\n", "\n", "export_dir = tf.contrib.estimator.export_all_saved_models(\n", "    model,\n", "    export_dir_base=EXPORT_DIR_BASE,\n", "    input_receiver_fn_map=rcvr_fn_map\n", ")\n", "\n", "print(\"Model exported to\", export_dir)"]}, {"block": 34, "type": "code", "linesLength": 10, "startIndex": 317, "lines": ["saved_model = tf.contrib.estimator.SavedModelEstimator(export_dir)\n", "\n", "result = saved_model.evaluate(\n", "    tf_utils.pandas_input_fn(\n", "        df=test,\n", "        y_col=RATING_COL\n", "    ),\n", "    steps=None\n", ")\n", "print(result)"]}, {"block": 35, "type": "code", "linesLength": 2, "startIndex": 327, "lines": ["test_sample = test.iloc[0]\n", "test_sample"]}, {"block": 36, "type": "code", "linesLength": 33, "startIndex": 329, "lines": ["def predict_input_fn():\n", "    example = tf.train.Example()\n", "    \n", "    example.features.feature[USER_COL].int64_list.value.extend([test_sample[USER_COL]])\n", "    example.features.feature[ITEM_COL].int64_list.value.extend([test_sample[ITEM_COL]])\n", "    example.features.feature[ITEM_FEAT_COL].float_list.value.extend(test_sample[ITEM_FEAT_COL])\n", "    return {'inputs':tf.constant([example.SerializeToString()])}\n", "\n", "# prediction = list()\n", "print(next(saved_model.predict(predict_input_fn)))\n", "# def predict_input_fn():\n", "#     example = tf.train.Example()\n", "#     example.features.feature['UserId'].bytes_list.value.extend(['496'])\n", "#     example.features.feature['MovieId'].bytes_list.value.extend(['136'])\n", "#     return {'inputs': tf.constant([example.SerializeToString()])}\n", "\n", "\n", "\n", "\n", "# Convert input data into serialized Example strings.\n", "\n", "\n", "# features = tf.parse_example(\n", "#     serialized=serialized_examples,\n", "#     features=make_parse_example_spec(feature_columns))\n", "# predictions_dict = next(prediction)\n", "# predictions_dict\n", "# pred_input_fn = tf.estimator.inputs.pandas_input_fn(\n", "#     x=test_x,\n", "#     num_epochs=1,\n", "#     shuffle=False\n", "# )\n", "\n"]}, {"block": 37, "type": "code", "linesLength": 1, "startIndex": 362, "lines": ["# TODO Cleanup EXPORT_DIR_BASE and MODEL_DIR\n"]}, {"block": 38, "type": "code", "linesLength": 0, "startIndex": 363, "lines": []}]