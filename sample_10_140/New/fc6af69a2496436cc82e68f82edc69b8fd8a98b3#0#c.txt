[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["# All the Linear Algebra You Need for AI"]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 1, "lines": ["The purpose of this notebook is to serve as an explanation of two crucial linear algebra operations used when coding neural networks: matrix multiplication and broadcasting."]}, {"block": 2, "type": "markdown", "linesLength": 1, "startIndex": 2, "lines": ["## Introduction"]}, {"block": 3, "type": "markdown", "linesLength": 1, "startIndex": 3, "lines": ["**Matrix multiplication** is a way of combining two matrices (involving multiplying and summing their entries in a particular way).  **Broadcasting** refers to how libraries such as Numpy and PyTorch can perform operations on matrices/vectors with mismatched dimensions (in particular cases, with set rules).  We will use broadcasting to show an alternative way of thinking about matrix multiplication from, different from the way it is standardly taught."]}, {"block": 4, "type": "markdown", "linesLength": 12, "startIndex": 4, "lines": ["In keeping with the [fast.ai teaching philosophy](http://www.fast.ai/2016/10/08/teaching-philosophy/) of [\"the whole game\"](https://www.amazon.com/Making-Learning-Whole-Principles-Transform/dp/0470633719/ref=sr_1_1?ie=UTF8&qid=1505094653), we will:\n", "\n", "- first use a pre-defined class for our neural network\n", "- then define the net ourselves to see where it uses matrix multiplication & broadcasting\n", "- and finally dig into the details of how those operations work\n", "\n", "This is different from how most math courses are taught, where you have to learn all the individual elements before you can combine them (Harvard professor David Perkins call this *elementitis*), but it is similar to how topics like *driving* and *baseball* are taught.  That is, you can start driving without [knowing how an internal combustion engine works](https://medium.com/towards-data-science/thoughts-after-taking-the-deeplearning-ai-courses-8568f132153), and children begin playing baseball before they learn all the formal rules.\n", "\n", "<img src=\"images/demba_combustion_engine.png\" alt=\"\" style=\"width: 50%\"/>\n", "<center>\n", "(source: [Demba Ba](https://github.com/zalandoresearch/fashion-mnist) and [Arvind Nagaraj](https://medium.com/towards-data-science/thoughts-after-taking-the-deeplearning-ai-courses-8568f132153))\n", "</center>"]}, {"block": 5, "type": "markdown", "linesLength": 1, "startIndex": 16, "lines": ["### More linear algebra resources"]}, {"block": 6, "type": "markdown", "linesLength": 7, "startIndex": 17, "lines": ["This notebook was originally created for a 40 minute talk I gave at the [O'Reilly AI conference in San Francisco](https://conferences.oreilly.com/artificial-intelligence/ai-ca).  If you want further resources for linear algebra, here are a few recommendations:\n", "\n", "- [3Blue1Brown Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) videos about *geometric intuition* (fantastic! gorgeous!)\n", "- [Khan Academy Linear Algebra](https://www.khanacademy.org/math/linear-algebra)\n", "- [Immersive linear algebra](http://immersivemath.com/ila/) free online textbook with interactive graphics\n", "- [Chapter 2](http://www.deeplearningbook.org/contents/linear_algebra.html) of Ian Goodfellow's Deep Learning Book\n", "- [Computational Linear Algebra](http://www.fast.ai/2017/07/17/num-lin-alg/): a free, online fast.ai course, originally taught in the University of San Francisco's Masters in Analytics program. It includes a free [online textbook](https://github.com/fastai/numerical-linear-algebra/blob/master/README.md) and [series of videos](https://www.youtube.com/playlist?list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY). This course is very different from standard linear algebra (which often focuses on how **humans** do matrix calculations), because it is about how to get **computers** to do matrix computations with speed and accuracy, and incorporates modern tools and algorithms.  All the material is taught in Python and centered around solving practical problems such as removing the background from a surveillance video or implementing Google's PageRank search algorithm on Wikipedia pages."]}, {"block": 7, "type": "markdown", "linesLength": 1, "startIndex": 24, "lines": ["## Our Tools"]}, {"block": 8, "type": "markdown", "linesLength": 1, "startIndex": 25, "lines": ["We will be using the open source fast.ai [deep learning library, fastai](https://github.com/fastai/fastai), which provides high level abstractions and best practices on top of PyTorch."]}, {"block": 9, "type": "code", "linesLength": 2, "startIndex": 26, "lines": ["import sys\n", "sys.path.insert(0, '../')"]}, {"block": 10, "type": "code", "linesLength": 2, "startIndex": 28, "lines": ["%load_ext autoreload\n", "%autoreload 2"]}, {"block": 11, "type": "code", "linesLength": 2, "startIndex": 30, "lines": ["from fastai.imports import *\n", "from fastai.torch_imports import *"]}, {"block": 12, "type": "markdown", "linesLength": 1, "startIndex": 32, "lines": ["### PyTorch"]}, {"block": 13, "type": "markdown", "linesLength": 11, "startIndex": 33, "lines": ["The fastai deep learning library uses [PyTorch](http://pytorch.org/), a Python framework for tensors and dynamic neural networks with GPU acceleration, which was released by Facebook's AI team.  \n", "\n", "From the [PyTorch documentation](http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html):\n", "\n", "<img src=\"images/what_is_pytorch.png\" alt=\"pytorch\" style=\"width: 80%\"/>\n", "\n", "**Further learning**: If you are curious to learn what *dynamic* neural networks are, you may want to watch [this talk](https://www.youtube.com/watch?v=Z15cBAuY7Sc) by Soumith Chintala, Facebook AI researcher and core PyTorch contributor.\n", "\n", "If you want to learn more PyTorch, you can try this [tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) or this [learning by examples](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html).\n", "\n", "**Note about GPUs**: If you are not using a GPU, you will need to remove the `.cuda()` from the methods below. GPU usage is not required for this tutorial, but I thought it would be of interest to some of you.  To learn how to create an AWS instance with a GPU, you can watch the [fast.ai setup lesson](http://course.fast.ai/lessons/aws.html).]"]}, {"block": 14, "type": "markdown", "linesLength": 1, "startIndex": 44, "lines": ["## Datasets"]}, {"block": 15, "type": "markdown", "linesLength": 1, "startIndex": 45, "lines": ["Today we will be working with MNIST, a classic data set of hand-written digits.  Solutions to this problem are used by banks to automatically recognize the amounts on checks, and by the postal service to automatically recognize zip codes on mail."]}, {"block": 16, "type": "markdown", "linesLength": 1, "startIndex": 46, "lines": ["<img src=\"images/mnist.png\" alt=\"\" style=\"width: 60%\"/>"]}, {"block": 17, "type": "markdown", "linesLength": 6, "startIndex": 47, "lines": ["Next, we will look at **CIFAR 10**, a dataset that consists of 32x32 *color* images in 10 different categories.  Color images have an extra dimension, containing RGB values, compared to black & white images.\n", "\n", "<img src=\"images/cifar10.png\" alt=\"\" style=\"width: 70%\"/>\n", "<center>\n", "(source: [Cifar 10](https://www.cs.toronto.edu/~kriz/cifar.html))\n", "</center>"]}, {"block": 18, "type": "markdown", "linesLength": 1, "startIndex": 53, "lines": ["### Download"]}, {"block": 19, "type": "markdown", "linesLength": 1, "startIndex": 54, "lines": ["Let's download, unzip, and format the data:"]}, {"block": 20, "type": "code", "linesLength": 1, "startIndex": 55, "lines": ["path = '../data/'"]}, {"block": 21, "type": "code", "linesLength": 5, "startIndex": 56, "lines": ["URL='http://deeplearning.net/data/mnist/'\n", "FILENAME='mnist.pkl.gz'\n", "\n", "def load_mnist(filename):\n", "    return pickle.load(gzip.open(filename, 'rb'), encoding='latin-1')"]}, {"block": 22, "type": "code", "linesLength": 2, "startIndex": 61, "lines": ["get_data(URL+FILENAME, path+FILENAME)\n", "((x, y), (x_valid, y_valid), (x_test, y_test)) = load_mnist(path+FILENAME)"]}, {"block": 23, "type": "markdown", "linesLength": 1, "startIndex": 63, "lines": ["### Normalize"]}, {"block": 24, "type": "markdown", "linesLength": 1, "startIndex": 64, "lines": ["One of the challenges in training neural networks is keeping your numbers from exploding (going to infinity) or vanishing (going to zero).  There are several different ways to add normalization to address this.  We will subtract off the mean and standard deviation from our training set:"]}, {"block": 25, "type": "code", "linesLength": 2, "startIndex": 65, "lines": ["mean = x.mean()\n", "std = x.std()"]}, {"block": 26, "type": "code", "linesLength": 2, "startIndex": 67, "lines": ["x=(x-mean)/std\n", "x.mean(), x.std()"]}, {"block": 27, "type": "markdown", "linesLength": 1, "startIndex": 69, "lines": ["Note that for consistency (with the parameters we learn when training), we subtract the mean and standard deviation of our training set from our validation set. "]}, {"block": 28, "type": "code", "linesLength": 2, "startIndex": 70, "lines": ["x_valid = (x_valid-mean)/std\n", "x_valid.mean(), x_valid.std()"]}, {"block": 29, "type": "markdown", "linesLength": 1, "startIndex": 72, "lines": ["### Look at the data"]}, {"block": 30, "type": "markdown", "linesLength": 1, "startIndex": 73, "lines": ["#### Helper methods"]}, {"block": 31, "type": "code", "linesLength": 7, "startIndex": 74, "lines": ["%matplotlib inline\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "def show(img, title=None):\n", "    plt.imshow(img, interpolation='none', cmap=\"gray\")\n", "    if title is not None: plt.title(title)"]}, {"block": 32, "type": "code", "linesLength": 8, "startIndex": 81, "lines": ["def plots(ims, figsize=(12,6), rows=2, titles=None):\n", "    f = plt.figure(figsize=figsize)\n", "    cols = len(ims)//rows\n", "    for i in range(len(ims)):\n", "        sp = f.add_subplot(rows, cols, i+1)\n", "        sp.axis('Off')\n", "        if titles is not None: sp.set_title(titles[i], fontsize=16)\n", "        plt.imshow(ims[i], interpolation='none', cmap='gray')"]}, {"block": 33, "type": "markdown", "linesLength": 1, "startIndex": 89, "lines": ["In any sort of data science work, it's important to look at your data, to make sure you understand the format, how it's stored, what type of values it holds, etc. To make it easier to work with, let's reshape it into 2d images from the flattened 1d format."]}, {"block": 34, "type": "markdown", "linesLength": 1, "startIndex": 90, "lines": ["#### Helper methods"]}, {"block": 35, "type": "code", "linesLength": 7, "startIndex": 91, "lines": ["%matplotlib inline\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "def show(img, title=None):\n", "    plt.imshow(img, interpolation='none', cmap=\"gray\")\n", "    if title is not None: plt.title(title)"]}, {"block": 36, "type": "code", "linesLength": 8, "startIndex": 98, "lines": ["def plots(ims, figsize=(12,6), rows=2, titles=None):\n", "    f = plt.figure(figsize=figsize)\n", "    cols = len(ims)//rows\n", "    for i in range(len(ims)):\n", "        sp = f.add_subplot(rows, cols, i+1)\n", "        sp.axis('Off')\n", "        if titles is not None: sp.set_title(titles[i], fontsize=16)\n", "        plt.imshow(ims[i], interpolation='none', cmap='gray')"]}, {"block": 37, "type": "markdown", "linesLength": 1, "startIndex": 106, "lines": ["#### Plots "]}, {"block": 38, "type": "code", "linesLength": 1, "startIndex": 107, "lines": ["x_imgs = np.reshape(x_valid, (-1,28,28)); x_imgs.shape"]}, {"block": 39, "type": "markdown", "linesLength": 1, "startIndex": 108, "lines": ["We can look at part of an image:"]}, {"block": 40, "type": "code", "linesLength": 1, "startIndex": 109, "lines": ["x_imgs[0,10:15,10:15]"]}, {"block": 41, "type": "code", "linesLength": 1, "startIndex": 110, "lines": ["show(x_imgs[0], y[0])"]}, {"block": 42, "type": "markdown", "linesLength": 1, "startIndex": 111, "lines": ["It's the digit 5!  And that's stored in the y value:"]}, {"block": 43, "type": "code", "linesLength": 1, "startIndex": 112, "lines": ["y[0]"]}, {"block": 44, "type": "code", "linesLength": 1, "startIndex": 113, "lines": ["show(x_imgs[0,10:15,10:15])"]}, {"block": 45, "type": "code", "linesLength": 1, "startIndex": 114, "lines": ["plots(x_imgs[:8], titles=y[:8])"]}, {"block": 46, "type": "markdown", "linesLength": 1, "startIndex": 115, "lines": ["## Neural Net (with nn.torch)"]}, {"block": 47, "type": "markdown", "linesLength": 3, "startIndex": 116, "lines": ["We will begin with the highest level abstraction: using a neural net defined by PyTorch's Sequential class (Keras has a similar Sequential class).  \n", "\n", "We will use fastai's ImageClassifierData, which holds our training and validation sets and will provide batches of that data in a form ready for use by a PyTorch model."]}, {"block": 48, "type": "code", "linesLength": 6, "startIndex": 119, "lines": ["from fastai.metrics import *\n", "from fastai.model import *\n", "from fastai.dataset import *\n", "from fastai.core import *\n", "\n", "import torch.nn as nn"]}, {"block": 49, "type": "code", "linesLength": 1, "startIndex": 125, "lines": ["md = ImageClassifierData.from_arrays(path, (x,y), (x_valid, y_valid))"]}, {"block": 50, "type": "markdown", "linesLength": 1, "startIndex": 126, "lines": ["Neural networks consist of **linear layers alternating with non-linear layers**.  This creates functions which are incredibly flexible."]}, {"block": 51, "type": "code", "linesLength": 5, "startIndex": 127, "lines": ["net = nn.Sequential(\n", "    nn.Linear(28*28, 256),\n", "    nn.ReLU(),\n", "    nn.Linear(256, 10)\n", ").cuda()"]}, {"block": 52, "type": "markdown", "linesLength": 3, "startIndex": 132, "lines": ["Each input is a vector of size $28\\times 28$ pixels and our output is of size $10$ (since there are 10 digits: 0, 1, ..., 9).\n", "\n", "I just chose $256$ as the number of hidden states, you could change this to something else."]}, {"block": 53, "type": "markdown", "linesLength": 3, "startIndex": 135, "lines": ["- **Loss**: what function is the optimizer trying to minimize?  We need to say how we're defining the error.\n", "- **Optimizer**: algorithm for finding the minimum. typically these are variations on *stochastic gradient descent*, involve taking a step that appears to be the right direction based on the gradient\n", "- **Metrics**: other calculations you want to see as you train"]}, {"block": 54, "type": "code", "linesLength": 3, "startIndex": 138, "lines": ["loss=F.cross_entropy\n", "metrics=[accuracy]\n", "opt=optim.Adam(net.parameters())"]}, {"block": 55, "type": "code", "linesLength": 1, "startIndex": 141, "lines": ["fit(net, md, epochs=1, crit=loss, opt=opt, metrics=metrics)"]}, {"block": 56, "type": "code", "linesLength": 1, "startIndex": 142, "lines": ["preds = predict(net, md.val_dl)"]}, {"block": 57, "type": "code", "linesLength": 1, "startIndex": 143, "lines": ["preds[0]"]}, {"block": 58, "type": "code", "linesLength": 1, "startIndex": 144, "lines": ["preds = preds.max(1)[1]"]}, {"block": 59, "type": "markdown", "linesLength": 1, "startIndex": 145, "lines": ["Let's see how some of our preditions look!"]}, {"block": 60, "type": "code", "linesLength": 1, "startIndex": 146, "lines": ["plots(x_imgs[:8], titles=preds[:8])"]}, {"block": 61, "type": "markdown", "linesLength": 1, "startIndex": 147, "lines": ["## Coding the Neural Net ourselves"]}, {"block": 62, "type": "markdown", "linesLength": 1, "startIndex": 148, "lines": ["A Tensor is a *multi-dimensional matrix containing elements of a single data type*: a group of data, all with the same type (e.g. A Tensor could store a 4 x 4 x 6 matrix of 32-bit signed integers)."]}, {"block": 63, "type": "code", "linesLength": 2, "startIndex": 149, "lines": ["from torch.autograd import Variable\n", "def get_weights(*dims): return nn.Parameter(torch.randn(*dims)/dims[0])"]}, {"block": 64, "type": "markdown", "linesLength": 1, "startIndex": 151, "lines": ["We will define the neural network ourselves.  Forward describes how the neural net converts inputs --> outputs"]}, {"block": 65, "type": "code", "linesLength": 14, "startIndex": 152, "lines": ["class SimpleMnist(nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.l1_w = get_weights(28*28, 256)  # Layer 1 weights\n", "        self.l1_b = get_weights(256)   # Layer 1 bias\n", "        self.l2_w = get_weights(256, 10)  # Layer 2 weights\n", "        self.l2_b = get_weights(10)   # Layer 2 bias\n", "\n", "    def forward(self, x):\n", "        x = x.view(x.size(0), -1)\n", "        x = torch.mm(x, self.l1_w) + self.l1_b\n", "        x = x * (x > 0).float()\n", "        x = torch.mm(x, self.l2_w) + self.l2_b\n", "        return x"]}, {"block": 66, "type": "code", "linesLength": 2, "startIndex": 166, "lines": ["net2 = SimpleMnist().cuda()\n", "opt=optim.Adam(net2.parameters())"]}, {"block": 67, "type": "code", "linesLength": 1, "startIndex": 168, "lines": ["fit(net2, md, epochs=1, crit=loss, opt=opt, metrics=metrics)"]}, {"block": 68, "type": "code", "linesLength": 2, "startIndex": 169, "lines": ["preds = predict(net2, md.val_dl).max(1)[1]\n", "plots(x_imgs[:8], titles=preds[:8])"]}, {"block": 69, "type": "markdown", "linesLength": 1, "startIndex": 171, "lines": ["## what torch.mm (matrix multiplication) is doing"]}, {"block": 70, "type": "markdown", "linesLength": 1, "startIndex": 172, "lines": ["Now let's dig in to what we were doing with `torch.mm`: matrix multiplication.  First, let's start with a simpler building block: **broadcasting**."]}, {"block": 71, "type": "markdown", "linesLength": 1, "startIndex": 173, "lines": ["### Broadcasting"]}, {"block": 72, "type": "markdown", "linesLength": 1, "startIndex": 174, "lines": ["#### Background"]}, {"block": 73, "type": "markdown", "linesLength": 5, "startIndex": 175, "lines": ["Broadcasting and element-wise operations are supported in the same way by both numpy and pytorch.\n", "\n", "Operators (+,-,\\*,/,>,<,==) are usually element-wise.\n", "\n", "Examples of element-wise operations:"]}, {"block": 74, "type": "code", "linesLength": 2, "startIndex": 180, "lines": ["a = np.array([10, 6, -4])\n", "b = np.array([2, 8, 7])"]}, {"block": 75, "type": "code", "linesLength": 1, "startIndex": 182, "lines": ["a + b"]}, {"block": 76, "type": "code", "linesLength": 1, "startIndex": 183, "lines": ["a < b"]}, {"block": 77, "type": "markdown", "linesLength": 1, "startIndex": 184, "lines": ["#### Broadcasting with a scalar"]}, {"block": 78, "type": "code", "linesLength": 1, "startIndex": 185, "lines": ["a > 0"]}, {"block": 79, "type": "markdown", "linesLength": 1, "startIndex": 186, "lines": ["How are we able to do a > 0?  0 is being **broadcast** to have the same dimensions as a."]}, {"block": 80, "type": "markdown", "linesLength": 1, "startIndex": 187, "lines": ["Other examples of broadcasting with a scalar (as we did when we normalized our data):"]}, {"block": 81, "type": "code", "linesLength": 1, "startIndex": 188, "lines": ["a + 1"]}, {"block": 82, "type": "code", "linesLength": 1, "startIndex": 189, "lines": ["m = np.array([[1, 2, 3], [4,5,6], [7,8,9]]); m"]}, {"block": 83, "type": "code", "linesLength": 1, "startIndex": 190, "lines": ["m * 2"]}, {"block": 84, "type": "markdown", "linesLength": 1, "startIndex": 191, "lines": ["#### Broadcasting a vector to a matrix"]}, {"block": 85, "type": "markdown", "linesLength": 1, "startIndex": 192, "lines": ["We can also broadcast a vector to a matrix:"]}, {"block": 86, "type": "code", "linesLength": 1, "startIndex": 193, "lines": ["c = np.array([10,20,30]); c"]}, {"block": 87, "type": "code", "linesLength": 1, "startIndex": 194, "lines": ["m + c"]}, {"block": 88, "type": "code", "linesLength": 1, "startIndex": 195, "lines": ["np.broadcast_to(c, (3,3))"]}, {"block": 89, "type": "code", "linesLength": 1, "startIndex": 196, "lines": ["m.shape, c.shape"]}, {"block": 90, "type": "markdown", "linesLength": 1, "startIndex": 197, "lines": ["Expand dims"]}, {"block": 91, "type": "code", "linesLength": 1, "startIndex": 198, "lines": ["np.expand_dims(c,0).shape"]}, {"block": 92, "type": "code", "linesLength": 1, "startIndex": 199, "lines": ["m + np.expand_dims(c,0)"]}, {"block": 93, "type": "code", "linesLength": 1, "startIndex": 200, "lines": ["np.expand_dims(c,1).shape"]}, {"block": 94, "type": "code", "linesLength": 1, "startIndex": 201, "lines": ["m + np.expand_dims(c,1)"]}, {"block": 95, "type": "code", "linesLength": 1, "startIndex": 202, "lines": ["np.broadcast_to(np.expand_dims(c,1), (3,3))"]}, {"block": 96, "type": "markdown", "linesLength": 1, "startIndex": 203, "lines": ["### Matrix Multiplication"]}, {"block": 97, "type": "code", "linesLength": 1, "startIndex": 204, "lines": ["m, c"]}, {"block": 98, "type": "markdown", "linesLength": 1, "startIndex": 205, "lines": ["#### Matrix-Vector Multiplication"]}, {"block": 99, "type": "code", "linesLength": 1, "startIndex": 206, "lines": ["m @ c  # np.matmul(m, c)"]}, {"block": 100, "type": "markdown", "linesLength": 1, "startIndex": 207, "lines": ["**NOT** matrix multiplication:"]}, {"block": 101, "type": "code", "linesLength": 1, "startIndex": 208, "lines": ["m * c"]}, {"block": 102, "type": "code", "linesLength": 1, "startIndex": 209, "lines": ["(m * c).sum(axis=1)"]}, {"block": 103, "type": "code", "linesLength": 1, "startIndex": 210, "lines": ["c"]}, {"block": 104, "type": "code", "linesLength": 1, "startIndex": 211, "lines": ["np.broadcast_to(c, (3,3))"]}, {"block": 105, "type": "markdown", "linesLength": 1, "startIndex": 212, "lines": ["Nice visualization [matrixmultiplication.xyz](http://matrixmultiplication.xyz/)"]}, {"block": 106, "type": "markdown", "linesLength": 1, "startIndex": 213, "lines": ["Draw a picture"]}, {"block": 107, "type": "markdown", "linesLength": 1, "startIndex": 214, "lines": ["From a machine learning perspective, this is saying how much we want to weight each column.  Different features are different weights of each column.  Each feature can be a different set of weights.  The output of each feature is 1 column (a weighted average of the input columns)."]}, {"block": 108, "type": "code", "linesLength": 1, "startIndex": 215, "lines": ["d = np.array([30,20,10])"]}, {"block": 109, "type": "code", "linesLength": 1, "startIndex": 216, "lines": ["nn = np.stack([c, d], axis=1); nn"]}, {"block": 110, "type": "code", "linesLength": 1, "startIndex": 217, "lines": ["n = np.array([[10,30],[20,20],[30,10]])"]}, {"block": 111, "type": "code", "linesLength": 1, "startIndex": 218, "lines": ["m @ n"]}, {"block": 112, "type": "code", "linesLength": 1, "startIndex": 219, "lines": ["(m * c).sum(axis=1)"]}, {"block": 113, "type": "code", "linesLength": 1, "startIndex": 220, "lines": ["(m * d).sum(axis=1)"]}, {"block": 114, "type": "markdown", "linesLength": 1, "startIndex": 221, "lines": ["### Matrix and Tensor Products"]}, {"block": 115, "type": "markdown", "linesLength": 1, "startIndex": 222, "lines": ["#### Matrix-Vector Products:"]}, {"block": 116, "type": "markdown", "linesLength": 7, "startIndex": 223, "lines": ["The matrix below gives the probabilities of moving from 1 health state to another in 1 year.  If the current health states for a group are:\n", "- 85% asymptomatic\n", "- 10% symptomatic\n", "- 5% AIDS\n", "- 0% death\n", "\n", "what will be the % in each health state in 1 year?"]}, {"block": 117, "type": "markdown", "linesLength": 1, "startIndex": 230, "lines": ["<img src=\"images/markov_health.jpg\" alt=\"floating point\" style=\"width: 80%\"/>(Source: [Concepts of Markov Chains](https://www.youtube.com/watch?v=0Il-y_WLTo4))"]}, {"block": 118, "type": "markdown", "linesLength": 1, "startIndex": 231, "lines": ["#### Answer"]}, {"block": 119, "type": "code", "linesLength": 1, "startIndex": 232, "lines": ["import numpy as np"]}, {"block": 120, "type": "code", "linesLength": 1, "startIndex": 233, "lines": ["#Exercise: Use Numpy to compute the answer to the above\n"]}, {"block": 121, "type": "markdown", "linesLength": 1, "startIndex": 234, "lines": ["#### Matrix-Matrix Products"]}, {"block": 122, "type": "markdown", "linesLength": 1, "startIndex": 235, "lines": ["<img src=\"images/shop.png\" alt=\"floating point\" style=\"width: 100%\"/>(Source: [Several Simple Real-world Applications of Linear Algebra Tools](https://www.mff.cuni.cz/veda/konference/wds/proc/pdf06/WDS06_106_m8_Ulrychova.pdf))"]}, {"block": 123, "type": "markdown", "linesLength": 1, "startIndex": 236, "lines": ["#### Answer"]}, {"block": 124, "type": "code", "linesLength": 1, "startIndex": 237, "lines": ["#Exercise: Use Numpy to compute the answer to the above\n"]}, {"block": 125, "type": "markdown", "linesLength": 1, "startIndex": 238, "lines": ["#### Image Data"]}, {"block": 126, "type": "markdown", "linesLength": 5, "startIndex": 239, "lines": ["Images can be represented by matrices.\n", "\n", "<img src=\"images/digit.gif\" alt=\"digit\" style=\"width: 55%\"/>\n", "  (Source: [Adam Geitgey\n", "](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721))\n"]}, {"block": 127, "type": "markdown", "linesLength": 1, "startIndex": 244, "lines": ["## More Resources"]}, {"block": 128, "type": "markdown", "linesLength": 1, "startIndex": 245, "lines": ["[3 Blue 1 Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab): beautiful linear algebra videos"]}, {"block": 129, "type": "markdown", "linesLength": 9, "startIndex": 246, "lines": ["[PyTorch](http://pytorch.org/) is a Python framework for tensors and dynamic neural networks with GPU acceleration.  Many of the core contributors work on Facebook's AI team.  In many ways, it is similar to Numpy, only with the increased parallelization of using a GPU.\n", "\n", "From the [PyTorch documentation](http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html):\n", "\n", "<img src=\"images/what_is_pytorch.png\" alt=\"pytorch\" style=\"width: 80%\"/>\n", "\n", "**Further learning**: If you are curious to learn what *dynamic* neural networks are, you may want to watch [this talk](https://www.youtube.com/watch?v=Z15cBAuY7Sc) by Soumith Chintala, Facebook AI researcher and core PyTorch contributor.\n", "\n", "If you want to learn more PyTorch, you can try this [tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) or this [learning by examples](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html)."]}, {"block": 130, "type": "code", "linesLength": 0, "startIndex": 255, "lines": []}]