[{"block": 0, "type": "markdown", "linesLength": 4, "startIndex": 0, "lines": ["<i>Copyright (c) Microsoft Corporation. All rights reserved.<br>\n", "Licensed under the MIT License.</i>\n", "<br><br>\n", "# SVD Hyperparameter Tuning with Azure Machine Learning"]}, {"block": 1, "type": "markdown", "linesLength": 12, "startIndex": 4, "lines": ["In this notebook, we show how to tune the hyperparameters of a matrix factorization algorithm by utilizing **Azure Machine Learning service** ([AzureML](https://azure.microsoft.com/en-us/services/machine-learning-service/)) in the context of movie recommendations. To use AzureML you will need an Azure subscription. We use the SVD algorithm from the Surprise library.\n", "\n", "We present the overall process of utilizing AzureML by demonstrating some key steps while avoiding too much detail. \n", "\n", "For more details about the **SVD** algorithm:\n", "* [Surprise SVD deep-dive notebook](../02_model/surprise_svd_deep_dive.ipynb)\n", "* [Original paper](http://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf)\n", "* [Surprise homepage](https://surprise.readthedocs.io/en/stable/)\n", "  \n", "Regarding **AzureML**, please refer to:\n", "* [Quickstart notebook](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python)\n", "* [Hyperdrive](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters)"]}, {"block": 2, "type": "code", "linesLength": 21, "startIndex": 16, "lines": ["import sys\n", "sys.path.append(\"../../\")\n", "import time\n", "import os\n", "import surprise\n", "import papermill as pm\n", "import pandas as pd\n", "import shutil\n", "from reco_utils.dataset import movielens\n", "from reco_utils.dataset.python_splitters import python_random_split\n", "from reco_utils.evaluation.python_evaluation import rmse, precision_at_k, ndcg_at_k\n", "from reco_utils.recommender.surprise.surprise_utils import compute_predictions, compute_all_predictions\n", "\n", "print(\"System version: {}\".format(sys.version))\n", "print(\"Surprise version: {}\".format(surprise.__version__))\n", "\n", "import azureml as aml\n", "import azureml.widgets\n", "import azureml.train.hyperdrive as hd\n", "\n", "print(\"Azure ML SDK Version:\", aml.core.VERSION)"]}, {"block": 3, "type": "markdown", "linesLength": 0, "startIndex": 37, "lines": []}, {"block": 4, "type": "code", "linesLength": 26, "startIndex": 37, "lines": ["# AzureML workspace info. Note, will look up \"aml_config\\config.json\" first, then fall back to using this\n", "SUBSCRIPTION_ID = '<subscription-id>'\n", "RESOURCE_GROUP  = '<resource-group>'\n", "WORKSPACE_NAME  = '<workspace-name>'\n", "\n", "# Connect to a workspace\n", "try:\n", "    ws = aml.core.Workspace.from_config()\n", "except aml.exceptions.UserErrorException:\n", "    try:\n", "        ws = aml.core.Workspace(\n", "            subscription_id=SUBSCRIPTION_ID,\n", "            resource_group=RESOURCE_GROUP,\n", "            workspace_name=WORKSPACE_NAME\n", "        )\n", "        ws.write_config()\n", "    except aml.exceptions.AuthenticationException:\n", "        ws = None\n", "\n", "if ws is None:\n", "    raise ValueError(\n", "        \"\"\"Cannot access the AzureML workspace w/ the config info provided.\n", "        Please check if you entered the correct id, group name and workspace name\"\"\"\n", "    )\n", "else:\n", "    print(\"AzureML workspace name: \", ws.name)"]}, {"block": 5, "type": "markdown", "linesLength": 4, "startIndex": 63, "lines": ["From the following cells, we\n", "1. Create a *remote compute target* (cpu_cluster) if it does not exist already,\n", "2. Mount a *data store* and upload the training set, and\n", "3. Run a hyperparameter tuning experiment."]}, {"block": 6, "type": "markdown", "linesLength": 5, "startIndex": 67, "lines": ["### Create a Remote Compute Target\n", "\n", "We create an AI Compute for our remote compute target. The script will load the cluster if it already exists. You can look at [this document](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets) to learn more about setting up a *compute target*.\n", "\n", "> Note: we create a low priority cluster to save costs."]}, {"block": 7, "type": "code", "linesLength": 30, "startIndex": 72, "lines": ["from azureml.core.compute import ComputeTarget, AmlCompute\n", "from azureml.core.compute_target import ComputeTargetException\n", "\n", "# Remote compute (cluster) configuration. If you want to save costs decrease these.\n", "# Each standard_D2_V2 VM has 2 vCPUs, 7GB memory, 100GB SSD storage\n", "\n", "VM_SIZE = 'STANDARD_D2_V2'\n", "VM_PRIORITY = 'lowpriority'\n", "# Cluster nodes\n", "MIN_NODES = 4\n", "MAX_NODES = 8\n", "\n", "# Choose a name for your CPU cluster\n", "cpu_cluster_name = \"cpuclustersvd\"\n", "\n", "# Verify that cluster does not exist already\n", "try:\n", "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n", "    print('Found existing cluster, use it.')\n", "except ComputeTargetException:\n", "    compute_config = AmlCompute.provisioning_configuration(vm_size=VM_SIZE, \n", "                                                           min_nodes=MIN_NODES, \n", "                                                           vm_priority=VM_PRIORITY,\n", "                                                           max_nodes=MAX_NODES)\n", "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n", "\n", "cpu_cluster.wait_for_completion(show_output=True)\n", "\n", "# Use the 'status' property to get a detailed status for the current cluster. \n", "print(cpu_cluster.status.serialize())"]}, {"block": 8, "type": "markdown", "linesLength": 1, "startIndex": 102, "lines": ["Set up the configuration of the remote cluster and conda dependencies from the repository yaml file. "]}, {"block": 9, "type": "code", "linesLength": 25, "startIndex": 103, "lines": ["from azureml.core.runconfig import RunConfiguration\n", "from azureml.core.conda_dependencies import CondaDependencies\n", "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n", "\n", "# Create a new runconfig object\n", "run_amlcompute = RunConfiguration()\n", "\n", "# Use the cpu_cluster you created above. \n", "run_amlcompute.target = cpu_cluster\n", "\n", "# Enable Docker\n", "run_amlcompute.environment.docker.enabled = True\n", "\n", "# Set Docker base image to the default CPU-based image\n", "run_amlcompute.environment.docker.base_image = DEFAULT_CPU_IMAGE\n", "\n", "# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\n", "run_amlcompute.environment.python.user_managed_dependencies = False\n", "\n", "# Auto-prepare the Docker image when used for execution (if it is not already prepared)\n", "run_amlcompute.auto_prepare_environment = True\n", "\n", "# Specify CondaDependencies obj, add necessary packages\n", "run_amlcompute.environment.python.conda_dependencies = CondaDependencies(conda_dependencies_file_path=\n", "                                                                         '../../scripts/reco_base.yaml')"]}, {"block": 10, "type": "markdown", "linesLength": 3, "startIndex": 128, "lines": ["### Prepare Dataset\n", "1. Download data and split into training and testing sets\n", "2. Upload the training set to the default **blob storage** of the workspace."]}, {"block": 11, "type": "code", "linesLength": 2, "startIndex": 131, "lines": ["# Select Movielens data size: 100k, 1m, 10m, or 20m\n", "MOVIELENS_DATA_SIZE = '100k'"]}, {"block": 12, "type": "code", "linesLength": 6, "startIndex": 133, "lines": ["data = movielens.load_pandas_df(\n", "    size=MOVIELENS_DATA_SIZE,\n", "    header=[\"userID\", \"itemID\", \"rating\"]\n", ")\n", "\n", "data.head()"]}, {"block": 13, "type": "code", "linesLength": 1, "startIndex": 139, "lines": ["train, validation, test = python_random_split(data, [0.7, 0.15, 0.15])"]}, {"block": 14, "type": "code", "linesLength": 20, "startIndex": 140, "lines": ["DATA_DIR = 'aml_data'\n", "os.makedirs(DATA_DIR, exist_ok=True)\n", "\n", "TRAIN_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_train.pkl\"\n", "train.to_pickle(os.path.join(DATA_DIR, TRAIN_FILE_NAME))\n", "\n", "VAL_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_val.pkl\"\n", "validation.to_pickle(os.path.join(DATA_DIR, VAL_FILE_NAME))\n", "\n", "TEST_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_test.pkl\"\n", "test.to_pickle(os.path.join(DATA_DIR, TEST_FILE_NAME))\n", "\n", "# Note, all the files under DATA_DIR will be uploaded to the data store\n", "ds = ws.get_default_datastore()\n", "ds.upload(\n", "    src_dir=DATA_DIR,\n", "    target_path='data',\n", "    overwrite=True,\n", "    show_progress=True\n", ")"]}, {"block": 15, "type": "markdown", "linesLength": 1, "startIndex": 160, "lines": ["We also prepare a training script [svd_training.py](../../reco_utils/aml/svd_training.py) for the hyperparameter tuning, which will log our target metrics such as [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation) and/or [NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) to AzureML experiment so that we can track the metrics and optimize the primary metric via **hyperdrive**."]}, {"block": 16, "type": "code", "linesLength": 9, "startIndex": 161, "lines": ["SCRIPT_DIR = 'aml_script'\n", "\n", "# Clean-up scripts if already exists\n", "shutil.rmtree(SCRIPT_DIR, ignore_errors=True)\n", "\n", "# Copy scripts to SCRIPT_DIR temporarly\n", "shutil.copytree(os.path.join('..', '..', 'reco_utils'), os.path.join(SCRIPT_DIR, 'reco_utils'))\n", "\n", "ENTRY_SCRIPT_NAME = 'reco_utils/aml/svd_training.py'"]}, {"block": 17, "type": "markdown", "linesLength": 5, "startIndex": 170, "lines": ["Now we define a search space for the hyperparameters. All the parameter values will be passed to our training script.\n", "\n", "We specify the output directory as ./outputs. The outputs directory is specially treated by Azure ML in that all the content in this directory gets uploaded to the workspace as part of the run history. The files written to this directory are therefore accessible even once the remote run is over. In the training script (svd_training.py), we use the output directory for saving the trained models. \n", "\n", "AzureML hyperdrive provides `RandomParameterSampling`, `GridParameterSampling`, and `BayesianParameterSampling`. Details about each approach are beyond the scope of this notebook and can be found in [Azure doc](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters). Here, we use the Bayesian sampling."]}, {"block": 18, "type": "code", "linesLength": 54, "startIndex": 175, "lines": ["EXP_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_svd_model\"\n", "PRIMARY_METRIC = 'precision_at_k'\n", "RATING_METRICS = ['rmse']\n", "RANKING_METRICS = ['precision_at_k', 'ndcg_at_k']  \n", "USERCOL = 'userID'\n", "ITEMCOL = 'itemID'\n", "RECOMMEND_SEEN = False\n", "K = 10\n", "RANDOM_STATE = 0\n", "VERBOSE = True\n", "NUM_EPOCHS = 30\n", "BIASED = True\n", "\n", "script_params = {\n", "    '--datastore': ds.as_mount(),\n", "    '--train-datapath': \"data/\" + TRAIN_FILE_NAME,\n", "    '--validation-datapath': \"data/\" + VAL_FILE_NAME,\n", "    '--output_dir': './outputs',\n", "    '--surprise-reader': 'ml-100k',\n", "    '--rating-metrics': RATING_METRICS,\n", "    '--ranking-metrics': RANKING_METRICS,\n", "    '--usercol': USERCOL,\n", "    '--itemcol': ITEMCOL,\n", "    '--k': K,\n", "    '--random-state': RANDOM_STATE,\n", "    '--epochs': NUM_EPOCHS,\n", "}\n", "\n", "if BIASED:\n", "    script_params['--biased'] = ''\n", "if VERBOSE:\n", "    script_params['--verbose'] = ''\n", "if RECOMMEND_SEEN:\n", "    script_params['--recommend-seen'] = ''\n", "    \n", "# hyperparameters search space\n", "# We do not set 'lr_all' and 'reg_all' because they will be overriden by the other lr_ and reg_ parameters\n", "\n", "hyper_params = {\n", "    'n_factors': hd.choice(10, 50, 100, 150, 200),\n", "    'init_mean': hd.uniform(-0.5, 0.5),\n", "    'init_std_dev': hd.uniform(0.01, 0.2),\n", "    'lr_bu': hd.uniform(1e-6, 0.1), \n", "    'lr_bi': hd.uniform(1e-6, 0.1), \n", "    'lr_pu': hd.uniform(1e-6, 0.1), \n", "    'lr_qi': hd.uniform(1e-6, 0.1), \n", "    'reg_bu': hd.uniform(1e-6, 1),\n", "    'reg_bi': hd.uniform(1e-6, 1), \n", "    'reg_pu': hd.uniform(1e-6, 1), \n", "    'reg_qi': hd.uniform(1e-6, 1)\n", "}\n", "\n", "# Note, BayesianParameterSampling only support choice, uniform, and quniform\n", "ps = hd.BayesianParameterSampling(hyper_params)"]}, {"block": 19, "type": "markdown", "linesLength": 8, "startIndex": 229, "lines": ["Once you submit the experiment, you can see the progress from the notebook by using `azureml.widgets.RunDetails`. You can directly check the details from the Azure portal as well. To get the link, run `run.get_portal_url()`.\n", "\n", "For RandomSampling, you can use early termnination policy\n", "```\n", "policy = hd.BanditPolicy(evaluation_interval=1, slack_factor=0.1, delay_evaluation=3)\n", "```\n", "\n", "> Since we will do hyperparameter tuning, we create a `HyperDriveRunConfig` and pass it to the experiment object. If you already know what hyperparameters to use and still want to utilize AzureML for other purposes (e.g. model management), you can set the hyperparameter values directly to `script_params` and run the experiment, `run = exp.submit(est)`, instead.  "]}, {"block": 20, "type": "code", "linesLength": 21, "startIndex": 237, "lines": ["# Hyperdrive experimentation configuration\n", "MAX_TOTAL_RUNS = 100  # Number of runs (training-and-evaluation) to search for the best hyperparameters. \n", "MAX_CONCURRENT_RUNS = 8\n", "\n", "est = azureml.train.estimator.Estimator(\n", "    source_directory=SCRIPT_DIR,\n", "    entry_script=ENTRY_SCRIPT_NAME,\n", "    script_params=script_params,\n", "    compute_target=cpu_cluster,\n", "    conda_packages=['pandas', 'scikit-learn'],\n", "    pip_packages=['scikit-surprise']\n", ")\n", "\n", "hd_config = hd.HyperDriveRunConfig(\n", "    estimator=est, \n", "    hyperparameter_sampling=ps,\n", "    primary_metric_name=PRIMARY_METRIC,\n", "    primary_metric_goal=hd.PrimaryMetricGoal.MAXIMIZE, \n", "    max_total_runs=MAX_TOTAL_RUNS,\n", "    max_concurrent_runs=MAX_CONCURRENT_RUNS\n", ")"]}, {"block": 21, "type": "code", "linesLength": 6, "startIndex": 258, "lines": ["# Create an experiment to track the runs in the workspace\n", "exp = aml.core.Experiment(workspace=ws, name=EXP_NAME)\n", "run = exp.submit(config=hd_config)\n", "\n", "azureml.widgets.RunDetails(run).show()\n", "run.wait_for_completion(show_output=True)"]}, {"block": 22, "type": "code", "linesLength": 5, "startIndex": 264, "lines": ["# Get best run and printout metrics\n", "best_run = run.get_best_run_by_primary_metric()\n", "\n", "best_run_metrics = best_run.get_metrics()\n", "parameter_values = best_run.get_details()['runDefinition']['Arguments']"]}, {"block": 23, "type": "code", "linesLength": 1, "startIndex": 269, "lines": ["best_run_metrics"]}, {"block": 24, "type": "code", "linesLength": 1, "startIndex": 270, "lines": ["print(\" \".join(parameter_values))"]}, {"block": 25, "type": "markdown", "linesLength": 1, "startIndex": 271, "lines": ["Now evaluate the metrics on the test data. To do this, get the SVD model that was saved as model.dump in the training script."]}, {"block": 26, "type": "code", "linesLength": 2, "startIndex": 272, "lines": ["os.makedirs('aml_model', exist_ok=True)\n", "best_run.download_file('outputs/model.dump', output_file_path='aml_model/')"]}, {"block": 27, "type": "code", "linesLength": 1, "startIndex": 274, "lines": ["svd = surprise.dump.load('aml_model/model.dump')[1]"]}, {"block": 28, "type": "code", "linesLength": 10, "startIndex": 275, "lines": ["test_results = {}\n", "predictions = compute_predictions(svd, test, usercol=\"userID\", itemcol=\"itemID\")\n", "for metric in RATING_METRICS:\n", "    test_results[metric] = eval(metric)(test, predictions)\n", "\n", "all_predictions = compute_all_predictions(svd, train, usercol=\"userID\", itemcol=\"itemID\", recommend_seen=RECOMMEND_SEEN)\n", "for metric in RANKING_METRICS:\n", "    test_results[metric] = eval(metric)(test, all_predictions, col_prediction='prediction', k=K)\n", "\n", "print(test_results)"]}, {"block": 29, "type": "code", "linesLength": 5, "startIndex": 285, "lines": ["try:\n", "    shutil.rmtree(SCRIPT_DIR)\n", "    shutil.rmtree(DATA_DIR)\n", "except (PermissionError, FileNotFoundError):\n", "    pass"]}, {"block": 30, "type": "markdown", "linesLength": 6, "startIndex": 290, "lines": ["### References\n", "\n", "* [Matrix factorization algorithms in Surprise](https://surprise.readthedocs.io/en/stable/matrix_factorization.html) \n", "* [Surprise SVD deep-dive notebook](../02_model/surprise_svd_deep_dive.ipynb)\n", "* [Fine-tune natural language processing models using Azure Machine Learning service](https://azure.microsoft.com/en-us/blog/fine-tune-natural-language-processing-models-using-azure-machine-learning-service/)\n", "* [Training, hyperparameter tune, and deploy with TensorFlow](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training-with-deep-learning/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb)\n"]}]