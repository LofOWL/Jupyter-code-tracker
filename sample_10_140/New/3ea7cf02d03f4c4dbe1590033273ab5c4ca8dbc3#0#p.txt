[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["# Data split"]}, {"block": 1, "type": "markdown", "linesLength": 3, "startIndex": 1, "lines": ["Data splitting is one of the most vital tasks in assessing recommendation systems. Splitting strategy greatly affects the evaluation protocol so that it should always be taken into careful consideration by practitioners.\n", "\n", "The code hereafter explains how one applies different splitting strategies for specific scenarios."]}, {"block": 2, "type": "markdown", "linesLength": 1, "startIndex": 4, "lines": ["## 0 Global settings"]}, {"block": 3, "type": "code", "linesLength": 17, "startIndex": 5, "lines": ["# set the environment path to find Recommenders\n", "import sys\n", "sys.path.append(\"../../\")\n", "\n", "import pyspark\n", "import pandas as pd\n", "import numpy as np\n", "from datetime import datetime, timedelta\n", "\n", "from reco_utils.common.spark_utils import start_or_get_spark\n", "from reco_utils.recommender.sar.sar_pyspark import SARpySparkReference\n", "from reco_utils.dataset.url_utils import maybe_download\n", "from reco_utils.dataset.python_splitters import python_random_split, python_chrono_split\n", "from reco_utils.dataset.spark_splitters import spark_random_split, spark_chrono_split\n", "\n", "print(\"System version: {}\".format(sys.version))\n", "print(\"Pyspark version: {}\".format(pyspark.__version__))"]}, {"block": 4, "type": "code", "linesLength": 8, "startIndex": 22, "lines": ["DATA_URL = \"http://files.grouplens.org/datasets/movielens/ml-100k/u.data\"\n", "DATA_PATH = \"ml-100k.data\"\n", "\n", "COL_USER = \"UserId\"\n", "COL_ITEM = \"MovieId\"\n", "COL_RATING = \"Rating\"\n", "COL_PREDICTION = \"Rating\"\n", "COL_TIMESTAMP = \"Timestamp\""]}, {"block": 5, "type": "markdown", "linesLength": 1, "startIndex": 30, "lines": ["## 1 Data preparation"]}, {"block": 6, "type": "markdown", "linesLength": 1, "startIndex": 31, "lines": ["### 1.1 Data understanding"]}, {"block": 7, "type": "markdown", "linesLength": 1, "startIndex": 32, "lines": ["For illustration purpose, the data used in the examples below is the Movielens-100K dataset."]}, {"block": 8, "type": "code", "linesLength": 1, "startIndex": 33, "lines": ["filepath = maybe_download(DATA_URL, DATA_PATH)"]}, {"block": 9, "type": "code", "linesLength": 1, "startIndex": 34, "lines": ["data = pd.read_csv(filepath, sep=\"\\t\", names=[COL_USER, COL_ITEM, COL_RATING, COL_TIMESTAMP])"]}, {"block": 10, "type": "markdown", "linesLength": 1, "startIndex": 35, "lines": ["A glimpse at the data"]}, {"block": 11, "type": "code", "linesLength": 1, "startIndex": 36, "lines": ["data.head()"]}, {"block": 12, "type": "markdown", "linesLength": 1, "startIndex": 37, "lines": ["A little more..."]}, {"block": 13, "type": "code", "linesLength": 1, "startIndex": 38, "lines": ["data.describe()"]}, {"block": 14, "type": "markdown", "linesLength": 1, "startIndex": 39, "lines": ["And, more..."]}, {"block": 15, "type": "code", "linesLength": 6, "startIndex": 40, "lines": ["print(\n", "    \"Total number of ratings are\\t{}\".format(data.shape[0]),\n", "    \"Total number of users are\\t{}\".format(data[COL_USER].nunique()),\n", "    \"Total number of items are\\t{}\".format(data[COL_ITEM].nunique()),\n", "    sep=\"\\n\"\n", ")"]}, {"block": 16, "type": "markdown", "linesLength": 1, "startIndex": 46, "lines": ["### 1.2 Data transformation"]}, {"block": 17, "type": "markdown", "linesLength": 1, "startIndex": 47, "lines": ["Original timestamps are converted to ISO format."]}, {"block": 18, "type": "code", "linesLength": 4, "startIndex": 48, "lines": ["data[COL_TIMESTAMP]= data.apply(\n", "    lambda x: datetime.strftime(datetime(1970, 1, 1, 0, 0, 0) + timedelta(seconds=x[COL_TIMESTAMP].item()), \"%Y-%m-%d %H:%M:%S\"), \n", "    axis=1\n", ")"]}, {"block": 19, "type": "code", "linesLength": 1, "startIndex": 52, "lines": ["data.head()"]}, {"block": 20, "type": "markdown", "linesLength": 1, "startIndex": 53, "lines": ["## 2 Experimentation protocol"]}, {"block": 21, "type": "markdown", "linesLength": 3, "startIndex": 54, "lines": ["Experimentation protocol is usually set up to favor a reasonable evaluation for a specific recommendation scenario. For example,\n", "* *Recommender-A* is to recommend movies to people by taking people's collaborative rating similarities. To make sure the evaluation is statisically sound, the same set of users for both model building and testing should be used (to avoid any cold-ness of users), and a stratified splitting strategy should be taken.\n", "* *Recommender-B* is to recommend fashion products to customers. It makes sense that evaluation of the recommender considers time-dependency of customer purchases, as apparently, tastes of the customers in fashion items may be drifting over time. In this case, a chronologically splitting should be used."]}, {"block": 22, "type": "markdown", "linesLength": 1, "startIndex": 57, "lines": ["## 3 Data split"]}, {"block": 23, "type": "markdown", "linesLength": 1, "startIndex": 58, "lines": ["### 3.1 Random split"]}, {"block": 24, "type": "markdown", "linesLength": 1, "startIndex": 59, "lines": ["Random split simply takes in a data set and outputs the splits of the data, given the split ratios."]}, {"block": 25, "type": "code", "linesLength": 1, "startIndex": 60, "lines": ["data_train, data_test = python_random_split(data, ratio=0.7)"]}, {"block": 26, "type": "code", "linesLength": 1, "startIndex": 61, "lines": ["data_train.shape[0], data_test.shape[0]"]}, {"block": 27, "type": "markdown", "linesLength": 1, "startIndex": 62, "lines": ["Sometimes a multi-split is needed."]}, {"block": 28, "type": "code", "linesLength": 1, "startIndex": 63, "lines": ["data_train, data_validate, data_test = python_random_split(data, ratio=[0.6, 0.2, 0.2])"]}, {"block": 29, "type": "code", "linesLength": 1, "startIndex": 64, "lines": ["data_train.shape[0], data_validate.shape[0], data_test.shape[0]"]}, {"block": 30, "type": "markdown", "linesLength": 1, "startIndex": 65, "lines": ["Ratios can be integers as well."]}, {"block": 31, "type": "code", "linesLength": 1, "startIndex": 66, "lines": ["data_train, data_validate, data_test = python_random_split(data, ratio=[3, 1, 1])"]}, {"block": 32, "type": "markdown", "linesLength": 1, "startIndex": 67, "lines": ["For producing the same results."]}, {"block": 33, "type": "code", "linesLength": 1, "startIndex": 68, "lines": ["data_train.shape[0], data_validate.shape[0], data_test.shape[0]"]}, {"block": 34, "type": "markdown", "linesLength": 1, "startIndex": 69, "lines": ["### 3.2 Chronological split"]}, {"block": 35, "type": "markdown", "linesLength": 1, "startIndex": 70, "lines": ["Chronogically splitting method takes in a dataset and splits it on timestamp. "]}, {"block": 36, "type": "markdown", "linesLength": 1, "startIndex": 71, "lines": ["#### 3.2.1 \"Filter by\""]}, {"block": 37, "type": "markdown", "linesLength": 1, "startIndex": 72, "lines": ["Chrono splitting can be either by \"user\" or \"item\". For example, if it is by \"user\" and the splitting ratio is 0.7, it means that first 70% ratings for each user in the data will be put into one split while the other 30% is in another. It is worth noting that a chronological split is not \"random\" because splitting is timestamp-dependent."]}, {"block": 38, "type": "code", "linesLength": 4, "startIndex": 73, "lines": ["data_train, data_test = python_chrono_split(\n", "    data, ratio=0.7, filter_by=\"user\",\n", "    col_user=COL_USER, col_item=COL_ITEM, col_timestamp=COL_TIMESTAMP\n", ")"]}, {"block": 39, "type": "markdown", "linesLength": 1, "startIndex": 77, "lines": ["Take a look at the results for one particular user:"]}, {"block": 40, "type": "markdown", "linesLength": 1, "startIndex": 78, "lines": ["* The last 10 rows of the train data:"]}, {"block": 41, "type": "code", "linesLength": 1, "startIndex": 79, "lines": ["data_train[data_train[COL_USER] == 1].tail(10)"]}, {"block": 42, "type": "markdown", "linesLength": 1, "startIndex": 80, "lines": ["* The first 10 rows of the test data:"]}, {"block": 43, "type": "code", "linesLength": 1, "startIndex": 81, "lines": ["data_test[data_test[COL_USER] == 1].head(10)"]}, {"block": 44, "type": "markdown", "linesLength": 1, "startIndex": 82, "lines": ["Timestamps of train data are all precedent to those in test data."]}, {"block": 45, "type": "markdown", "linesLength": 1, "startIndex": 83, "lines": ["#### 3.3.2 Min-rating filter"]}, {"block": 46, "type": "markdown", "linesLength": 3, "startIndex": 84, "lines": ["A min-rating filter is applied to data before it is split by using chronological splitter. The reason of doing this is that, for multi-split, there should be sufficient number of ratings for user/item in the data.\n", "\n", "For example, the following means splitting only applies to users that have at least 10 ratings."]}, {"block": 47, "type": "code", "linesLength": 4, "startIndex": 87, "lines": ["data_train, data_test = python_chrono_split(\n", "    data, filter_by=\"user\", min_rating=10, ratio=0.7,\n", "    col_user=COL_USER, col_item=COL_ITEM, col_timestamp=COL_TIMESTAMP\n", ")"]}, {"block": 48, "type": "markdown", "linesLength": 1, "startIndex": 91, "lines": ["Number of rows in the yielded splits of data may not sum to the original ones as users with fewer than 10 ratings are filtered out in the splitting."]}, {"block": 49, "type": "code", "linesLength": 1, "startIndex": 92, "lines": ["data_train.shape[0] + data_test.shape[0], data.shape[0]"]}, {"block": 50, "type": "markdown", "linesLength": 1, "startIndex": 93, "lines": ["### 3.4 Data split in scale"]}, {"block": 51, "type": "markdown", "linesLength": 1, "startIndex": 94, "lines": ["Spark DataFrame is used for scalable splitting. This allows splitting operation performed on large dataset that is distributed across Spark cluster."]}, {"block": 52, "type": "markdown", "linesLength": 1, "startIndex": 95, "lines": ["For example, the below illustrates how to do a random split on the given Spark DataFrame. For simplicity reason, the same Movielens data, which is in Pandas DataFrame, is transformed into Spark DataFrame and used for splitting."]}, {"block": 53, "type": "code", "linesLength": 1, "startIndex": 96, "lines": ["spark = start_or_get_spark()"]}, {"block": 54, "type": "code", "linesLength": 1, "startIndex": 97, "lines": ["data_spark = spark.read.csv(filepath)"]}, {"block": 55, "type": "code", "linesLength": 1, "startIndex": 98, "lines": ["data_spark_train, data_spark_test = spark_random_split(data_spark, ratio=0.7)"]}, {"block": 56, "type": "markdown", "linesLength": 1, "startIndex": 99, "lines": ["Interestingly, it was noticed that Spark random split does not guarantee a deterministic result. This sometimes leads to issues when data is relatively small while users seek for a precision split. "]}, {"block": 57, "type": "code", "linesLength": 1, "startIndex": 100, "lines": ["data_spark_train.count(), data_spark_test.count()"]}, {"block": 58, "type": "markdown", "linesLength": 1, "startIndex": 101, "lines": ["## References"]}, {"block": 59, "type": "markdown", "linesLength": 3, "startIndex": 102, "lines": ["1. Dimitris Paraschakis et al, \"Comparative Evaluation of Top-N Recommenders in e-Commerce: An Industrial Perspective\", IEEE ICMLA, 2015, Miami, FL, USA.\n", "2. Guy Shani and Asela Gunawardana, \"Evaluating Recommendation Systems\", Recommender Systems Handbook, Springer, 2015. \n", "3. Apache Spark, url: https://spark.apache.org/."]}]