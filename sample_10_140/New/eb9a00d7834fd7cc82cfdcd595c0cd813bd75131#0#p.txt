[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["# Stochastic Gradient Descent (SGD)"]}, {"block": 1, "type": "code", "linesLength": 11, "startIndex": 1, "lines": ["%matplotlib inline\n", "import math,sys,os,numpy as np\n", "import torch\n", "from matplotlib import pyplot as plt, rcParams, animation, rc\n", "from __future__ import print_function, division\n", "from ipywidgets import interact, interactive, fixed\n", "from ipywidgets.widgets import *\n", "rc('animation', html='html5')\n", "rcParams['figure.figsize'] = 3, 3\n", "%precision 4\n", "np.set_printoptions(precision=4, linewidth=100)"]}, {"block": 2, "type": "markdown", "linesLength": 1, "startIndex": 12, "lines": ["### Linear Regression problem"]}, {"block": 3, "type": "markdown", "linesLength": 1, "startIndex": 13, "lines": ["The goal of linear regression is to fit a line to a set of points."]}, {"block": 4, "type": "code", "linesLength": 9, "startIndex": 14, "lines": ["# Here we generate some fake data\n", "def lin(a,b,x): return a*x+b\n", "\n", "def gen_fake_data(n, a, b):\n", "    x = s = np.random.uniform(0,1,n) \n", "    y = lin(a,b,x) + 0.1 * np.random.normal(0,3,n)\n", "    return x, y\n", "\n", "x, y = gen_fake_data(50, 3., 8.)"]}, {"block": 5, "type": "code", "linesLength": 1, "startIndex": 23, "lines": ["plt.scatter(x,y, s=8); plt.xlabel(\"x\"); plt.ylabel(\"y\"); "]}, {"block": 6, "type": "markdown", "linesLength": 1, "startIndex": 24, "lines": ["You want to find *parameters* (weights) $a$ and $b$ such that you minimize the *error* between the points and the line $a\\cdot x + b$. Note that here $a$ and $b$ are unknown. For a regression problem the most common *error function* or *lost function* is the **mean squared error**. "]}, {"block": 7, "type": "code", "linesLength": 4, "startIndex": 25, "lines": ["def mean_square_error(y_hat, y):\n", "    \"\"\" Returns the mean square error.\n", "    \"\"\"\n", "    return np.power(y_hat - y, 2).sum()/y.shape[0]"]}, {"block": 8, "type": "markdown", "linesLength": 1, "startIndex": 29, "lines": ["Suppose we believe $a = 10$ and $b = 5$ then we can compute `y_hat` which is our *prediction* and then compute our error."]}, {"block": 9, "type": "code", "linesLength": 3, "startIndex": 30, "lines": ["y_hat =  lin(10,5,x)\n", "\n", "mean_square_error(y_hat, y)"]}, {"block": 10, "type": "code", "linesLength": 6, "startIndex": 33, "lines": ["def mean_square_error_loss(a, b, x, y):\n", "    \"\"\" Returns the mean square error.\n", "    \"\"\"\n", "    # computes a prediction based on a, and b\n", "    y_hat =  lin(a,b,x)\n", "    return np.power(y_hat - y, 2).sum()/y.shape[0]"]}, {"block": 11, "type": "code", "linesLength": 1, "startIndex": 39, "lines": ["mean_square_error_loss(10, 5, x, y)"]}, {"block": 12, "type": "markdown", "linesLength": 1, "startIndex": 40, "lines": ["How do we find the best values for $a$ and $b$? "]}, {"block": 13, "type": "markdown", "linesLength": 1, "startIndex": 41, "lines": ["### Gradient Descent"]}, {"block": 14, "type": "markdown", "linesLength": 1, "startIndex": 42, "lines": ["For a fixed dataset $x$ and $y$ `mean_square_error_loss(a,b)` is a function of $a$ and $b$. We would like to find the values of $a$ and $b$ that minimize that function."]}, {"block": 15, "type": "markdown", "linesLength": 1, "startIndex": 43, "lines": ["**Gradient descent** is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function. This iterative minimization is achieved by taking steps in the negative direction of the function gradient."]}, {"block": 16, "type": "markdown", "linesLength": 1, "startIndex": 44, "lines": ["Here is gradient descent implemented in pytorch"]}, {"block": 17, "type": "code", "linesLength": 2, "startIndex": 45, "lines": ["# generate some more data\n", "x, y = gen_fake_data(1000, 3., 8.)"]}, {"block": 18, "type": "code", "linesLength": 1, "startIndex": 47, "lines": ["dtype = torch.cuda.FloatTensor"]}, {"block": 19, "type": "code", "linesLength": 3, "startIndex": 48, "lines": ["# from numpy to pytorch\n", "x = torch.from_numpy(x)\n", "y = torch.from_numpy(y)"]}, {"block": 20, "type": "code", "linesLength": 6, "startIndex": 51, "lines": ["from torch.autograd import Variable\n", "# Create random Tensors to hold x and y, and wrap them in Variables.\n", "# Setting requires_grad=False indicates that we do not need to compute gradients\n", "# with respect to these Variables.\n", "x = Variable(x.type(dtype), requires_grad=False)\n", "y = Variable(y.type(dtype), requires_grad=False)"]}, {"block": 21, "type": "code", "linesLength": 5, "startIndex": 57, "lines": ["# Create random Tensors for weights a and b, and wrap them in Variables.\n", "# Setting requires_grad=True indicates that we want to compute gradients with\n", "# respect to these Variables.\n", "a = Variable(torch.randn(1).type(dtype), requires_grad=True)\n", "b = Variable(torch.randn(1).type(dtype), requires_grad=True)"]}, {"block": 22, "type": "code", "linesLength": 1, "startIndex": 62, "lines": ["print(a, b)"]}, {"block": 23, "type": "code", "linesLength": 29, "startIndex": 63, "lines": ["learning_rate = 1e-4\n", "for t in range(500):\n", "    # Forward pass: compute predicted y using operations on Variables\n", "    y_pred = x.mul(a) + b\n", "    loss = (y_pred - y).pow(2).sum()\n", "    if t % 100 == 0:\n", "        print(loss.data/n)\n", "    \n", "    # Manually zero the gradients\n", "    if a.grad is not None:\n", "        if a.grad.volatile:\n", "            a.grad.data.zero_()\n", "            b.grad.data.zero_()    \n", "        else:\n", "            data = a.grad.data\n", "            a.grad = Variable(data.new().resize_as_(data).zero_())\n", "            data = b.grad.data\n", "            b.grad = Variable(data.new().resize_as_(data).zero_())\n", "    \n", "    # Computes the gradient of loss with respect to all Variables with requires_grad=True.\n", "    # After this call a.grad and b.grad will be Variables holding the gradient\n", "    # of the loss with respect to a and b respectively.\n", "    loss.backward()\n", "    \n", "    # Update a and b using gradient descent; a.data and b.data are Tensors,\n", "    # a.grad and b.grad are Variables and a.grad.data and b.grad.data are\n", "    # Tensors.\n", "    a.data -= learning_rate * a.grad.data\n", "    b.data -= learning_rate * b.grad.data"]}, {"block": 24, "type": "code", "linesLength": 1, "startIndex": 92, "lines": ["print(a, b)"]}, {"block": 25, "type": "markdown", "linesLength": 1, "startIndex": 93, "lines": ["### Visualizing gradient descent"]}, {"block": 26, "type": "code", "linesLength": 1, "startIndex": 94, "lines": ["x, y = gen_fake_data(50, 3., 8.)"]}, {"block": 27, "type": "code", "linesLength": 3, "startIndex": 95, "lines": ["def sse(y,y_pred): return ((y-y_pred)**2).sum()\n", "def loss(y,a,b,x): return sse(y, lin(a,b,x))\n", "def avg_loss(y,a,b,x): return np.sqrt(loss(y,a,b,x)/n)"]}, {"block": 28, "type": "code", "linesLength": 3, "startIndex": 98, "lines": ["a_guess=-1.\n", "b_guess=1.\n", "avg_loss(y, a_guess, b_guess, x)"]}, {"block": 29, "type": "code", "linesLength": 12, "startIndex": 101, "lines": ["fig = plt.figure(dpi=100, figsize=(5, 4))\n", "plt.scatter(x,y)\n", "line, = plt.plot(x,lin(a_guess,b_guess,x))\n", "plt.close()\n", "\n", "def animate(i):\n", "    line.set_ydata(lin(a_guess,b_guess,x))\n", "    for i in range(30): upd()\n", "    return line,\n", "\n", "ani = animation.FuncAnimation(fig, animate, np.arange(0, 40), interval=100)\n", "ani"]}, {"block": 30, "type": "code", "linesLength": 0, "startIndex": 113, "lines": []}]