[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["## Additional training functions"]}, {"block": 1, "type": "markdown", "linesLength": 6, "startIndex": 1, "lines": ["[`train`](/train.html#train) provides a number of extension methods that are added to [`Learner`](/basic_train.html#Learner) (see below for a list and details), along with three simple callbacks:\n", "\n", "- [`ShowGraph`](/train.html#ShowGraph)\n", "- [`GradientClipping`](/train.html#GradientClipping)\n", "- [`BnFreeze`](/train.html#BnFreeze)\n", "- [`AccumulateScheduler`](/train.html#AccumulateScheduler)"]}, {"block": 2, "type": "code", "linesLength": 3, "startIndex": 7, "lines": ["from fastai.gen_doc.nbdoc import *\n", "from fastai.train import *\n", "from fastai.vision import *\n"]}, {"block": 3, "type": "markdown", "linesLength": 1, "startIndex": 10, "lines": ["## [`Learner`](/basic_train.html#Learner) extension methods"]}, {"block": 4, "type": "markdown", "linesLength": 1, "startIndex": 11, "lines": ["These methods are automatically added to all [`Learner`](/basic_train.html#Learner) objects created after importing this module. They provide convenient access to a number of callbacks, without requiring them to be manually created."]}, {"block": 5, "type": "code", "linesLength": 1, "startIndex": 12, "lines": ["show_doc(fit_one_cycle)"]}, {"block": 6, "type": "code", "linesLength": 1, "startIndex": 13, "lines": ["show_doc(one_cycle_scheduler)"]}, {"block": 7, "type": "markdown", "linesLength": 1, "startIndex": 14, "lines": ["See [`OneCycleScheduler`](/callbacks.one_cycle.html#OneCycleScheduler) for details."]}, {"block": 8, "type": "code", "linesLength": 1, "startIndex": 15, "lines": ["show_doc(lr_find)"]}, {"block": 9, "type": "markdown", "linesLength": 1, "startIndex": 16, "lines": ["See [`LRFinder`](/callbacks.lr_finder.html#LRFinder) for details."]}, {"block": 10, "type": "code", "linesLength": 1, "startIndex": 17, "lines": ["show_doc(to_fp16)"]}, {"block": 11, "type": "markdown", "linesLength": 1, "startIndex": 18, "lines": ["See [`MixedPrecision`](/callbacks.fp16.html#MixedPrecision) for details."]}, {"block": 12, "type": "code", "linesLength": 1, "startIndex": 19, "lines": ["show_doc(to_fp32)"]}, {"block": 13, "type": "code", "linesLength": 1, "startIndex": 20, "lines": ["show_doc(mixup)"]}, {"block": 14, "type": "markdown", "linesLength": 1, "startIndex": 21, "lines": ["See [`MixUpCallback`](/callbacks.mixup.html#MixUpCallback) for more details."]}, {"block": 15, "type": "code", "linesLength": 1, "startIndex": 22, "lines": ["show_doc(ClassificationInterpretation)"]}, {"block": 16, "type": "code", "linesLength": 6, "startIndex": 23, "lines": ["path = untar_data(URLs.MNIST_SAMPLE)\n", "data = ImageDataBunch.from_folder(path)\n", "learn = cnn_learner(data, models.resnet18)\n", "learn.fit(1)\n", "preds,y,losses = learn.get_preds(with_loss=True)\n", "interp = ClassificationInterpretation(learn, preds, y, losses)"]}, {"block": 17, "type": "code", "linesLength": 1, "startIndex": 29, "lines": ["show_doc(ClassificationInterpretation.top_losses)"]}, {"block": 18, "type": "markdown", "linesLength": 1, "startIndex": 30, "lines": ["Returns tuple of *(losses,indices)*."]}, {"block": 19, "type": "code", "linesLength": 1, "startIndex": 31, "lines": ["interp.top_losses(9)"]}, {"block": 20, "type": "code", "linesLength": 1, "startIndex": 32, "lines": ["show_doc(ClassificationInterpretation.plot_confusion_matrix)"]}, {"block": 21, "type": "markdown", "linesLength": 1, "startIndex": 33, "lines": ["If [`normalize`](/vision.data.html#normalize), plots the percentages with `norm_dec` digits. `slice_size` can be used to avoid out of memory error if your set is too big. `kwargs` are passed to `plt.figure`."]}, {"block": 22, "type": "code", "linesLength": 1, "startIndex": 34, "lines": ["interp.plot_confusion_matrix()"]}, {"block": 23, "type": "code", "linesLength": 1, "startIndex": 35, "lines": ["show_doc(ClassificationInterpretation.confusion_matrix)"]}, {"block": 24, "type": "code", "linesLength": 1, "startIndex": 36, "lines": ["interp.confusion_matrix()"]}, {"block": 25, "type": "code", "linesLength": 1, "startIndex": 37, "lines": ["show_doc(ClassificationInterpretation.most_confused)"]}, {"block": 26, "type": "markdown", "linesLength": 1, "startIndex": 38, "lines": ["#### Working with large datasets"]}, {"block": 27, "type": "markdown", "linesLength": 5, "startIndex": 39, "lines": ["When working with large datasets, memory problems can arise when computing the confusion matrix. For example, an error can look like this:\n", "\n", "    RuntimeError: $ Torch: not enough memory: you tried to allocate 64GB. Buy new RAM!\n", "\n", "In this case it is possible to force [`ClassificationInterpretation`](/train.html#ClassificationInterpretation) to compute the confusion matrix for data slices and then aggregate the result by specifying slice_size parameter. "]}, {"block": 28, "type": "code", "linesLength": 1, "startIndex": 44, "lines": ["interp.confusion_matrix(slice_size=10)"]}, {"block": 29, "type": "code", "linesLength": 1, "startIndex": 45, "lines": ["interp.plot_confusion_matrix(slice_size=10)"]}, {"block": 30, "type": "code", "linesLength": 1, "startIndex": 46, "lines": ["interp.most_confused(slice_size=10)"]}, {"block": 31, "type": "markdown", "linesLength": 1, "startIndex": 47, "lines": ["## Additional callbacks"]}, {"block": 32, "type": "markdown", "linesLength": 1, "startIndex": 48, "lines": ["We'll show examples below using our MNIST sample. As usual the `on_something` methods are directly called by the fastai library, no need to call them yourself."]}, {"block": 33, "type": "code", "linesLength": 2, "startIndex": 49, "lines": ["path = untar_data(URLs.MNIST_SAMPLE)\n", "data = ImageDataBunch.from_folder(path)"]}, {"block": 34, "type": "code", "linesLength": 1, "startIndex": 51, "lines": ["show_doc(ShowGraph, title_level=3)"]}, {"block": 35, "type": "markdown", "linesLength": 4, "startIndex": 52, "lines": ["```python\n", "learn = cnn_learner(data, models.resnet18, metrics=accuracy, callback_fns=ShowGraph)\n", "learn.fit(3)\n", "```"]}, {"block": 36, "type": "markdown", "linesLength": 1, "startIndex": 56, "lines": ["![Training graph](imgs/train_graph.gif)"]}, {"block": 37, "type": "code", "linesLength": 1, "startIndex": 57, "lines": ["show_doc(ShowGraph.on_epoch_end)"]}, {"block": 38, "type": "code", "linesLength": 1, "startIndex": 58, "lines": ["show_doc(GradientClipping)"]}, {"block": 39, "type": "code", "linesLength": 3, "startIndex": 59, "lines": ["learn = cnn_learner(data, models.resnet18, metrics=accuracy,\n", "    callback_fns=partial(GradientClipping, clip=0.1))\n", "learn.fit(1)"]}, {"block": 40, "type": "code", "linesLength": 1, "startIndex": 62, "lines": ["show_doc(GradientClipping.on_backward_end)"]}, {"block": 41, "type": "code", "linesLength": 1, "startIndex": 63, "lines": ["show_doc(BnFreeze)"]}, {"block": 42, "type": "markdown", "linesLength": 1, "startIndex": 64, "lines": ["For batchnorm layers where `requires_grad==False`, you generally don't want to update their moving average statistics, in order to avoid the model's statistics getting out of sync with its pre-trained weights. You can add this callback to automate this freezing of statistics (internally, it calls `eval` on these layers)."]}, {"block": 43, "type": "code", "linesLength": 2, "startIndex": 65, "lines": ["learn = cnn_learner(data, models.resnet18, metrics=accuracy, callback_fns=BnFreeze)\n", "learn.fit(1)"]}, {"block": 44, "type": "code", "linesLength": 1, "startIndex": 67, "lines": ["show_doc(BnFreeze.on_epoch_begin)"]}, {"block": 45, "type": "code", "linesLength": 1, "startIndex": 68, "lines": ["show_doc(AccumulateScheduler)"]}, {"block": 46, "type": "markdown", "linesLength": 17, "startIndex": 69, "lines": ["Let's force `batch_size=2` to mimic a scenario where we can't fit enough batch samples to our memory. We can then set `n_step` as desired to have an effective batch_size of `effective_batch_size=batch_size*n_step`.\n", "\n", "It is also important to use loss func with `reduce='sum'` in order to calculate exact average accumulated gradients.\n", "\n", "Another important note for users is that `batchnorm` is not yet adapted to accumulated gradients. So you should use this callback at your own risk until a hero fixes it :)\n", "\n", "Here we demonstrate this callback with a model without `batchnorm` layers, alternatively you can use `nn.InstanceNorm` or [`nn.GroupNorm`](https://pytorch.org/docs/stable/nn.html#torch.nn.GroupNorm).\n", "\n", "```\n", "from torchvision.models import vgg11\n", "\n", "data = ImageDataBunch.from_folder(path, bs=2)\n", "\n", "learn = cnn_learner(data, resnet18, metrics=accuracy, loss_func=CrossEntropyFlat(reduction='sum'),\n", "                    callback_fns=partial(AccumulateScheduler, n_step=16))\n", "learn.fit(1)\n", "```"]}, {"block": 47, "type": "markdown", "linesLength": 1, "startIndex": 86, "lines": ["## Undocumented Methods - Methods moved below this line will intentionally be hidden"]}, {"block": 48, "type": "markdown", "linesLength": 1, "startIndex": 87, "lines": ["## New Methods - Please document or move to the undocumented section"]}, {"block": 49, "type": "code", "linesLength": 1, "startIndex": 88, "lines": ["show_doc(ClassificationInterpretation.plot_top_losses)"]}, {"block": 50, "type": "markdown", "linesLength": 0, "startIndex": 89, "lines": []}, {"block": 51, "type": "code", "linesLength": 1, "startIndex": 89, "lines": ["show_doc(ClassificationInterpretation.from_learner)"]}, {"block": 52, "type": "markdown", "linesLength": 0, "startIndex": 90, "lines": []}, {"block": 53, "type": "code", "linesLength": 1, "startIndex": 90, "lines": ["show_doc(ClassificationInterpretation.top_losses)"]}, {"block": 54, "type": "markdown", "linesLength": 0, "startIndex": 91, "lines": []}, {"block": 55, "type": "code", "linesLength": 1, "startIndex": 91, "lines": ["show_doc(ClassificationInterpretation.confusion_matrix)"]}, {"block": 56, "type": "markdown", "linesLength": 0, "startIndex": 92, "lines": []}, {"block": 57, "type": "code", "linesLength": 1, "startIndex": 92, "lines": ["show_doc(ClassificationInterpretation.most_confused)"]}, {"block": 58, "type": "markdown", "linesLength": 0, "startIndex": 93, "lines": []}, {"block": 59, "type": "code", "linesLength": 1, "startIndex": 93, "lines": ["show_doc(ClassificationInterpretation.plot_confusion_matrix)"]}, {"block": 60, "type": "markdown", "linesLength": 0, "startIndex": 94, "lines": []}, {"block": 61, "type": "code", "linesLength": 1, "startIndex": 94, "lines": ["show_doc(ClassificationInterpretation.plot_multi_top_losses)"]}, {"block": 62, "type": "markdown", "linesLength": 0, "startIndex": 95, "lines": []}]