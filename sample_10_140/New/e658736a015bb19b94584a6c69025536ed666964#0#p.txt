[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["# Spark Collaborative Filtering (ALS) Deep Dive"]}, {"block": 1, "type": "markdown", "linesLength": 3, "startIndex": 1, "lines": ["Spark MLlib provides collaborative filtering algorithm that can be used for training a matrix factorization model, which predicts explicit or implicit ratings of users on items, for recommendations.\n", "\n", "This notebook presents a deep dive into Spark collaborative filtering algorithm."]}, {"block": 2, "type": "markdown", "linesLength": 1, "startIndex": 4, "lines": ["## 0 Global settings\n"]}, {"block": 3, "type": "code", "linesLength": 16, "startIndex": 5, "lines": ["%matplotlib inline\n", "\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "import numpy as np\n", "import sys\n", "from itertools import product\n", "\n", "from reco_utils.common.spark_utils import start_or_get_spark\n", "from reco_utils.evaluation.spark_evaluation import SparkRankingEvaluation, SparkRatingEvaluation\n", "from reco_utils.dataset.url_utils import maybe_download\n", "from reco_utils.dataset.spark_splitters import spark_random_split\n", "\n", "from pyspark.ml.recommendation import ALS\n", "from pyspark.sql.functions import col\n", "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"]}, {"block": 4, "type": "code", "linesLength": 2, "startIndex": 21, "lines": ["print(\"System version: {}\".format(sys.version))\n", "print(\"Pandas version: {}\".format(pd.__version__))"]}, {"block": 5, "type": "markdown", "linesLength": 1, "startIndex": 23, "lines": ["Data URL and path"]}, {"block": 6, "type": "code", "linesLength": 2, "startIndex": 24, "lines": ["DATA_URL = \"http://files.grouplens.org/datasets/movielens/ml-100k/u.data\"\n", "DATA_PATH = \"ml-100k.data\""]}, {"block": 7, "type": "markdown", "linesLength": 1, "startIndex": 26, "lines": ["Data column names"]}, {"block": 8, "type": "code", "linesLength": 12, "startIndex": 27, "lines": ["COL_USER = \"userID\"\n", "COL_ITEM = \"itemID\"\n", "COL_RATING = \"rating\"\n", "COL_PREDICTION = \"prediction\"\n", "COL_TIMESTAMP = \"timestamp\"\n", "\n", "HEADER = {\n", "    \"col_user\": COL_USER,\n", "    \"col_item\": COL_ITEM,\n", "    \"col_rating\": COL_RATING,\n", "    \"col_prediction\": COL_PREDICTION,\n", "}"]}, {"block": 9, "type": "markdown", "linesLength": 1, "startIndex": 39, "lines": ["Model hyper parameters"]}, {"block": 10, "type": "code", "linesLength": 3, "startIndex": 40, "lines": ["RANK = 10\n", "MAX_ITER = 5\n", "REG_PARAM = 0.01"]}, {"block": 11, "type": "markdown", "linesLength": 1, "startIndex": 43, "lines": ["Number of recommended items"]}, {"block": 12, "type": "code", "linesLength": 1, "startIndex": 44, "lines": ["K = 10"]}, {"block": 13, "type": "markdown", "linesLength": 25, "startIndex": 45, "lines": ["## 1 Matrix factorization algorithm\n", "\n", "### 1.1 Matrix factorization for collaborative filtering problem\n", "\n", "Matrix factorization is a common technique used in recommendation tasks. Basically, a matrix factorization algorithm tries to find latent factors that represent intrinsic user and item attributes in a lower dimension. That is,\n", "\n", "$$\\hat r_{u,i} = q_{i}^{T}p_{u}$$\n", "\n", "where $\\hat r_{u,i}$ is the predicted ratings for user $u$ and item $i$, and $q_{i}^{T}$ and $p_{u}$ are latent factors for item and user, respectively. The challenge to the matrix factorization problem is to find $q_{i}^{T}$ and $p_{u}$. This is achieved by methods such as matrix decomposition. A learning approach is therefore developed to converge the decomposition results close to the observed ratings as much as possible. Furthermore, to avoid overfitting issue, the learning process is regularized. For example, a basic form of such matrix factorization algorithm is represented as below.\n", "\n", "$$\\min\\sum(\\hat r_{u,i} - q_{i}^{T}p_{u})^2 + \\lambda(||q_{i}||^2 + ||p_{u}||^2)$$\n", "\n", "where $\\lambda$ is a the regularization parameter. \n", "\n", "In case explict ratings are not available, implicit ratings which are usually derived from users' historical interactions with the items (e.g., clicks, views, purchases, etc.). To account for such implicity ratings, the original matrix factorization algorithm can be formulated as \n", "\n", "$$\\min\\sum c_{u,i}(p_{u,i} - q_{i}^{T}p_{u})^2 + \\lambda(||q_{i}||^2 + ||p_{u}||^2)$$\n", "\n", "where $c_{u,i}=1+\\alpha r_{u,i}$ and $p_{u,i}=1$ if $r_{u,i}>0$ and $p_{u,i}=0$ if $r_{u,i}=0$. $r_{u,i}$ is a numerical representation of users' preferences (e.g., number of cliks, etc.). \n", "\n", "### 1.2 Alternating Least Square (ALS)\n", "\n", "Owing to the term of $q_{i}^{T}p_{u}$ the loss function is non-convex. Gradient descent method can be applied but this will incur expensive computations. Alternating Least Square (ALS) algorithm is therefore developed to overcome the issue. \n", "\n", "The basic idea of ALS is to fix one of $q$ and $p$ at a time for optimization while keeping the other as constant. This makes the objective convext and solvable. The alternating between $q$ and $p$ stops when there is a convergence to the optimal. It is worth noting that this iterative computation can be parallelised and/or distributed, which makes the algorithm desirable for use cases where dataset is large and thus the user-item rating matrix is super sparse (this is usually observed in recommendation system scenario). A comprehensive discussion of ALS and its distributed computation can be found [here](http://stanford.edu/~rezab/classes/cme323/S15/notes/lec14.pdf)."]}, {"block": 14, "type": "markdown", "linesLength": 7, "startIndex": 70, "lines": ["## 2 Spark Mllib implementation\n", "\n", "The matrix factorization algorithm is available as `ALS` module in [Spark `ml`](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html) for DataFrame or [Spark `mllib`](https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html) for RDD. \n", "\n", "* The uniqueness of ALS implementation is that it distributes and parallelizes the matrix factorization model training by using \"Alternating Least Square\" method. \n", "* In the training method, there are parameters that can be selected to control the model performance.\n", "* Both explicit and implicit ratings are supported by Spark ALS model."]}, {"block": 15, "type": "markdown", "linesLength": 5, "startIndex": 77, "lines": ["## 3 Spark ALS based Movielens recommender\n", "\n", "In the following codes, Movielens-100K dataset is used to illustrate the ALS algorithm in Spark.\n", "\n", "First, a Spark session is initialized."]}, {"block": 16, "type": "code", "linesLength": 1, "startIndex": 82, "lines": ["spark = start_or_get_spark(\"Spark ALS\", \"local\")"]}, {"block": 17, "type": "markdown", "linesLength": 1, "startIndex": 83, "lines": ["### 3.1 Load and prepare data"]}, {"block": 18, "type": "markdown", "linesLength": 1, "startIndex": 84, "lines": ["Data is read from csv into a Spark DataFrame."]}, {"block": 19, "type": "code", "linesLength": 10, "startIndex": 85, "lines": ["filepath = maybe_download(DATA_URL, DATA_PATH)\n", "\n", "dfs = spark.read.csv(\n", "    filepath, \n", "    inferSchema=True, \n", "    sep=\"\\t\", \n", ") \\\n", ".withColumnRenamed(\"_c0\", COL_USER).withColumnRenamed(\"_c1\", COL_ITEM) \\\n", ".withColumnRenamed(\"_c2\", COL_RATING) \\\n", ".withColumnRenamed(\"_c3\", COL_TIMESTAMP)"]}, {"block": 20, "type": "code", "linesLength": 1, "startIndex": 95, "lines": ["dfs.show(5)"]}, {"block": 21, "type": "markdown", "linesLength": 1, "startIndex": 96, "lines": ["Data is then randomly split to 80-20 halfs for training and testing."]}, {"block": 22, "type": "code", "linesLength": 1, "startIndex": 97, "lines": ["dfs_train, dfs_test = spark_random_split(dfs, ratio=0.8)"]}, {"block": 23, "type": "code", "linesLength": 2, "startIndex": 98, "lines": ["dfs_train = dfs_train.select(COL_USER, COL_ITEM, COL_RATING)\n", "dfs_test = dfs_test.select(COL_USER, COL_ITEM, COL_RATING).withColumn(COL_RATING, col(COL_RATING).cast(\"float\"))"]}, {"block": 24, "type": "markdown", "linesLength": 1, "startIndex": 100, "lines": ["### 3.2 Train a movielens model "]}, {"block": 25, "type": "markdown", "linesLength": 1, "startIndex": 101, "lines": ["It is worth noting that Spark ALS model allows dropping cold users to favor a robust evaluation with the testing data. In case there are cold users, Spark ALS implementation allows users to drop cold users in order to make sure evaluations on the prediction results are sound."]}, {"block": 26, "type": "code", "linesLength": 11, "startIndex": 102, "lines": ["als = ALS(\n", "    maxIter=MAX_ITER, \n", "    rank=RANK,\n", "    regParam=REG_PARAM, \n", "    userCol=COL_USER, \n", "    itemCol=COL_ITEM, \n", "    ratingCol=COL_RATING, \n", "    coldStartStrategy=\"drop\"\n", ")\n", "\n", "model = als.fit(dfs_train)"]}, {"block": 27, "type": "markdown", "linesLength": 3, "startIndex": 113, "lines": ["### 3.3 Prediction with the model\n", "\n", "The trained model can be used to predict ratings with a given test data."]}, {"block": 28, "type": "code", "linesLength": 1, "startIndex": 116, "lines": ["dfs_pred = model.transform(dfs_test).drop(COL_RATING)"]}, {"block": 29, "type": "markdown", "linesLength": 1, "startIndex": 117, "lines": ["With the prediction results, the model performance can be evaluated."]}, {"block": 30, "type": "code", "linesLength": 16, "startIndex": 118, "lines": ["evaluations = SparkRatingEvaluation(\n", "    dfs_test, \n", "    dfs_pred,\n", "    col_user=COL_USER,\n", "    col_item=COL_ITEM,\n", "    col_rating=COL_RATING,\n", "    col_prediction=COL_PREDICTION\n", ")\n", "\n", "print(\n", "    \"RMSE score = {}\".format(evaluations.rmse()),\n", "    \"MAE score = {}\".format(evaluations.mae()),\n", "    \"R2 score = {}\".format(evaluations.rsquared()),\n", "    \"Explained variance score = {}\".format(evaluations.exp_var()),\n", "    sep=\"\\n\"\n", ")"]}, {"block": 31, "type": "markdown", "linesLength": 1, "startIndex": 134, "lines": ["Sometimes ranking metrics are of interest to data scientists."]}, {"block": 32, "type": "code", "linesLength": 17, "startIndex": 135, "lines": ["evaluations = SparkRankingEvaluation(\n", "    dfs_test, \n", "    dfs_pred,\n", "    col_user=COL_USER,\n", "    col_item=COL_ITEM,\n", "    col_rating=COL_RATING,\n", "    col_prediction=COL_PREDICTION,\n", "    k=K\n", ")\n", "\n", "print(\n", "    \"Precision@k = {}\".format(evaluations.precision_at_k()),\n", "    \"Recall@k = {}\".format(evaluations.recall_at_k()),\n", "    \"NDCG@k = {}\".format(evaluations.ndcg_at_k()),\n", "    \"Mean average precision = {}\".format(evaluations.map_at_k()),\n", "    sep=\"\\n\"\n", ")"]}, {"block": 33, "type": "markdown", "linesLength": 11, "startIndex": 152, "lines": ["### 3.4 Fine tune the model\n", "\n", "Prediction performance of a Spark ALS model is often affected by the parameters of \n", "\n", "|Parameter|Description|Default value|Notes|\n", "|-------------|-----------------|------------------|-----------------|\n", "|`rank`|Number of latent factors|10|The larger the more intrinsic factors considered in the factorization modeling.|\n", "|`regParam`|Regularization parameter|1.0|The value needs to be selected empirically to avoid overfitting.|\n", "|`maxIters`|Maximum number of iterations|10|The more iterations the better the model converges to the optimal point.|\n", "\n", "It is always a good practice to start model building with default parameter values and then sweep the parameter in a range to find optimal set of parameter combination. For illustration purpose, the following parameter set is used for training ALS models for comparison study purpose."]}, {"block": 34, "type": "code", "linesLength": 4, "startIndex": 163, "lines": ["param_dict = {\n", "    \"rank\": [10, 15, 20],\n", "    \"regParam\": [0.001, 0.1, 1.0]\n", "}"]}, {"block": 35, "type": "markdown", "linesLength": 1, "startIndex": 167, "lines": ["To generate dictionary for each parameter combination which can then be fed into model training."]}, {"block": 36, "type": "code", "linesLength": 24, "startIndex": 168, "lines": ["def get_grid_search(params):\n", "    '''\n", "    Grid search of parameters, i.e., all possible permutations.\n", "    '''\n", "    param_new = {}\n", "    param_fixed = {}\n", "    for key, value in params.items():\n", "        if isinstance(value, list):\n", "            param_new[key] = value\n", "        else:\n", "            param_fixed[key] = value\n", "\n", "    items = sorted(param_new.items())\n", "    keys, values = zip(*items)\n", "\n", "    params_exp = []\n", "    for v in product(*values):\n", "        param_exp = dict(zip(keys, v))\n", "        param_exp.update(param_fixed)\n", "        params_exp.append(param_exp)\n", "\n", "    return params_exp\n", "\n", "param_grid = get_grid_search(param_dict)"]}, {"block": 37, "type": "markdown", "linesLength": 1, "startIndex": 192, "lines": ["Train models with parameters specified in the parameter grid. Evaluate the model with, for example, the RMSE metric, and then record the metrics for visualization."]}, {"block": 38, "type": "code", "linesLength": 28, "startIndex": 193, "lines": ["rmse_score = []\n", "\n", "for g in param_grid:\n", "    als = ALS(\n", "        **g,\n", "        userCol=COL_USER, \n", "        itemCol=COL_ITEM, \n", "        ratingCol=COL_RATING, \n", "        coldStartStrategy=\"drop\"\n", "    )\n", "    \n", "    model = als.fit(dfs_train)\n", "    \n", "    dfs_pred = model.transform(dfs_test).drop(COL_RATING)\n", "    \n", "    evaluations = SparkRatingEvaluation(\n", "        dfs_test, \n", "        dfs_pred,\n", "        col_user=COL_USER,\n", "        col_item=COL_ITEM,\n", "        col_rating=COL_RATING,\n", "        col_prediction=COL_PREDICTION\n", "    )\n", "\n", "    rmse_score.append(evaluations.rmse())\n", "\n", "rmse_score = [float('%.4f' % x) for x in rmse_score]\n", "rmse_score_array = np.reshape(rmse_score, (-1, 3))"]}, {"block": 39, "type": "markdown", "linesLength": 1, "startIndex": 221, "lines": ["The calculated RMSE scores can be visualized to comparatively study how model performance is affected by different parameters."]}, {"block": 40, "type": "code", "linesLength": 19, "startIndex": 222, "lines": ["def plot_heatmap_gridsearch(rmse_score_array):\n", "    fig, ax = plt.subplots()\n", "    im = ax.imshow(rmse_score_array)\n", "\n", "    ax.set_xticks(np.arange(len(param_dict[\"rank\"])))\n", "    ax.set_yticks(np.arange(len(param_dict[\"regParam\"])))\n", "\n", "    ax.set_xticklabels(param_dict[\"rank\"])\n", "    ax.set_yticklabels(param_dict[\"regParam\"])\n", "\n", "    for i in range(len(param_dict[\"rank\"])):\n", "        for j in range(len(param_dict[\"regParam\"])):\n", "            text = ax.text(i, j, rmse_score_array[i, j],\n", "                           ha=\"center\", va=\"center\", color=\"w\")\n", "\n", "    fig.tight_layout()\n", "    plt.show()\n", "    \n", "plot_heatmap_gridsearch(rmse_score_array)"]}, {"block": 41, "type": "markdown", "linesLength": 1, "startIndex": 241, "lines": ["It is noted from the visualization that the RMSE does not decrease together with increase of `rank`, which may be owing to the reason of overfitting. When `rank` is 10 and `regParam` is 0.1, the lowest RMSE score is achieved, which indicates that the model is optimal."]}, {"block": 42, "type": "markdown", "linesLength": 1, "startIndex": 242, "lines": ["### 3.5 Top K recommendation"]}, {"block": 43, "type": "markdown", "linesLength": 1, "startIndex": 243, "lines": ["#### 3.5.1 Top k for all users (items)"]}, {"block": 44, "type": "code", "linesLength": 1, "startIndex": 244, "lines": ["dfs_rec = model.recommendForAllUsers(10)"]}, {"block": 45, "type": "code", "linesLength": 1, "startIndex": 245, "lines": ["dfs_rec.show(10)"]}, {"block": 46, "type": "markdown", "linesLength": 1, "startIndex": 246, "lines": ["#### 3.5.2 Top k for a selected set of users (items)"]}, {"block": 47, "type": "code", "linesLength": 3, "startIndex": 247, "lines": ["users = dfs_train.select(als.getUserCol()).distinct().limit(3)\n", "\n", "dfs_rec_subset = model.recommendForUserSubset(users, 10)"]}, {"block": 48, "type": "code", "linesLength": 1, "startIndex": 250, "lines": ["dfs_rec_subset.show(10)"]}, {"block": 49, "type": "markdown", "linesLength": 7, "startIndex": 251, "lines": ["#### 3.5.3 Run-time consideration for top-k recommendation\n", "\n", "It is worth noting that usually computing the top-k recommendations for all users is the bottleneck of the whole pipeline (model training and scoring) of an ALS based recommendation system. This is because\n", "* Getting top k from all user-item pairs requires a cross join which is usually very computationally expensive. \n", "* Inner-product of user-item pairs are calculated individually instead of leveraging matrix block multiplication features which are available in certain contemporary computing acceleration libraries (e.g., BLAS).\n", "\n", "More details about the deficiencies of the native implementations of the top k recommendations in Spark can be found [here](https://engineeringblog.yelp.com/2018/05/scaling-collaborative-filtering-with-pyspark.html)."]}, {"block": 50, "type": "markdown", "linesLength": 1, "startIndex": 258, "lines": ["## References"]}, {"block": 51, "type": "markdown", "linesLength": 8, "startIndex": 259, "lines": ["1. Yehuda Koren, Robert Bell, and Chris Volinsky, \"Matrix Factorization Techniques for Recommender Systems\n", "\", ACM Computer, Vol. 42, Issue 8, pp 30-37, Aug., 2009.\n", "2. Yifan Hu, Yehuda Koren, and Chris Volinsky, \"Collaborative Filtering for Implicit Feedback Datasets\n", "\", Proc. IEEE ICDM, 2008, Dec, Pisa, Italy.\n", "3. Apache Spark. url: https://spark.apache.org/docs/latest/ml-collaborative-filtering.html\n", "4. matplotlib. url: https://matplotlib.org/gallery/images_contours_and_fields/image_annotated_heatmap.html\n", "5. Scaling collaborative filtering with PySpark. url: https://engineeringblog.yelp.com/2018/05/scaling-collaborative-filtering-with-pyspark.html\n", "6. Matrix Completion via Alternating Least Square (ALS). url: http://stanford.edu/~rezab/classes/cme323/S15/notes/lec14.pdf"]}]