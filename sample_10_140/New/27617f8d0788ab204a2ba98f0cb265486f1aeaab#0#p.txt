[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["# Data transformation (collaborative filtering)"]}, {"block": 1, "type": "markdown", "linesLength": 3, "startIndex": 1, "lines": ["It is usually observed in the real-world datasets that users may have different types of interactions with items. These interactions may also appear more than one times in the history. Given that this is a typical problem in practical recommendation system design, the notebook shares data transformation techniques that can be used for different scenarios.\n", "\n", "The discussion in this notebooks is only applicable to the collaborative-filtering typed algorithms"]}, {"block": 2, "type": "markdown", "linesLength": 1, "startIndex": 4, "lines": ["## 0 Global settings"]}, {"block": 3, "type": "code", "linesLength": 26, "startIndex": 5, "lines": ["# set the environment path to find Recommenders\n", "import sys\n", "sys.path.append(\"../../\")\n", "\n", "import pyspark\n", "import pandas as pd\n", "import numpy as np\n", "import datetime\n", "import math\n", "\n", "from reco_utils.common.spark_utils import start_or_get_spark\n", "from reco_utils.dataset.url_utils import maybe_download\n", "from reco_utils.dataset.python_splitters import (\n", "    python_random_split, \n", "    python_chrono_split, \n", "    python_stratified_split\n", ")\n", "from reco_utils.dataset.spark_splitters import (\n", "    spark_random_split, \n", "    spark_chrono_split, \n", "    spark_stratified_split,\n", "    spark_timestamp_split\n", ")\n", "\n", "print(\"System version: {}\".format(sys.version))\n", "print(\"Pyspark version: {}\".format(pyspark.__version__))"]}, {"block": 4, "type": "markdown", "linesLength": 1, "startIndex": 31, "lines": ["## 1 Data creation"]}, {"block": 5, "type": "markdown", "linesLength": 1, "startIndex": 32, "lines": ["To facilitate the illustration in the notebook, several dummy datasets are created. "]}, {"block": 6, "type": "markdown", "linesLength": 11, "startIndex": 33, "lines": ["### 1.1 Explicit feedbacks\n", "\n", "Interactions between the users and the items are **ratings** or user preferences such as **like** or **dislike**. These types of interactions are termed as *explicit feedbacks* of users on the items they interact with.\n", "\n", "The following is the code to generate a dataset with explicit feedbacks.\n", "\n", "In the data,\n", "* there are 3 users whose IDs are 1, 2, 3.\n", "* there are 3 items whose IDs are 1, 2, 3.\n", "* Items are rated by users for only once.\n", "* timestamps of when the ratings are given are also recorded."]}, {"block": 7, "type": "code", "linesLength": 10, "startIndex": 44, "lines": ["data1 = pd.DataFrame({\n", "    \"UserId\": [1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3],\n", "    \"ItemId\": [1, 1, 2, 2, 2, 1, 2, 1, 2, 3, 3, 3, 3, 3, 1],\n", "    \"Rating\": [4, 4, 3, 3, 3, 4, 5, 4, 5, 5, 5, 5, 5, 5, 4],\n", "    \"Timestamp\": [\n", "        '2000-01-01', '2000-01-01', '2000-01-02', '2000-01-02', '2000-01-02',\n", "        '2000-01-01', '2000-01-01', '2000-01-03', '2000-01-03', '2000-01-03',\n", "        '2000-01-01', '2000-01-03', '2000-01-03', '2000-01-03', '2000-01-04'\n", "    ]\n", "})"]}, {"block": 8, "type": "code", "linesLength": 1, "startIndex": 54, "lines": ["data1"]}, {"block": 9, "type": "markdown", "linesLength": 11, "startIndex": 55, "lines": ["### 1.2 Implicit feedbacks\n", "\n", "Many times there are no explicit ratings or preferences given by users. The interactions are usually implicit. For example, a user may puchase something on a website, click an item in an mobile app, or order food from a restaurant. This information may demonstrate users' preference towards the users, but the preference is reflected from the data in a **implicit** manner. \n", "\n", "Another data is created to illustrate the implicit feedbacks. \n", "\n", "In the data,\n", "* there are 3 users whose IDs are 1, 2, 3.\n", "* there are 3 items whose IDs are 1, 2, 3.\n", "* There are no ratings or explicit feedbacks given by the users. Sometimes there may be types of events. In this dummy dataset, for illustration purpose, there are three types for the interactions between users and items, that is, **click**, **add**, and **purchase**, meaning \"click on the item\", \"add the item into cart\", and \"purchase the item\", respectively.\n", "* timestamps of when the ratings are given are also recorded."]}, {"block": 10, "type": "code", "linesLength": 14, "startIndex": 66, "lines": ["data2 = pd.DataFrame({\n", "    \"UserId\": [1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3],\n", "    \"ItemId\": [1, 1, 2, 2, 2, 1, 2, 1, 2, 3, 3, 3, 3, 3, 1],\n", "    \"Type\": [\n", "        'click', 'click', 'click', 'click', 'purchase',\n", "        'click', 'purchase', 'add', 'purchase', 'purchase',\n", "        'click', 'click', 'add', 'purchase', 'click'\n", "    ],\n", "    \"Timestamp\": [\n", "        '2000-01-01', '2000-01-01', '2000-01-02', '2000-01-02', '2000-01-02',\n", "        '2000-01-01', '2000-01-01', '2000-01-03', '2000-01-03', '2000-01-03',\n", "        '2000-01-01', '2000-01-03', '2000-01-03', '2000-01-03', '2000-01-04'\n", "    ]\n", "})"]}, {"block": 11, "type": "code", "linesLength": 1, "startIndex": 80, "lines": ["data2"]}, {"block": 12, "type": "markdown", "linesLength": 1, "startIndex": 81, "lines": ["## 2 Explicit and implicit feedbacks"]}, {"block": 13, "type": "markdown", "linesLength": 3, "startIndex": 82, "lines": ["Many collaborative filtering algorithms are built on a user-item sparse matrix, thus unique user-item pairs are required in such settings. \n", "\n", "For explicit feedback datasets, this can simply be done by deduplicating the repeated user-item-rating tuples. For example, for `data`, this can be simply done as follows"]}, {"block": 14, "type": "code", "linesLength": 1, "startIndex": 85, "lines": ["data1 = data1.drop_duplicates()"]}, {"block": 15, "type": "code", "linesLength": 1, "startIndex": 86, "lines": ["data1"]}, {"block": 16, "type": "markdown", "linesLength": 1, "startIndex": 87, "lines": ["In the implicit feedback use cases, there are several methods to perform the deduplication, depending on the requirements of the actual business needs."]}, {"block": 17, "type": "markdown", "linesLength": 5, "startIndex": 88, "lines": ["### 2.1 Data aggregation\n", "\n", "Usually, data is aggregated by user to generate some scores that represent preferences (in some algorithms like SAR, the score is called *affinity score*, for simplicity reason, hereafter the scores are termed as *affinity*).\n", "\n", "It is worth mentioning that in such case, the affinity scores are different from the ratings in the explicit data set, in terms of value distribution. This is usually termed as an [ordinal regression](https://en.wikipedia.org/wiki/Ordinal_regression) problem, which has been studied in [Koren's paper](https://pdfs.semanticscholar.org/934a/729409d6fbd9894a94d4af66bd82222b5515.pdf). In this case, "]}, {"block": 18, "type": "markdown", "linesLength": 3, "startIndex": 93, "lines": ["#### 2.2.1 Count\n", "\n", "The most simple technique is to count times of interactions between user and item for producing affinity scores. The following shows the aggregation of counts of user-item interactions in `data2` regardless the interaction type."]}, {"block": 19, "type": "code", "linesLength": 2, "startIndex": 96, "lines": ["data2_count = data2.groupby(['UserId', 'ItemId']).agg({'Timestamp': 'count'}).reset_index()\n", "data2_count.columns = ['UserId', 'ItemId', 'Affinity']"]}, {"block": 20, "type": "code", "linesLength": 1, "startIndex": 98, "lines": ["data2_count"]}, {"block": 21, "type": "markdown", "linesLength": 1, "startIndex": 99, "lines": ["#### 2.2.1 Weighted count"]}, {"block": 22, "type": "markdown", "linesLength": 1, "startIndex": 100, "lines": ["It is useful to consider the types of different interactions as weights in the count aggregation. For example, assuming weights of the three differen types, \"click\", \"add\", and \"purchase\", are 1, 2, and 3, respectively. A weighted-count can be done as the following"]}, {"block": 23, "type": "code", "linesLength": 15, "startIndex": 101, "lines": ["# Add column of weights\n", "data2_w = data2.copy()\n", "\n", "conditions = [\n", "    data2_w['Type'] == 'click',\n", "    data2_w['Type'] == 'add',\n", "    data2_w['Type'] == 'purchase'\n", "]\n", "\n", "choices = [1, 2, 3]\n", "\n", "data2_w['Weight'] = np.select(conditions, choices, default='black')\n", "\n", "# Convert to numeric type.\n", "data2_w['Weight'] = pd.to_numeric(data2_w['Weight'])"]}, {"block": 24, "type": "code", "linesLength": 3, "startIndex": 116, "lines": ["# Do count with weight.\n", "data2_wcount = data2_w.groupby(['UserId', 'ItemId'])['Weight'].sum().reset_index()\n", "data2_wcount.columns = ['UserId', 'ItemId', 'Affinity']"]}, {"block": 25, "type": "code", "linesLength": 1, "startIndex": 119, "lines": ["data2_wcount"]}, {"block": 26, "type": "markdown", "linesLength": 1, "startIndex": 120, "lines": ["#### 2.2.2 Time dependent count"]}, {"block": 27, "type": "markdown", "linesLength": 6, "startIndex": 121, "lines": ["In many scenarios, time dependency plays a critical role in preparing dataset for building a collaborative filtering model that captures user interests drift over time. One of the common techniques for achieving time dependent count is to add a time decay factor in the counting. This technique is used in [SAR](https://github.com/Microsoft/Recommenders/blob/master/notebooks/02_model/sar_single_node_deep_dive.ipynb). Formula for getting affinity score for each user-item pair is \n", "\n", "$$a_{ij}=\\sum_k (w_k \\text{exp}[-\\text{log}_2(\\frac{t_0-t_k}{T})] $$\n", "where $a_{ij}$ is the affinity score, $w_k$ is the interaction weight, $t_0$ is a reference time, $t_k$ is the timestamp for the $k$-th interaction, and $T$ is a hyperparameter that controls the speed of decay.\n", "\n", "The following shows how SAR applies time decay in aggregating counts for the implicit feedback scenario. "]}, {"block": 28, "type": "markdown", "linesLength": 1, "startIndex": 127, "lines": ["In this case, we use 30 days as the half-life parameter, and use the current time as the time reference."]}, {"block": 29, "type": "code", "linesLength": 3, "startIndex": 128, "lines": ["T = 30\n", "\n", "t_ref = datetime.datetime.today()"]}, {"block": 30, "type": "code", "linesLength": 6, "startIndex": 131, "lines": ["# Calculate the weighted count with time decay.\n", "\n", "data2_w['Timedecay'] = data2_w.apply(\n", "    lambda x: x['Weight'] * math.exp(-math.log2((t_ref - pd.to_datetime(x['Timestamp'])).days / T)), \n", "    axis=1\n", ")"]}, {"block": 31, "type": "code", "linesLength": 1, "startIndex": 137, "lines": ["data2_w"]}, {"block": 32, "type": "markdown", "linesLength": 1, "startIndex": 138, "lines": ["Affinity scores of user-item pairs can be calculated then by summing the 'Timedecay' column values."]}, {"block": 33, "type": "code", "linesLength": 2, "startIndex": 139, "lines": ["data2_wt = data2_w.groupby(['UserId', 'ItemId'])['Timedecay'].sum().reset_index()\n", "data2_wt.columns = ['UserId', 'ItemId', 'Affinity']"]}, {"block": 34, "type": "code", "linesLength": 1, "startIndex": 141, "lines": ["data2_wt"]}, {"block": 35, "type": "markdown", "linesLength": 1, "startIndex": 142, "lines": ["### 2.2 Negative sampling"]}, {"block": 36, "type": "markdown", "linesLength": 3, "startIndex": 143, "lines": ["The above aggregation is based on assumptions that user-item interactions can be interpreted as preferences by taking the factors like \"number of interation times\", \"weights\", \"time decay\", etc. Sometimes these assumptions are biased, and only the interactions themselves matter. That is, the original dataset with implicit interaction records can be binarized into one that has only 1 or 0, indicating if a user has interacted with an item, respectively.\n", "\n", "For example, the following generates data that contains existing interactions between users and items. "]}, {"block": 37, "type": "code", "linesLength": 3, "startIndex": 146, "lines": ["data2_b = data2[['UserId', 'ItemId']].copy()\n", "data2_b['Feedback'] = 1\n", "data2_b = data2_b.drop_duplicates()"]}, {"block": 38, "type": "code", "linesLength": 1, "startIndex": 149, "lines": ["data2_b"]}, {"block": 39, "type": "markdown", "linesLength": 3, "startIndex": 150, "lines": ["\"Negative sampling\" is a technique that samples negative feedbacks. Similar to the aggregation techniques, negative feedbacks cna be defined differently in different scenarios. In this case, for example, we can regard the items that a user has not interacted as those that the user does not like. This may be a strong assumption in many user cases, but it is reasonable to build a model when the interaction times between user and item are not that many.\n", "\n", "The following shows that, on top of `data2_b`, there are another 2 negative samples are generated which are tagged with \"0\" in the \"Feedback\" column."]}, {"block": 40, "type": "code", "linesLength": 2, "startIndex": 153, "lines": ["users = data2['UserId'].unique()\n", "items = data2['ItemId'].unique()"]}, {"block": 41, "type": "code", "linesLength": 6, "startIndex": 155, "lines": ["interaction_lst = []\n", "for user in users:\n", "    for item in items:\n", "        interaction_lst.append([user, item, 0])\n", "\n", "data_all = pd.DataFrame(data=interaction_lst, columns=[\"UserId\", \"ItemId\", \"FeedbackAll\"])"]}, {"block": 42, "type": "code", "linesLength": 1, "startIndex": 161, "lines": ["data_all"]}, {"block": 43, "type": "code", "linesLength": 1, "startIndex": 162, "lines": ["data2_ns = pd.merge(data_all, data2_b, on=['UserId', 'ItemId'], how='outer').fillna(0).drop('FeedbackAll', axis=1)"]}, {"block": 44, "type": "code", "linesLength": 1, "startIndex": 163, "lines": ["data2_ns"]}, {"block": 45, "type": "markdown", "linesLength": 1, "startIndex": 164, "lines": ["Also note that sometimes the negative sampling may also impact the count-based aggregation scheme. That is, the count may start from 0 instead of 1, and 0 means there is no interaction between the user and item. "]}, {"block": 46, "type": "markdown", "linesLength": 5, "startIndex": 165, "lines": ["# References\n", "\n", "1. X. He *et al*, Neural Collaborative Filtering, WWW 2017. \n", "2. Y. Hu *et al*, Collaborative filtering for implicit feedback datasets, ICDM 2008.\n", "3. Smart Adapative Recommendation (SAR), url: https://github.com/Microsoft/Recommenders/blob/master/notebooks/02_model/sar_single_node_deep_dive.ipynb"]}]