[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["## Memory management utils"]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 1, "lines": ["Utility functions for memory management. Currently primarily for GPU."]}, {"block": 2, "type": "code", "linesLength": 2, "startIndex": 2, "lines": ["from fastai.gen_doc.nbdoc import *\n", "from fastai.utils.mem import * "]}, {"block": 3, "type": "code", "linesLength": 1, "startIndex": 4, "lines": ["show_doc(gpu_mem_get)"]}, {"block": 4, "type": "markdown", "linesLength": 5, "startIndex": 5, "lines": ["[`gpu_mem_get`](/utils.mem.html#gpu_mem_get)\n", "\n", "* for gpu returns `GPUMemory(total, free, used)`\n", "* for cpu returns `GPUMemory(0, 0, 0)`\n", "* for invalid gpu id returns `GPUMemory(0, 0, 0)`"]}, {"block": 5, "type": "code", "linesLength": 1, "startIndex": 10, "lines": ["show_doc(gpu_mem_get_all)"]}, {"block": 6, "type": "markdown", "linesLength": 3, "startIndex": 11, "lines": ["[`gpu_mem_get_all`](/utils.mem.html#gpu_mem_get_all)\n", "* for gpu returns `[ GPUMemory(total_0, free_0, used_0), GPUMemory(total_1, free_1, used_1), .... ]`\n", "* for cpu returns `[]`\n"]}, {"block": 7, "type": "code", "linesLength": 1, "startIndex": 14, "lines": ["show_doc(gpu_mem_get_free)"]}, {"block": 8, "type": "markdown", "linesLength": 0, "startIndex": 15, "lines": []}, {"block": 9, "type": "code", "linesLength": 1, "startIndex": 15, "lines": ["show_doc(gpu_mem_get_free_no_cache)"]}, {"block": 10, "type": "markdown", "linesLength": 0, "startIndex": 16, "lines": []}, {"block": 11, "type": "code", "linesLength": 1, "startIndex": 16, "lines": ["show_doc(gpu_mem_get_used)"]}, {"block": 12, "type": "markdown", "linesLength": 0, "startIndex": 17, "lines": []}, {"block": 13, "type": "code", "linesLength": 1, "startIndex": 17, "lines": ["show_doc(gpu_mem_get_used_no_cache)"]}, {"block": 14, "type": "markdown", "linesLength": 1, "startIndex": 18, "lines": ["[`gpu_mem_get_used_no_cache`](/utils.mem.html#gpu_mem_get_used_no_cache)"]}, {"block": 15, "type": "code", "linesLength": 1, "startIndex": 19, "lines": ["show_doc(gpu_mem_get_used_fast)"]}, {"block": 16, "type": "markdown", "linesLength": 1, "startIndex": 20, "lines": ["[`gpu_mem_get_used_fast`](/utils.mem.html#gpu_mem_get_used_fast)"]}, {"block": 17, "type": "code", "linesLength": 1, "startIndex": 21, "lines": ["show_doc(gpu_with_max_free_mem)"]}, {"block": 18, "type": "markdown", "linesLength": 3, "startIndex": 22, "lines": ["[`gpu_with_max_free_mem`](/utils.mem.html#gpu_with_max_free_mem):\n", "* for gpu returns: `gpu_with_max_free_ram_id, its_free_ram`\n", "* for cpu returns: `None, 0`\n"]}, {"block": 19, "type": "code", "linesLength": 1, "startIndex": 25, "lines": ["show_doc(preload_pytorch)"]}, {"block": 20, "type": "markdown", "linesLength": 1, "startIndex": 26, "lines": ["[`preload_pytorch`](/utils.mem.html#preload_pytorch) is helpful when GPU memory is being measured, since the first time any operation on `cuda` is performed by pytorch, usually about 0.5GB gets used by CUDA context."]}, {"block": 21, "type": "code", "linesLength": 1, "startIndex": 27, "lines": ["show_doc(GPUMemory, title_level=4)"]}, {"block": 22, "type": "markdown", "linesLength": 1, "startIndex": 28, "lines": ["[`GPUMemory`](/utils.mem.html#GPUMemory) is a namedtuple that is returned by functions like [`gpu_mem_get`](/utils.mem.html#gpu_mem_get) and [`gpu_mem_get_all`](/utils.mem.html#gpu_mem_get_all)."]}, {"block": 23, "type": "code", "linesLength": 1, "startIndex": 29, "lines": ["show_doc(b2mb)"]}, {"block": 24, "type": "markdown", "linesLength": 1, "startIndex": 30, "lines": ["[`b2mb`](/utils.mem.html#b2mb) is a helper utility that just does `int(bytes/2**20)`"]}, {"block": 25, "type": "markdown", "linesLength": 1, "startIndex": 31, "lines": ["## Memory Tracing Utils"]}, {"block": 26, "type": "code", "linesLength": 1, "startIndex": 32, "lines": ["show_doc(GPUMemTrace, title_level=4)"]}, {"block": 27, "type": "markdown", "linesLength": 95, "startIndex": 33, "lines": ["**Arguments**:\n", "\n", "* `silent` shortcut to make `report` and `report_n_reset` silent w/o needing to remove those calls - this can be done from the constructor, or alternatively you can call `silent` method anywhere to do the same.\n", "\n", "**Definitions**:\n", "\n", "* **Delta Used** is the difference between current used memory and used memory at the start of the counter.\n", "\n", "* **Delta Peaked** is the memory overhead if any. It's calculated in two steps:\n", "   1. The base measurement is the difference between the peak memory and the used memory at the start of the counter.\n", "   2. Then if delta used is positive it gets subtracted from the base value.\n", "   \n", "   It indicates the size of the blip.\n", "\n", "   **Warning**: currently the peak memory usage tracking is implemented using a python thread, which is very unreliable, since there is no guarantee the thread will get a chance at running at the moment the peak memory is occuring (or it might not get a chance to run at all). Therefore we need pytorch to implement multiple concurrent and resettable [`torch.cuda.max_memory_allocated`](https://pytorch.org/docs/stable/cuda.html#torch.cuda.max_memory_allocated) counters. Please vote for this [feature request](https://github.com/pytorch/pytorch/issues/16266).\n", "\n", "**Usage Examples**:\n", "\n", "Setup:\n", "```\n", "from fastai.utils.mem import GPUMemTrace\n", "def some_code(): pass\n", "mtrace = GPUMemTrace()\n", "```\n", "\n", "Example 1: basic measurements via `report` (prints) and via [`data`](/tabular.data.html#tabular.data) (returns) accessors\n", "```\n", "some_code()\n", "mtrace.report()\n", "delta_used, delta_peaked = mtrace.data()\n", "\n", "some_code()\n", "mtrace.report('2nd run of some_code()')\n", "delta_used, delta_peaked = mtrace.data()\n", "```\n", "`report`'s optional `note` argument can be helpful if you have many `report` calls and you want to understand which is which in the outputs.\n", "\n", "Example 2: measure in a loop, resetting the counter before each run\n", "```\n", "for i in range(10):\n", "    mtrace.reset()\n", "    some_code()\n", "    mtrace.report(f'i={i}')\n", "```\n", "`reset` resets all the counters.\n", "\n", "Example 3: like example 2, but having `report` automatically reset the counters\n", "```\n", "mtrace.reset()\n", "for i in range(10):\n", "    some_code()\n", "    mtrace.report_n_reset(f'i={i}')\n", "```\n", "\n", "The tracing starts immediately upon the [`GPUMemTrace`](/utils.mem.html#GPUMemTrace) object creation, and stops when that object is deleted. But it can also be `stop`ed, `start`ed manually as well.\n", "```\n", "mtrace.start()\n", "mtrace.stop()\n", "```\n", "`stop` is in particular useful if you want to **freeze** the [`GPUMemTrace`](/utils.mem.html#GPUMemTrace) object and to be able to query its data on `stop` some time down the road.\n", "\n", "Example 4: Silencing report calls w/o removing them\n", "```\n", "mtrace = GPUMemTrace(silent=True)\n", "mtrace.report() # nothing will be printed\n", "mtrace.silent(silent=False)\n", "mtrace.report() # printing resumed\n", "mtrace.silent(silent=True)\n", "mtrace.report() # nothing will be printed\n", "```\n", "\n", "**Context Manager**:\n", "\n", "[`GPUMemTrace`](/utils.mem.html#GPUMemTrace) can also be used as a context manager:\n", "\n", "Get the numerical data (in rounder MBs):\n", "```\n", "with GPUMemTrace() as mtrace:\n", "    some_code()\n", "delta_used, delta_peaked = mtrace.data()\n", "```\n", "\n", "Print the used and peaked deltas:\n", "```\n", "with GPUMemTrace() as mtrace:\n", "    some_code()\n", "mem_trace.report(\"measured in ctx\")\n", "\n", "```\n", "or the same w/o the context note:\n", "```\n", "with GPUMemTrace() as mtrace:\n", "    some_code()\n", "print(mtrace)\n", "```"]}, {"block": 28, "type": "markdown", "linesLength": 6, "startIndex": 128, "lines": ["## Workarounds to the leaky ipython traceback on exception\n", "\n", "ipython has a feature where it stores tb with all the `locals()` tied in, which\n", "prevents `gc.collect()` from freeing those variables and leading to a leakage.\n", "\n", "Therefore we cleanse the tb before handing it over to ipython. The 2 ways of doing it are by either using the [`gpu_mem_restore`](/utils.mem.html#gpu_mem_restore) decorator or the [`gpu_mem_restore_ctx`](/utils.mem.html#gpu_mem_restore_ctx) context manager which are described next:"]}, {"block": 29, "type": "code", "linesLength": 1, "startIndex": 134, "lines": ["show_doc(gpu_mem_restore)"]}, {"block": 30, "type": "markdown", "linesLength": 12, "startIndex": 135, "lines": ["[`gpu_mem_restore`](/utils.mem.html#gpu_mem_restore) is a decorator to be used with any functions that interact with CUDA (top-level is fine)\n", "\n", "* under non-ipython environment it doesn't do anything.\n", "* under ipython currently it strips tb by default only for the \"CUDA out of memory\" exception.\n", "\n", "The env var `FASTAI_TB_CLEAR_FRAMES` changes this behavior when run under ipython,\n", "depending on its value: \n", "\n", "* \"0\": never  strip tb (makes it possible to always use `%debug` magic, but with leaks)\n", "* \"1\": always strip tb (never need to worry about leaks, but `%debug` won't work)\n", "\n", "e.g. `os.environ['FASTAI_TB_CLEAR_FRAMES']=\"0\"` will set it to 0.\n"]}, {"block": 31, "type": "code", "linesLength": 1, "startIndex": 147, "lines": ["show_doc(gpu_mem_restore_ctx, title_level=4)"]}, {"block": 32, "type": "markdown", "linesLength": 6, "startIndex": 148, "lines": ["if function decorator is not a good option, you can use a context manager instead. For example:\n", "```\n", "with gpu_mem_restore_ctx():\n", "   learn.fit_one_cycle(1,1e-2)\n", "```\n", "This particular one will clear tb on any exception."]}, {"block": 33, "type": "markdown", "linesLength": 1, "startIndex": 154, "lines": ["## Undocumented Methods - Methods moved below this line will intentionally be hidden"]}, {"block": 34, "type": "code", "linesLength": 1, "startIndex": 155, "lines": ["show_doc(GPUMemTrace.report)"]}, {"block": 35, "type": "markdown", "linesLength": 0, "startIndex": 156, "lines": []}, {"block": 36, "type": "code", "linesLength": 1, "startIndex": 156, "lines": ["show_doc(GPUMemTrace.silent)"]}, {"block": 37, "type": "markdown", "linesLength": 0, "startIndex": 157, "lines": []}, {"block": 38, "type": "code", "linesLength": 1, "startIndex": 157, "lines": ["show_doc(get_ref_free_exc_info)"]}, {"block": 39, "type": "markdown", "linesLength": 0, "startIndex": 158, "lines": []}, {"block": 40, "type": "code", "linesLength": 1, "startIndex": 158, "lines": ["show_doc(GPUMemTrace.start)"]}, {"block": 41, "type": "markdown", "linesLength": 0, "startIndex": 159, "lines": []}, {"block": 42, "type": "code", "linesLength": 1, "startIndex": 159, "lines": ["show_doc(GPUMemTrace.reset)"]}, {"block": 43, "type": "markdown", "linesLength": 0, "startIndex": 160, "lines": []}, {"block": 44, "type": "code", "linesLength": 1, "startIndex": 160, "lines": ["show_doc(GPUMemTrace.peak_monitor_stop)"]}, {"block": 45, "type": "markdown", "linesLength": 0, "startIndex": 161, "lines": []}, {"block": 46, "type": "code", "linesLength": 1, "startIndex": 161, "lines": ["show_doc(GPUMemTrace.stop)"]}, {"block": 47, "type": "markdown", "linesLength": 0, "startIndex": 162, "lines": []}, {"block": 48, "type": "code", "linesLength": 1, "startIndex": 162, "lines": ["show_doc(GPUMemTrace.report_n_reset)"]}, {"block": 49, "type": "markdown", "linesLength": 0, "startIndex": 163, "lines": []}, {"block": 50, "type": "code", "linesLength": 1, "startIndex": 163, "lines": ["show_doc(GPUMemTrace.peak_monitor_func)"]}, {"block": 51, "type": "markdown", "linesLength": 0, "startIndex": 164, "lines": []}, {"block": 52, "type": "code", "linesLength": 1, "startIndex": 164, "lines": ["show_doc(GPUMemTrace.data_set)"]}, {"block": 53, "type": "markdown", "linesLength": 0, "startIndex": 165, "lines": []}, {"block": 54, "type": "code", "linesLength": 1, "startIndex": 165, "lines": ["show_doc(GPUMemTrace.data)"]}, {"block": 55, "type": "markdown", "linesLength": 0, "startIndex": 166, "lines": []}, {"block": 56, "type": "code", "linesLength": 1, "startIndex": 166, "lines": ["show_doc(GPUMemTrace.peak_monitor_start)"]}, {"block": 57, "type": "markdown", "linesLength": 0, "startIndex": 167, "lines": []}, {"block": 58, "type": "markdown", "linesLength": 1, "startIndex": 167, "lines": ["## New Methods - Please document or move to the undocumented section"]}]