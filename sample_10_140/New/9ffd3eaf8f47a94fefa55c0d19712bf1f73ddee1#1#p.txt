[{"block": 0, "type": "markdown", "linesLength": 33, "startIndex": 0, "lines": ["### Data reduction for paleomagnetic data aboard the JOIDES Resolution\n", "\n", "This notebook is for people wanting to download and manipulate data from an IODP Expedition using data in the LIMS Online Repository.  The basic procedure is as follows.  This notebook will guide you through the process step-by-step.   \n", "\n", "- Import the required python packages and set up the desired directory structure for each HOLE. \n", "- Download the Section Summary table using the \"download tabular data\" optiopn in LORE and put the .csv file in the HOLE working directory\n", "- Download the sample table for any discrete samples taken and put them in the HOLE working directory\n", "- Download the SRM data for the hole and put them in the SRM working directory in the HOLE working directory \n", "- Download the SRM discrete measurement data for the hole and put them in the SRM_discrete sample working directory.\n", "- Download the JR6 data and put them in the JR6 working directory\n", "- Download the KLY4S data and put them in the KLY4S working directory.\n", "- If you want to edit the archive half data for coring or lithologic disturbances:\n", "- Download the core disturbance info from Desklogic\n", "    - go to the Desklogic computer station and open DeskLogic (little yellow square)\n", "    - select the 'macroscopic' template\n", "    - select sample: hole, Archive, section half\n", "    - download\n", "    - export: include classification, Data and save on the Desktop\n", "    - if the network drives are not available, right click on the double square icon on bar\n", "    - switch the login profile to scientist and login with OES credentials (user name/email password)\n", "    - choose scientist and map the drives.  they should be now available to windows explorer\n", "    - copy to your HOLE working directory.  \n", "- If you want to use Xray information to edit your archive half data:  fill in  the XRAY disturbance summary file and put it into the HOLE working directory\n", "\n", "To start processing data from a single HOLE:\n", "\n", "- Duplicate this notebook with the HOLE name (e.g., U1534A) by first making a copy, then renaming it under the 'File' menu.  Follow the instructions below. \n", "\n", "For HELP:   \n", "    - for help on options in any python function, you can type:\n", "    help(MODULE.FUNCTION)  in the cell below.    For example click on the cell below for information on how to use convert.iodp_samples_csv.  This works for any python function.  \n", "    - email ltauxe@ucsd.edu.  Incluede a description of your problem, a screen shot of the error if appropriate and an example data file that is causing the difficulty (be mindful of embargo issues).  \n", "\n"]}, {"block": 1, "type": "markdown", "linesLength": 10, "startIndex": 33, "lines": ["## Table of Contents\n", "\n", "- [Preliminaries](#preliminaries) : import required packages and set up directory structure\n", "- [Make sample tables](#sample) : parse downloaded sample table to standard format\n", "- [Archive measurements](#archives) : parse downloaded srm archive measurements and perform editing functions\n", "- [Icefield orientation adjustments](#icefield) : attempt core orientation\n", "- [Discrete sample measurements](#discretes) : parse downloaded discrete sample data\n", "- [Downhole plots](#plots) : make downhole plots of remanence\n", "- [Anisotropy of magnetic susceptibility](#aniso) : plots of AMS\n", "- [Prepare files for uploading to MagIC](#upload) : you can upload data to the MagIC database as a private contribution.  Then when you have published your data, you can activate your contribution with the DOI of your publication.  "]}, {"block": 2, "type": "markdown", "linesLength": 10, "startIndex": 43, "lines": ["<a id='preliminaries'></a>\n", "\n", "\n", "## Preliminaries\n", "\n", "- In the cell below, edit the HOLE name  and set the HOLE latitude and longitude  where HOLE stands for the hole name.  The cell below will do this for you. \n", "\n", "- To set up the file structure the first time for every hole, set mkdir to True but only execute it once for each HOLE! After you have run this, set mkdir to False\n", "\n", "- Every time you open this notebook, you must click on the cell below and then click 'Run' in the menu above to execute it. "]}, {"block": 3, "type": "code", "linesLength": 62, "startIndex": 53, "lines": ["# import a bunch of packages for use in the notebook\n", "import pmagpy.pmag as pmag # a bunch of PmagPy modules\n", "import pmagpy.pmagplotlib as pmagplotlib\n", "import pmagpy.ipmag as ipmag\n", "import pmagpy.contribution_builder as cb\n", "from pmagpy import convert_2_magic as convert # conversion scripts for many lab formats\n", "from pmagpy import  iodp_funcs as iodp_funcs # functions for dealing with LIMS data\n", "import matplotlib.pyplot as plt # our plotting buddy\n", "import numpy as np # the fabulous NumPy package\n", "import pandas as pd # and of course Pandas\n", "%matplotlib inline \n", "from importlib import reload\n", "import warnings \n", "warnings.filterwarnings(\"ignore\")\n", "meas_files,spec_files,samp_files,site_files=[],[],[],[] # holders for magic file names\n", "import os\n", "\n", "\n", "\n", "# Modify these for your expedition\n", "exp_name,exp_description='IODP Expedition 999','IODP Test Site'\n", "\n", "# Edit these for each hole.  \n", "hole='U999A'\n", "hole_lat,hole_lon=0+0/60,0+0/60 # edit these for the current hole - get it from Hole summary on downloaded as LORE\n", "gad_inc=pmag.pinc(hole_lat)\n", "dscr_file=\"\"\n", "\n", "demag_step=0.010 # choose the demagnetization step (in tesla) for downhole plots \n", "\n", "\n", "# set up the directory structure for this hole\n", "\n", "\n", "\n", "jr6_dir=hole+'/JR6_data'\n", "kly4s_dir=hole+'/KLY4S_data'\n", "srm_archive_dir=hole+'/SRM_archive_data'\n", "srm_discrete_dir=hole+'/SRM_discrete_data'\n", "magic_dir=hole+'/'+hole+'_MagIC'\n", "\n", "# only run this once.  To create the file structure, change the \"False\" below to \"True\" and execute this cell\n", "\n", "# After you have run this cell once, please change the \"True\" back to \"False\" \n", "# or you will get an error because the directories already exist  \n", "\n", "mkdir=False \n", "if mkdir:\n", "    os.mkdir(hole)\n", "    os.mkdir(jr6_dir)\n", "    os.mkdir(kly4s_dir)\n", "    os.mkdir(srm_archive_dir)\n", "    os.mkdir(srm_discrete_dir)\n", "    os.mkdir(magic_dir)\n", "    os.mkdir('Figures')\n", "\n", "#After mkdir has been run ONCE, \n", "# set mkdir to False, download the summary file and put the name in here, then \n", "# re-run this cell.  Run it everytime you open this notebook. \n", "section_summary_file=\"Section Summary_17_5_2019.csv\" # set this to the downloaded summary file (e.g., Section Summary_15_5_2019.csv)\n", "if section_summary_file:\n", "    summary_file=hole+'/'+section_summary_file # Edit this\n"]}, {"block": 4, "type": "markdown", "linesLength": 10, "startIndex": 115, "lines": ["<a id='sample'></a>\n", "\n", "\n", "\n", "### Make the sample tables\n", "\n", "- Download the Sample Report and Section Summary tables (when available) for the HOLE from the LIMS online repository at http://web.ship.iodp.tamu.edu/LORE/ and save them as csv files.    \n", "- Put the two .csv files in the HOLE directory created above.   \n", "- Edit the name of the sample .csv  file and the 'secondary depth' column that you selected when downloading (the default is CSF-B as below) in the cell below. \n", "- Execute the cell to  populate the MagIC meta-data tables.  The depth information ends up in the lims_sites.txt table in the HOLE_MagIC directory, for example and gets used to create downhole plots."]}, {"block": 5, "type": "markdown", "linesLength": 1, "startIndex": 125, "lines": ["Run the following cell to create the samples, sites, and locations tables for the hole.  "]}, {"block": 6, "type": "code", "linesLength": 23, "startIndex": 126, "lines": ["# Fill in the name of your sample file (e.g., samp_file='samples_5_4_2019.csv' )\n", "# Make sure the sample file is in your HOLE directory. \n", "# Note:  this program will not run if the file is in use\n", "samp_file=\"samples_17_5_2019.csv\"\n", "if samp_file:\n", "    \n", "    comp_depth_key='Top depth CSF-B (m)'\n", "# do the heavy lifting: \n", "    convert.iodp_samples_csv(samp_file,input_dir_path=hole,spec_file='lims_specimens.txt',\\\n", "                         samp_file='lims_samples.txt',site_file='lims_sites.txt',\\\n", "                         dir_path=magic_dir,comp_depth_key=comp_depth_key,\\\n", "                exp_name=exp_name,exp_desc=exp_description,lat=hole_lat,\\\n", "                lon=hole_lon)\n", "# this collects the file names that were created so they can be combined with others, e.g., those\n", "# from the archive half measurements which are not in the sample table. \n", "    if 'lims_specimens.txt' not in spec_files:spec_files.append('lims_specimens.txt')\n", "    if 'lims_samples.txt' not in samp_files:samp_files.append('lims_samples.txt')\n", "    if 'lims_sites.txt' not in site_files:site_files.append('lims_sites.txt')\n", "# do it again to make copies for use with demag_gui\n", "    convert.iodp_samples_csv(samp_file,input_dir_path=hole,\\\n", "                         dir_path=magic_dir,comp_depth_key=comp_depth_key,\\\n", "                exp_name=exp_name,exp_desc=exp_description,lat=hole_lat,\\\n", "                lon=hole_lon)\n"]}, {"block": 7, "type": "markdown", "linesLength": 9, "startIndex": 149, "lines": ["<a id='archives'></a>\n", "\n", "\n", "### Convert the SRM archive half data for the Hole\n", "\n", "- download data for a single hole from LORE as a csv file.\n", "- put the file into the HOLE/SRM_archive_data directory in the HOLE directory\n", "- edit the file name and the composite depth column header (comp_depth_key) if desired.\n", "- if you have been busy measuring archives, writing the measurement files can take awhile so be patient.  \n"]}, {"block": 8, "type": "code", "linesLength": 16, "startIndex": 158, "lines": [" \n", "# Fill in the name of your srm archive half measruement file (e.g., samp_file='samples_5_4_2019.csv' )\n", "# Make sure the sample file is in your HOLE directory. \n", "# Note:  this program will not run if the file is in use\n", "\n", "srm_file=\"srmsection_17_5_2019.csv\"   \n", "if srm_file:\n", " \n", "    comp_depth_key='Depth CSF-B (m)'\n", "\n", "    convert.iodp_srm_lore(srm_file,meas_file='srm_arch_measurements.txt', comp_depth_key=comp_depth_key,\\\n", "                  dir_path=magic_dir,input_dir_path=srm_archive_dir,lat=hole_lat,lon=hole_lon)\n", "    if 'srm_arch_measurements.txt' not in meas_files:meas_files.append('srm_arch_measurements.txt')\n", "    if 'srm_arch_specimens.txt' not in spec_files:spec_files.append('srm_arch_specimens.txt')\n", "    if 'srm_arch_samples.txt' not in samp_files:samp_files.append('srm_arch_samples.txt')\n", "    if 'srm_arch_sites.txt' not in site_files:site_files.append('srm_arch_sites.txt')\n"]}, {"block": 9, "type": "markdown", "linesLength": 10, "startIndex": 174, "lines": ["Editing of SRM archive data.\n", "\n", "\n", "- filter for desired demag step (set in the preliminaries cell). \n", "- remove data from within 80 cm of core tops and 10 cm from section ends.\n", "- if desired: (set nodist=True), remove from \"disturbed\" intervals labled \"high\" from DescLogic. \n", "    - go to DescLogic and export the list of disturbances for the hole.\n", "    - put this in HOLE_disturbances.xlsx in the hole directory. note that HOLE is your hole name, set in the \"preliminaries\" cell. \n", "- remove data from disturbed intervals based on the Xrays. You have to create the Xray data file yourself. There is a template for this that must be followed.  \n", "- adjust the data to average normal dec=90\n"]}, {"block": 10, "type": "code", "linesLength": 20, "startIndex": 184, "lines": ["# to execute this cell, set False to True. turn it back to False so you don't rerun this by accident. \n", "remove_ends=True\n", "remove_desclogic_disturbance=False # no DescLogic examples for fake data!\n", "remove_xray_disturbance=False # no Xray disturbance file for fake data!  \n", "\n", "if True:\n", "    arch_demag_step=iodp_funcs.demag_step(magic_dir,hole,demag_step)    # pick the demag step\n", "    if remove_ends:\n", "        noends=iodp_funcs.remove_ends(arch_demag_step,hole) # remove the ends\n", "    else:\n", "        noends=arch_demag_step\n", "    if remove_desclogic_disturbance:\n", "        nodist=iodp_funcs.remove_disturbance(noends,hole) # remove coring disturbances\n", "    else: \n", "        nodist=noends\n", "    if remove_xray_disturbance: \n", "        no_xray_df=iodp_funcs.no_xray_disturbance(nodist,hole)\n", "    else:\n", "        no_xray_df=nodist\n", "    adj_dec_df,core_dec_adj=iodp_funcs.adj_dec(no_xray_df,hole)"]}, {"block": 11, "type": "markdown", "linesLength": 8, "startIndex": 204, "lines": ["<a id='discretes'></a>\n", "\n", "### Convert SRM discrete sample data to MagIC:\n", "- download data for a single hole from LORE as a csv file.\n", "    - for OFFLINE treatments (ARM,IRM, DTECH AF, thermal), download both the \n", "      \"standard\" and the extended file names\n", "- put the file into the HOLE/SRM_discrete_data directory in the HOLE directory\n", "- for \"regular\" SRM files (no offline treaments) edit the file name and execute the cell below:"]}, {"block": 12, "type": "code", "linesLength": 8, "startIndex": 212, "lines": ["srm_discrete_file= 'srmdiscrete_17_5_2019.csv' # SRM discrete measurements\n", "\n", "if srm_discrete_file:\n", "\n", "    convert.iodp_dscr_lore(srm_discrete_file,meas_file='srm_dscr_measurements.txt', \\\n", "                  dir_path=magic_dir,input_dir_path=srm_discrete_dir,spec_file='lims_specimens.txt')\n", "    if 'srm_dscr_measurements.txt' not in meas_files:meas_files.append('srm_dscr_measurements.txt')\n", "    dscr_file='srm_dscr_measurements.txt'"]}, {"block": 13, "type": "markdown", "linesLength": 1, "startIndex": 220, "lines": ["- for OFFLINE treatments: specify the extended discrete srm file name."]}, {"block": 14, "type": "code", "linesLength": 10, "startIndex": 221, "lines": ["dscr_ex_file='' # SRM discrete extended file\n", "srm_discrete_file='' # SRM discrete measurements\n", "if dscr_ex_file and srm_discrete_file:\n", "\n", "\n", "    convert.iodp_dscr_lore(srm_discrete_file,dscr_ex_file=dscr_ex_file,meas_file='srm_dscr_measurements.txt', \\\n", "                  dir_path=magic_dir,input_dir_path=srm_discrete_dir,spec_file='lims_specimens.txt',\\\n", "                      offline_meas_file='srm_dscr_offline_measurements.txt')\n", "    if 'srm_dscr_measurements.txt' not in meas_files:meas_files.append('srm_dscr_measurements.txt')\n", "    if 'srm_dscr_offline_measurements.txt' not in meas_files:meas_files.append('srm_dscr_offline_measurments.txt')"]}, {"block": 15, "type": "markdown", "linesLength": 1, "startIndex": 231, "lines": ["To make some quickie zijderveld plots in the notebook set the following the True.  To save all the plots, set save_plots to True"]}, {"block": 16, "type": "code", "linesLength": 3, "startIndex": 232, "lines": ["if dscr_file:\n", "    ipmag.zeq_magic(meas_file=dscr_file,\\\n", "                spec_file='lims_specimens.txt',input_dir_path=magic_dir,n_plots=\"all\",save_plots=False)"]}, {"block": 17, "type": "markdown", "linesLength": 1, "startIndex": 235, "lines": ["If you did a bunch of full demagnetizations, set the your maximum af field.  To save your plots, set save_plots to True.  To change the output format, change 'svg' to whatever you want ('pdf','eps','png').  "]}, {"block": 18, "type": "code", "linesLength": 13, "startIndex": 236, "lines": ["cnt=1\n", "max_field=0 # set this to your peak field (in tesla) to execute this\n", "\n", "if max_field:\n", "    srm_dscr_df=pd.read_csv(magic_dir+'/'+dscr_file,sep='\\t',header=1)\n", "    srm_dmag=srm_dscr_df[srm_dscr_df.treat_ac_field>=max_field] # find all the demag specimens\n", "    spc_list=srm_dmag.specimen.unique()\n", "\n", "    for spc in spc_list:\n", "        ipmag.zeq_magic(meas_file=dscr_file,specimen=spc,fignum=cnt,\\\n", "                spec_file='lims_specimens.txt',input_dir_path=magic_dir,save_plots=False,\n", "                       fmt='svg')\n", "        cnt+=3;\n"]}, {"block": 19, "type": "markdown", "linesLength": 4, "startIndex": 249, "lines": ["### Import the JR6 data.  \n", "- download the JR6 data from LIMS and put it in the JR6_data folder in your working directory.\n", "- edit the file name in the cell below to reflect the actual file name.  \n", "- execute the two cells in order. "]}, {"block": 20, "type": "code", "linesLength": 6, "startIndex": 253, "lines": ["jr6_file='spinner_17_5_2019.csv' # JR6 data file from LORE\n", "if jr6_file:\n", "    convert.iodp_jr6_lore(jr6_file,meas_file='jr6_measurements.txt',dir_path=magic_dir,\\\n", "                     input_dir_path=jr6_dir,spec_file='lims_specimens.txt',noave=False)\n", "    if 'jr6_measurements.txt' not in meas_files:meas_files.append('jr6_measurements.txt')\n", "    dscr_file='jr6_measurements.txt'"]}, {"block": 21, "type": "code", "linesLength": 17, "startIndex": 259, "lines": ["\n", "# combine srm and jr6 data: set this to True\n", "if True:\n", "    ipmag.combine_magic(['srm_dscr_measurements.txt','jr6_measurements.txt'],\n", "                        outfile='dscr_measurements.txt',dir_path=magic_dir)\n", "    dscr_file='dscr_measurements.txt'\n", "max_field=80 # set this to your peak field (in tesla) to execute this\n", "if max_field:\n", "    cnt=1\n", "    dscr_df=pd.read_csv(magic_dir+'/dscr_measurements.txt',sep='\\t',header=1)\n", "    dmag_df=dscr_df[dscr_df.treat_ac_field>=max_field] # find all the demag specimens\n", "    spc_list=dmag_df.specimen.unique()\n", "\n", "    for spc in spc_list:\n", "        ipmag.zeq_magic(meas_file='dscr_measurements.txt',specimen=spc,fignum=cnt,\\\n", "                spec_file='lims_specimens.txt',input_dir_path=magic_dir,save_plots=False)\n", "        cnt+=3;\n"]}, {"block": 22, "type": "markdown", "linesLength": 3, "startIndex": 276, "lines": ["<a id='aniso'></a>\n", "\n", "###  AMS data"]}, {"block": 23, "type": "markdown", "linesLength": 5, "startIndex": 279, "lines": ["Convert AMS data to MagIC\n", "\n", "- Download KAPPABRIDGE expanded magnetic susceptibility data from the Lims Online Repository\n", "- place the downloaded .csv file in the KLY4S_data in the HOLE directory.  \n", "- change kly4s_file below to the correct file name"]}, {"block": 24, "type": "code", "linesLength": 7, "startIndex": 284, "lines": ["kly4s_file='ex-kappa_17_5_2019.csv'\n", "if kly4s_file:\n", "    convert.iodp_kly4s_lore(kly4s_file, meas_out='kly4s_measurements.txt', \n", "           spec_infile='lims_specimens.txt', spec_out='kly4s_specimens.txt',\n", "           dir_path=magic_dir, input_dir_path=kly4s_dir,actual_volume=7)\n", "    if 'kly4s_measurements.txt' not in meas_files:meas_files.append('kly4s_measurements.txt')\n", "    if 'kly4s_specimens.txt' not in meas_files:meas_files.append('kly4s_specimens.txt')"]}, {"block": 25, "type": "markdown", "linesLength": 2, "startIndex": 291, "lines": ["To make a depth plot of your AMS data, download the Core Summary File from LORE, put it in your HOLE_MagIC directory and enter the name in the sum_file argument below. \n", "Then change the False to True in the cell below and run it."]}, {"block": 26, "type": "code", "linesLength": 6, "startIndex": 293, "lines": ["if True:\n", "    ipmag.ani_depthplot(spec_file='kly4s_specimens.txt', dir_path=magic_dir, \n", "                    samp_file='lims_samples.txt',site_file='lims_sites.txt',\n", "                   dmin=-1,dmax=-1,meas_file='kly4s_measurements.txt',\n", "                   sum_file='Core Summary_17_5_2019.csv')\n", "    plt.savefig('Figures/'+hole+'_anisotropy_xmastree.pdf')"]}, {"block": 27, "type": "markdown", "linesLength": 2, "startIndex": 299, "lines": ["This way makes equal area plots in core coordinates.... To run it, set the False to True.\n", "To save the plots, sset save_plots to True.  for different options, try (help(ipmag.aniso_magic_nb))."]}, {"block": 28, "type": "code", "linesLength": 3, "startIndex": 301, "lines": ["if True:\n", "    ipmag.aniso_magic_nb(infile=magic_dir+'/kly4s_specimens.txt',\\\n", "                     verbose=False,save_plots=False,ihext=False,iboot=True,ivec=True)"]}, {"block": 29, "type": "markdown", "linesLength": 5, "startIndex": 304, "lines": ["<a id='plots'></a>\n", "\n", "### Downhole Plots\n", "\n", "- Fill in the section summary file in the preliminaries folder and run  the cells below"]}, {"block": 30, "type": "code", "linesLength": 12, "startIndex": 309, "lines": ["dscr_file=\"dscr_measurements.txt\"\n", "if dscr_file:\n", "    srm_dscr_df=pd.read_csv(magic_dir+'/'+dscr_file,sep='\\t',header=1)\n", "    dscr_df=srm_dscr_df.copy(deep=True)\n", "    dscr_df=dscr_df[srm_dscr_df['treat_ac_field']==demag_step]\n", "    depth_data=pd.read_csv(magic_dir+'/lims_sites.txt',sep='\\t',header=1)\n", "    depth_data['specimen']=depth_data['site']\n", "    depth_data=depth_data[['specimen','core_depth']]\n", "    depth_data.sort_values(by='specimen')\n", "    dscr_df=pd.merge(dscr_df,depth_data,on='specimen')\n", "else:\n", "    dscr_df=\"\"\n"]}, {"block": 31, "type": "code", "linesLength": 30, "startIndex": 321, "lines": ["if section_summary_file:\n", "    arch_demag_step=pd.read_csv(hole+'/'+hole+'_arch_demag_step.csv')\n", "    adj_dec_df=pd.read_csv(hole+'/'+hole+'_dec_adjusted.csv')\n", "#Let's get the section boundaries.  \n", "# edit the file name for the Section Summary table downloaded from LIMS\n", "\n", "\n", "    summary_df=pd.read_csv(summary_file)\n", "    summary_df.dropna(subset=['Sect'],inplace=True)\n", "    if type(summary_df.Sect)!='str':\n", "        summary_df.Sect=summary_df.Sect.astype('int64')\n", "        summary_df.Sect=summary_df.Sect.astype('str')\n", "    summary_df=summary_df[summary_df['Sect'].str.contains('CC')==False]\n", "    max_depth=arch_demag_step['core_depth'].max()\n", "\n", "    summary_df=summary_df[summary_df['Top depth CSF-A (m)']<max_depth]\n", "    sect_depths=summary_df['Top depth CSF-A (m)'].values\n", "    summary_df['Core']=summary_df['Core'].astype('int')\n", "    labels=summary_df['Core'].astype('str')+summary_df['Type']+'-'+summary_df['Sect'].astype('str')\n", "\n", "    arch_demag_step=pd.read_csv(hole+'/'+hole+'_arch_demag_step.csv')\n", "    interval=100 # how to divide up the plots\n", "    depth_min,depth_max=0,interval\n", "    fignum=1\n", "    while depth_min<arch_demag_step.core_depth.max():\n", "        iodp_funcs.make_plot(arch_demag_step,adj_dec_df,sect_depths,hole,\\\n", "                         gad_inc,depth_min,depth_max,labels,spec_df=dscr_df,fignum=fignum)\n", "        depth_min+=interval\n", "        depth_max+=interval\n", "        fignum+=1"]}, {"block": 32, "type": "markdown", "linesLength": 5, "startIndex": 351, "lines": ["<a id='upload'></a>\n", "\n", "###  Tidy up for MagC\n", "\n", "Combine all the MagIC files for uploading to MagIC (http://earthref.org/MagIC).  \n"]}, {"block": 33, "type": "code", "linesLength": 6, "startIndex": 356, "lines": ["if False:\n", "    ipmag.combine_magic(spec_files,outfile='specimens.txt',dir_path=magic_dir)\n", "    ipmag.combine_magic(samp_files,outfile='samples.txt',dir_path=magic_dir)\n", "    ipmag.combine_magic(site_files,outfile='sites.txt',dir_path=magic_dir)\n", "    ipmag.combine_magic(meas_files,outfile='measurements.txt',dir_path=magic_dir)\n", "    ipmag.upload_magic()"]}, {"block": 34, "type": "code", "linesLength": 0, "startIndex": 362, "lines": []}]