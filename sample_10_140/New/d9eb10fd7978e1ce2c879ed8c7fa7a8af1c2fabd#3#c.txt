[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["# List of callbacks"]}, {"block": 1, "type": "code", "linesLength": 7, "startIndex": 1, "lines": ["from fastai.gen_doc.nbdoc import *\n", "from fastai.vision import *\n", "from fastai.text import *\n", "from fastai.callbacks import * \n", "from fastai.basic_train import * \n", "from fastai.train import * \n", "from fastai import callbacks"]}, {"block": 2, "type": "markdown", "linesLength": 3, "startIndex": 8, "lines": ["fastai's training loop is highly extensible, with a rich *callback* system. See the [`callback`](/callback.html#callback) docs if you're interested in writing your own callback. See below for a list of callbacks that are provided with fastai, grouped by the module they're defined in.\n", "\n", "Every callback that is passed to [`Learner`](/basic_train.html#Learner) with the `callback_fns` parameter will be automatically stored as an attribute. The attribute name is snake-cased, so for instance [`ActivationStats`](/callbacks.hooks.html#ActivationStats) will appear as `learn.activation_stats` (assuming your object is named `learn`)."]}, {"block": 3, "type": "markdown", "linesLength": 3, "startIndex": 11, "lines": ["## [`Callback`](/callback.html#Callback)\n", "\n", "This sub-package contains more sophisticated callbacks that each are in their own module. They are (click the link for more details):"]}, {"block": 4, "type": "markdown", "linesLength": 3, "startIndex": 14, "lines": ["### [`LRFinder`](/callbacks.lr_finder.html#LRFinder)\n", "\n", "Use Leslie Smith's [learning rate finder](https://www.jeremyjordan.me/nn-learning-rate/) to find a good learning rate for training your model. Let's see an example of use on the MNIST dataset with a simple CNN."]}, {"block": 5, "type": "code", "linesLength": 4, "startIndex": 17, "lines": ["path = untar_data(URLs.MNIST_SAMPLE)\n", "data = ImageDataBunch.from_folder(path)\n", "def simple_learner(): return Learner(data, simple_cnn((3,16,16,2)), metrics=[accuracy])\n", "learn = simple_learner()"]}, {"block": 6, "type": "markdown", "linesLength": 1, "startIndex": 21, "lines": ["The fastai librairy already has a Learner method called [`lr_find`](/train.html#lr_find) that uses [`LRFinder`](/callbacks.lr_finder.html#LRFinder) to plot the loss as a function of the learning rate"]}, {"block": 7, "type": "code", "linesLength": 1, "startIndex": 22, "lines": ["learn.lr_find()"]}, {"block": 8, "type": "code", "linesLength": 1, "startIndex": 23, "lines": ["learn.recorder.plot()"]}, {"block": 9, "type": "markdown", "linesLength": 1, "startIndex": 24, "lines": ["In this example, a learning rate around 2e-2 seems like the right fit."]}, {"block": 10, "type": "code", "linesLength": 1, "startIndex": 25, "lines": ["lr = 2e-2"]}, {"block": 11, "type": "markdown", "linesLength": 3, "startIndex": 26, "lines": ["### [`OneCycleScheduler`](/callbacks.one_cycle.html#OneCycleScheduler)\n", "\n", "Train with Leslie Smith's [1cycle annealing](https://sgugger.github.io/the-1cycle-policy.html) method. Let's train our simple learner using the one cycle policy."]}, {"block": 12, "type": "code", "linesLength": 1, "startIndex": 29, "lines": ["learn.fit_one_cycle(3, lr)"]}, {"block": 13, "type": "markdown", "linesLength": 1, "startIndex": 30, "lines": ["The learning rate and the momentum were changed during the epochs as follows (more info on the [dedicated documentation page](https://docs.fast.ai/callbacks.one_cycle.html))."]}, {"block": 14, "type": "code", "linesLength": 1, "startIndex": 31, "lines": ["learn.recorder.plot_lr(show_moms=True)"]}, {"block": 15, "type": "markdown", "linesLength": 3, "startIndex": 32, "lines": ["### [`MixUpCallback`](/callbacks.mixup.html#MixUpCallback)\n", "\n", "Data augmentation using the method from [mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412). It is very simple to add mixup in fastai :"]}, {"block": 16, "type": "code", "linesLength": 1, "startIndex": 35, "lines": ["learn = Learner(data, simple_cnn((3, 16, 16, 2)), metrics=[accuracy]).mixup()"]}, {"block": 17, "type": "markdown", "linesLength": 3, "startIndex": 36, "lines": ["### [`CSVLogger`](/callbacks.csv_logger.html#CSVLogger)\n", "\n", "Log the results of training in a csv file. Simply pass the CSVLogger callback to the Learner."]}, {"block": 18, "type": "code", "linesLength": 1, "startIndex": 39, "lines": ["learn = Learner(data, simple_cnn((3, 16, 16, 2)), metrics=[accuracy, error_rate], callback_fns=[CSVLogger])"]}, {"block": 19, "type": "code", "linesLength": 1, "startIndex": 40, "lines": ["learn.fit(3)"]}, {"block": 20, "type": "markdown", "linesLength": 1, "startIndex": 41, "lines": ["You can then read the csv."]}, {"block": 21, "type": "code", "linesLength": 1, "startIndex": 42, "lines": ["learn.csv_logger.read_logged_file()"]}, {"block": 22, "type": "markdown", "linesLength": 3, "startIndex": 43, "lines": ["### [`GeneralScheduler`](/callbacks.general_sched.html#GeneralScheduler)\n", "\n", "Create your own multi-stage annealing schemes with a convenient API. To illustrate, let's implement a 2 phase schedule."]}, {"block": 23, "type": "code", "linesLength": 7, "startIndex": 46, "lines": ["def fit_odd_shedule(learn, lr, mom):\n", "    n = len(learn.data.train_dl)\n", "    phases = [TrainingPhase(n, lr, mom, lr_anneal=annealing_cos), TrainingPhase(n*2, lr, mom, lr_anneal=annealing_poly(2))]\n", "    sched = GeneralScheduler(learn, phases)\n", "    learn.callbacks.append(sched)\n", "    total_epochs = 3\n", "    learn.fit(total_epochs)"]}, {"block": 24, "type": "code", "linesLength": 2, "startIndex": 53, "lines": ["learn = Learner(data, simple_cnn((3,16,16,2)), metrics=accuracy)\n", "fit_odd_shedule(learn, 1e-3, 0.9)"]}, {"block": 25, "type": "code", "linesLength": 1, "startIndex": 55, "lines": ["learn.recorder.plot_lr()"]}, {"block": 26, "type": "markdown", "linesLength": 3, "startIndex": 56, "lines": ["### [`MixedPrecision`](/callbacks.fp16.html#MixedPrecision)\n", "\n", "Use fp16 to [take advantage of tensor cores](https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html) on recent NVIDIA GPUs for a 200% or more speedup."]}, {"block": 27, "type": "markdown", "linesLength": 3, "startIndex": 59, "lines": ["### [`HookCallback`](/callbacks.hooks.html#HookCallback)\n", "\n", "Convenient wrapper for registering and automatically deregistering [PyTorch hooks](https://pytorch.org/tutorials/beginner/former_torchies/nn_tutorial.html#forward-and-backward-function-hooks). Also contains pre-defined hook callback: [`ActivationStats`](/callbacks.hooks.html#ActivationStats)."]}, {"block": 28, "type": "markdown", "linesLength": 3, "startIndex": 62, "lines": ["### [`RNNTrainer`](/callbacks.rnn.html#RNNTrainer)\n", "\n", "Callback taking care of all the tweaks to train an RNN."]}, {"block": 29, "type": "markdown", "linesLength": 3, "startIndex": 65, "lines": ["### [`TerminateOnNaNCallback`](/callbacks.tracker.html#TerminateOnNaNCallback)\n", "\n", "Stop training if the loss reaches NaN."]}, {"block": 30, "type": "markdown", "linesLength": 3, "startIndex": 68, "lines": ["### [`EarlyStoppingCallback`](/callbacks.tracker.html#EarlyStoppingCallback)\n", "\n", "Stop training if a given metric/validation loss doesn't improve."]}, {"block": 31, "type": "markdown", "linesLength": 3, "startIndex": 71, "lines": ["### [`SaveModelCallback`](/callbacks.tracker.html#SaveModelCallback)\n", "\n", "Save the model at every epoch, or the best model for a given metric/validation loss."]}, {"block": 32, "type": "markdown", "linesLength": 3, "startIndex": 74, "lines": ["### [`ReduceLROnPlateauCallback`](/callbacks.tracker.html#ReduceLROnPlateauCallback)\n", "\n", "Reduce the learning rate each time a given metric/validation loss doesn't improve by a certain factor."]}, {"block": 33, "type": "markdown", "linesLength": 1, "startIndex": 77, "lines": ["## [`train`](/train.html#train) and [`basic_train`](/basic_train.html#basic_train)"]}, {"block": 34, "type": "markdown", "linesLength": 3, "startIndex": 78, "lines": ["### [`Recorder`](/basic_train.html#Recorder)\n", "\n", "Track per-batch and per-epoch smoothed losses and metrics."]}, {"block": 35, "type": "markdown", "linesLength": 3, "startIndex": 81, "lines": ["### [`ShowGraph`](/train.html#ShowGraph)\n", "\n", "Dynamically display a learning chart during training."]}, {"block": 36, "type": "markdown", "linesLength": 3, "startIndex": 84, "lines": ["### [`BnFreeze`](/train.html#BnFreeze)\n", "\n", "Freeze batchnorm layer moving average statistics for non-trainable layers."]}, {"block": 37, "type": "markdown", "linesLength": 3, "startIndex": 87, "lines": ["### [`GradientClipping`](/train.html#GradientClipping)\n", "\n", "Clips gradient during training."]}, {"block": 38, "type": "code", "linesLength": 0, "startIndex": 90, "lines": []}]