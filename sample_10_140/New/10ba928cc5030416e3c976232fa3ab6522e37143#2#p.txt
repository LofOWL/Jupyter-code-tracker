[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["# Classes for callback implementors"]}, {"block": 1, "type": "code", "linesLength": 3, "startIndex": 1, "lines": ["from fastai.gen_doc.nbdoc import *\n", "from fastai.callback import * \n", "from fastai import *"]}, {"block": 2, "type": "markdown", "linesLength": 5, "startIndex": 4, "lines": ["fastai provides a powerful *callback* system, which is documented on the [`callbacks`](/callbacks.html#callbacks) page; look on that page if you're just looking for how to use existing callbacks. If you want to create your own, you'll need to use the classes discussed below.\n", "\n", "A key motivation for the callback system is that additional functionality can be entirely implemented in a single callback, so that it's easily read. By using this trick, we will have different methods categorized in different callbacks where we will find clearly stated all the interventions the method makes in training. For instance in the [`LRFinder`](/callbacks.lr_finder.html#LRFinder) callback, on top of running the fit function with exponentially growing LRs, it needs to handle some preparation and clean-up, and all this code can be in the same callback so we know exactly what it is doing and where to look if we need to change something.\n", "\n", "In addition, it allows our [`fit`](/basic_train.html#fit) function to be very clean and simple, yet still easily extended. So far in implementing a number of recent papers, we haven't yet come across any situation where we had to modify our training loop source code - we've been able to use callbacks every time."]}, {"block": 3, "type": "code", "linesLength": 1, "startIndex": 9, "lines": ["show_doc(Callback)"]}, {"block": 4, "type": "markdown", "linesLength": 10, "startIndex": 10, "lines": ["To create a new type of callback, you'll need to inherit from this class, and implement one or more methods as required for your purposes. Perhaps the easiest way to get started is to look at the source code for some of the pre-defined fastai callbacks. You might be surprised at how simple they are! For instance, here is the **entire** source code for [`GradientClipping`](/train.html#GradientClipping):\n", "\n", "```python\n", "@dataclass\n", "class GradientClipping(LearnerCallback):\n", "    clip:float\n", "    def on_backward_end(self, **kwargs):\n", "        if self.clip:\n", "            nn.utils.clip_grad_norm_(self.learn.model.parameters(), self.clip)\n", "```"]}, {"block": 5, "type": "markdown", "linesLength": 12, "startIndex": 20, "lines": ["You generally want your custom callback constructor to take a [`Learner`](/basic_train.html#Learner) parameter, e.g.:\n", "\n", "```python\n", "@dataclass\n", "class MyCallback(Callback):\n", "    learn:Learner\n", "```\n", "\n", "Note that this allows the callback user to just pass your callback name to `callback_fns` when constructing their [`Learner`](/basic_train.html#Learner), since that always passes `self` when constructing callbacks from `callback_fns`. In addition, by passing the learner, this callback will have access to everything: e.g all the inputs/outputs as they are calculated, the losses, and also the data loaders, the optimizer, etc. At any time:\n", "- Changing self.learn.data.train_dl or self.data.valid_dl will change them inside the fit function (we just need to pass the [`DataBunch`](/basic_data.html#DataBunch) object to the fit function and not data.train_dl/data.valid_dl)\n", "- Changing self.learn.opt.opt (We have an [`OptimWrapper`](/callback.html#OptimWrapper) on top of the actual optimizer) will change it inside the fit function.\n", "- Changing self.learn.data or self.learn.opt directly WILL NOT change the data or the optimizer inside the fit function."]}, {"block": 6, "type": "markdown", "linesLength": 12, "startIndex": 32, "lines": ["In any of the callbacks you can unpack in the kwargs:\n", "- `n_epochs`, contains the number of epochs the training will take in total\n", "- `epoch`, contains the number of the current\n", "- `iteration`, contains the number of iterations done since the beginning of training\n", "- `num_batch`, contains the number of the batch we're at in the dataloader\n", "- `last_input`, contains the last input that got through the model (eventually updated by a callback)\n", "- `last_target`, contains the last target that gor through the model (eventually updated by a callback)\n", "- `last_output`, contains the last output spitted by the model (eventually updated by a callback)\n", "- `last_loss`, contains the last loss computed (eventually updated by a callback)\n", "- `smooth_loss`, contains the smoothed version of the loss\n", "- `last_metrics`, contains the last validation loss and emtrics computed\n", "- `pbar`, the progress bar"]}, {"block": 7, "type": "markdown", "linesLength": 1, "startIndex": 44, "lines": ["### Methods your subclass can implement"]}, {"block": 8, "type": "markdown", "linesLength": 1, "startIndex": 45, "lines": ["All of these methods are optional; your subclass can handle as many or as few as you require."]}, {"block": 9, "type": "code", "linesLength": 1, "startIndex": 46, "lines": ["show_doc(Callback.on_train_begin)"]}, {"block": 10, "type": "markdown", "linesLength": 9, "startIndex": 47, "lines": ["Here we can initiliaze anything we need. \n", "The optimizer has now been initialized. We can change any hyper-parameters by typing, for instance:\n", "\n", "```\n", "self.opt.lr = new_lr\n", "self.opt.mom = new_mom\n", "self.opt.wd = new_wd\n", "self.opt.beta = new_beta\n", "```"]}, {"block": 11, "type": "code", "linesLength": 1, "startIndex": 56, "lines": ["show_doc(Callback.on_epoch_begin)"]}, {"block": 12, "type": "markdown", "linesLength": 2, "startIndex": 57, "lines": ["This is not technically required since we have `on_train_begin` for epoch 0 and `on_epoch_end` for all the other epochs,\n", "yet it makes writing code that needs to be done at the beginning of every epoch easy and more readable."]}, {"block": 13, "type": "code", "linesLength": 1, "startIndex": 59, "lines": ["show_doc(Callback.on_batch_begin)"]}, {"block": 14, "type": "markdown", "linesLength": 4, "startIndex": 60, "lines": ["Here is the perfect place to prepare everything before the model is called.\n", "Example: change the values of the hyperparameters (if we don't do it on_batch_end instead)\n", "\n", "If we return something, that will be the new value for `xb`,`yb`. "]}, {"block": 15, "type": "code", "linesLength": 1, "startIndex": 64, "lines": ["show_doc(Callback.on_loss_begin)"]}, {"block": 16, "type": "markdown", "linesLength": 5, "startIndex": 65, "lines": ["Here is the place to run some code that needs to be executed after the output has been computed but before the\n", "loss computation.\n", "Example: putting the output back in FP32 when training in mixed precision.\n", "\n", "If we return something, that will be the new value for the output."]}, {"block": 17, "type": "code", "linesLength": 1, "startIndex": 70, "lines": ["show_doc(Callback.on_backward_begin)"]}, {"block": 18, "type": "markdown", "linesLength": 5, "startIndex": 71, "lines": ["Here is the place to run some code that needs to be executed after the loss has been computed but before the gradient computation.\n", "Example: `reg_fn` in RNNs.\n", "\n", "If we return something, that will be the new value for loss. Since the recorder is always called first,\n", "it will have the raw loss."]}, {"block": 19, "type": "code", "linesLength": 1, "startIndex": 76, "lines": ["show_doc(Callback.on_backward_end)"]}, {"block": 20, "type": "markdown", "linesLength": 2, "startIndex": 77, "lines": ["Here is the place to run some code that needs to be executed after the gradients have been computed but\n", "before the optimizer is called."]}, {"block": 21, "type": "code", "linesLength": 1, "startIndex": 79, "lines": ["show_doc(Callback.on_step_end)"]}, {"block": 22, "type": "markdown", "linesLength": 2, "startIndex": 80, "lines": ["Here is the place to run some code that needs to be executed after the optimizer step but before the gradients\n", "are zeroed"]}, {"block": 23, "type": "code", "linesLength": 1, "startIndex": 82, "lines": ["show_doc(Callback.on_batch_end)"]}, {"block": 24, "type": "markdown", "linesLength": 4, "startIndex": 83, "lines": ["Here is the place to run some code that needs to be executed after a batch is fully done.\n", "Example: change the values of the hyperparameters (if we don't do it on_batch_begin instead)\n", "\n", "If we return true, the current epoch is interrupted (example: lr_finder stops the training when the loss explodes)"]}, {"block": 25, "type": "code", "linesLength": 1, "startIndex": 87, "lines": ["show_doc(Callback.on_epoch_end)"]}, {"block": 26, "type": "markdown", "linesLength": 4, "startIndex": 88, "lines": ["Here is the place to run some code that needs to be executed at the end of an epoch.\n", "Example: Save the model if we have a new best validation loss/metric.\n", "\n", "If we return true, the training stops (example: early stopping)"]}, {"block": 27, "type": "code", "linesLength": 1, "startIndex": 92, "lines": ["show_doc(Callback.on_train_end)"]}, {"block": 28, "type": "markdown", "linesLength": 3, "startIndex": 93, "lines": ["Here is the place to tidy everything. It's always executed even if there was an error during the training loop,\n", "and has an extra kwarg named exception to check if there was an exception or not.\n", "Examples: save log_files, load best model found during training"]}, {"block": 29, "type": "markdown", "linesLength": 1, "startIndex": 96, "lines": ["## Annealing functions"]}, {"block": 30, "type": "markdown", "linesLength": 1, "startIndex": 97, "lines": ["The following functions provide different annealing schedules. You probably won't need to call them directly, but would instead use them as part of a callback. Here's what each one looks like:"]}, {"block": 31, "type": "code", "linesLength": 6, "startIndex": 98, "lines": ["annealings = \"NO LINEAR COS EXP POLY\".split()\n", "fns = [annealing_no, annealing_linear, annealing_cos, annealing_exp, annealing_poly(0.8)]\n", "for fn, t in zip(fns, annealings):\n", "    plt.plot(np.arange(0, 100), [fn(2, 1e-2, o)\n", "        for o in np.linspace(0.01,1,100)], label=t)\n", "plt.legend();"]}, {"block": 32, "type": "code", "linesLength": 1, "startIndex": 104, "lines": ["show_doc(annealing_cos)"]}, {"block": 33, "type": "code", "linesLength": 1, "startIndex": 105, "lines": ["show_doc(annealing_exp)"]}, {"block": 34, "type": "code", "linesLength": 1, "startIndex": 106, "lines": ["show_doc(annealing_linear)"]}, {"block": 35, "type": "code", "linesLength": 1, "startIndex": 107, "lines": ["show_doc(annealing_no)"]}, {"block": 36, "type": "code", "linesLength": 1, "startIndex": 108, "lines": ["show_doc(annealing_poly)"]}, {"block": 37, "type": "code", "linesLength": 1, "startIndex": 109, "lines": ["show_doc(CallbackHandler)"]}, {"block": 38, "type": "markdown", "linesLength": 1, "startIndex": 110, "lines": ["You probably won't need to use this class yourself. It's used by fastai to combine all the callbacks together and call any relevant callback functions for each training stage. The methods below simply call the equivalent method in each callback function in [`self.callbacks`](/callbacks.html#callbacks). "]}, {"block": 39, "type": "code", "linesLength": 1, "startIndex": 111, "lines": ["show_doc(CallbackHandler.on_backward_begin)"]}, {"block": 40, "type": "code", "linesLength": 1, "startIndex": 112, "lines": ["show_doc(CallbackHandler.on_backward_end)"]}, {"block": 41, "type": "code", "linesLength": 1, "startIndex": 113, "lines": ["show_doc(CallbackHandler.on_batch_begin)"]}, {"block": 42, "type": "code", "linesLength": 1, "startIndex": 114, "lines": ["show_doc(CallbackHandler.on_batch_end)"]}, {"block": 43, "type": "code", "linesLength": 1, "startIndex": 115, "lines": ["show_doc(CallbackHandler.on_epoch_begin)"]}, {"block": 44, "type": "code", "linesLength": 1, "startIndex": 116, "lines": ["show_doc(CallbackHandler.on_epoch_end)"]}, {"block": 45, "type": "code", "linesLength": 1, "startIndex": 117, "lines": ["show_doc(CallbackHandler.on_loss_begin)"]}, {"block": 46, "type": "code", "linesLength": 1, "startIndex": 118, "lines": ["show_doc(CallbackHandler.on_step_end)"]}, {"block": 47, "type": "code", "linesLength": 1, "startIndex": 119, "lines": ["show_doc(CallbackHandler.on_train_begin)"]}, {"block": 48, "type": "code", "linesLength": 1, "startIndex": 120, "lines": ["show_doc(CallbackHandler.on_train_end)"]}, {"block": 49, "type": "code", "linesLength": 1, "startIndex": 121, "lines": ["show_doc(OptimWrapper)"]}, {"block": 50, "type": "markdown", "linesLength": 3, "startIndex": 122, "lines": ["This is a convenience class that provides a consistent API for getting and setting optimizer hyperparameters. For instance, for [`optim.Adam`](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) the momentum parameter is actually `betas[0]`, whereas for [`optim.SGD`](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) it's simply `momentum`. As another example, the details of handling weight decay depend on whether you are using `true_wd` or the traditional L2 regularization approach.\n", "\n", "This class also handles setting different WD and LR for each layer group, for discriminative layer training."]}, {"block": 51, "type": "code", "linesLength": 1, "startIndex": 125, "lines": ["show_doc(OptimWrapper.create)"]}, {"block": 52, "type": "code", "linesLength": 1, "startIndex": 126, "lines": ["show_doc(OptimWrapper.new)"]}, {"block": 53, "type": "code", "linesLength": 1, "startIndex": 127, "lines": ["show_doc(OptimWrapper.read_defaults)"]}, {"block": 54, "type": "code", "linesLength": 1, "startIndex": 128, "lines": ["show_doc(OptimWrapper.read_val)"]}, {"block": 55, "type": "code", "linesLength": 1, "startIndex": 129, "lines": ["show_doc(OptimWrapper.set_val)"]}, {"block": 56, "type": "code", "linesLength": 1, "startIndex": 130, "lines": ["show_doc(OptimWrapper.step)"]}, {"block": 57, "type": "code", "linesLength": 1, "startIndex": 131, "lines": ["show_doc(OptimWrapper.zero_grad)"]}, {"block": 58, "type": "code", "linesLength": 1, "startIndex": 132, "lines": ["show_doc(SmoothenValue)"]}, {"block": 59, "type": "markdown", "linesLength": 1, "startIndex": 133, "lines": ["Used for smoothing loss in [`Recorder`](/basic_train.html#Recorder)."]}, {"block": 60, "type": "code", "linesLength": 1, "startIndex": 134, "lines": ["show_doc(SmoothenValue.add_value)"]}, {"block": 61, "type": "code", "linesLength": 1, "startIndex": 135, "lines": ["show_doc(Stepper)"]}, {"block": 62, "type": "markdown", "linesLength": 1, "startIndex": 136, "lines": ["Used for creating annealing schedules, mainly for [`OneCycleScheduler`](/callbacks.one_cycle.html#OneCycleScheduler)."]}, {"block": 63, "type": "code", "linesLength": 1, "startIndex": 137, "lines": ["show_doc(Stepper.step)"]}, {"block": 64, "type": "code", "linesLength": 1, "startIndex": 138, "lines": ["show_doc(AverageMetric)"]}, {"block": 65, "type": "markdown", "linesLength": 1, "startIndex": 139, "lines": ["See the documentation on [`metrics`](/metrics.html#metrics) for more information."]}, {"block": 66, "type": "markdown", "linesLength": 1, "startIndex": 140, "lines": ["## Undocumented Methods - Methods moved below this line will intentionally be hidden"]}, {"block": 67, "type": "code", "linesLength": 1, "startIndex": 141, "lines": ["show_doc(AverageMetric.on_epoch_begin)"]}, {"block": 68, "type": "code", "linesLength": 1, "startIndex": 142, "lines": ["show_doc(AverageMetric.on_batch_end)"]}, {"block": 69, "type": "code", "linesLength": 1, "startIndex": 143, "lines": ["show_doc(AverageMetric.on_epoch_end)"]}, {"block": 70, "type": "markdown", "linesLength": 1, "startIndex": 144, "lines": ["## New Methods - Please document or move to the undocumented section"]}]