[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["# Intro to Intro to Machine Learning"]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 1, "lines": ["## Imports"]}, {"block": 2, "type": "code", "linesLength": 4, "startIndex": 2, "lines": ["%load_ext autoreload\n", "%autoreload 2\n", "\n", "%matplotlib inline"]}, {"block": 3, "type": "code", "linesLength": 8, "startIndex": 6, "lines": ["from fastai.imports import *\n", "from fastai.structured import *\n", "\n", "from pandas_summary import DataFrameSummary\n", "from sklearn.ensemble import RandomForestRegressor\n", "from IPython.display import display\n", "\n", "from sklearn import metrics"]}, {"block": 4, "type": "code", "linesLength": 1, "startIndex": 14, "lines": ["path = \"data/bulldozers/\""]}, {"block": 5, "type": "markdown", "linesLength": 1, "startIndex": 15, "lines": ["## Introduction to *Blue Book for Bulldozers*"]}, {"block": 6, "type": "markdown", "linesLength": 1, "startIndex": 16, "lines": ["### About our teaching"]}, {"block": 7, "type": "markdown", "linesLength": 1, "startIndex": 17, "lines": ["At fast.ai we have a distinctive [teaching philosophy](http://www.fast.ai/2016/10/08/teaching-philosophy/) of [\"the whole game\"](https://www.amazon.com/Making-Learning-Whole-Principles-Transform/dp/0470633719/ref=sr_1_1?ie=UTF8&qid=1505094653).  This is different from how most traditional math & technical courses are taught, where you have to learn all the individual elements before you can combine them (Harvard professor David Perkins call this *elementitis*), but it is similar to how topics like *driving* and *baseball* are taught.  That is, you can start driving without [knowing how an internal combustion engine works](https://medium.com/towards-data-science/thoughts-after-taking-the-deeplearning-ai-courses-8568f132153), and children begin playing baseball before they learn all the formal rules."]}, {"block": 8, "type": "markdown", "linesLength": 1, "startIndex": 18, "lines": ["### About this dataset"]}, {"block": 9, "type": "markdown", "linesLength": 3, "startIndex": 19, "lines": ["We will be looking at the Blue Book for Bulldozers Kaggle Competition: \"The goal of the contest is to predict the sale price of a particular piece of heavy equiment at auction based on it's usage, equipment type, and configuaration.  The data is sourced from auction result postings and includes information on usage and equipment configurations.\"\n", "\n", "This is a very common type of dataset and prediciton problem, and similar to what you may see in your project or workplace."]}, {"block": 10, "type": "markdown", "linesLength": 1, "startIndex": 22, "lines": ["### About Kaggle Competitions"]}, {"block": 11, "type": "markdown", "linesLength": 8, "startIndex": 23, "lines": ["Kaggle is an awesome resource for aspiring data scientists or anyone looking to improve their machine learning skills.  There is nothing like being able to get hands-on practice and receiving real-time feedback to help you improve your skills.\n", "\n", "Kaggle provides:\n", "\n", "1. Interesting data sets\n", "2. Feedback on how you're doing\n", "3. A leader board to see what's good, what's possible, and what's state-of-art.\n", "4. Blog posts by winning contestants share useful tips and techniques."]}, {"block": 12, "type": "markdown", "linesLength": 1, "startIndex": 31, "lines": ["## The data"]}, {"block": 13, "type": "markdown", "linesLength": 1, "startIndex": 32, "lines": ["### Look at the data"]}, {"block": 14, "type": "markdown", "linesLength": 14, "startIndex": 33, "lines": ["Kaggle provides info about some of the fields of our dataset; on the [Kaggle Data info](https://www.kaggle.com/c/bluebook-for-bulldozers/data) page they say the following:\n", "\n", "For this competition, you are predicting the sale price of bulldozers sold at auctions. The data for this competition is split into three parts:\n", "\n", "- **Train.csv** is the training set, which contains data through the end of 2011.\n", "- **Valid.csv** is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public leaderboard.\n", "- **Test.csv** is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition.\n", "\n", "The key fields are in train.csv are:\n", "\n", "- SalesID: the uniue identifier of the sale\n", "- MachineID: the unique identifier of a machine.  A machine can be sold multiple times\n", "- saleprice: what the machine sold for at auction (only provided in train.csv)\n", "- saledate: the date of the sale"]}, {"block": 15, "type": "markdown", "linesLength": 3, "startIndex": 47, "lines": ["*Question*\n", "\n", "What stands out to you from the above description?  What needs to be true of our training and validation sets?"]}, {"block": 16, "type": "code", "linesLength": 1, "startIndex": 50, "lines": ["df_raw = pd.read_csv(f'{path}Train.csv', low_memory=False, parse_dates=[\"saledate\"])"]}, {"block": 17, "type": "markdown", "linesLength": 1, "startIndex": 51, "lines": ["In any sort of data science work, it's **important to look at your data**, to make sure you understand the format, how it's stored, what type of values it holds, etc. Even if you've read descriptions about your data, the actual data may not be what you expect."]}, {"block": 18, "type": "code", "linesLength": 4, "startIndex": 52, "lines": ["def display_all(df):\n", "    with pd.option_context(\"display.max_rows\", 1000): \n", "        with pd.option_context(\"display.max_columns\", 1000): \n", "            display(df)"]}, {"block": 19, "type": "code", "linesLength": 1, "startIndex": 56, "lines": ["display_all(df_raw.tail().transpose())"]}, {"block": 20, "type": "code", "linesLength": 1, "startIndex": 57, "lines": ["display_all(df_raw.describe(include='all').transpose())"]}, {"block": 21, "type": "markdown", "linesLength": 1, "startIndex": 58, "lines": ["### Initial processing"]}, {"block": 22, "type": "code", "linesLength": 2, "startIndex": 59, "lines": ["m = RandomForestRegressor(n_jobs=-1)\n", "m.fit(df_raw.drop('SalePrice', axis=1), df_join.SalePrice)"]}, {"block": 23, "type": "markdown", "linesLength": 3, "startIndex": 61, "lines": ["This dataset contains a mix of **continuous** and **categorical** variables.\n", "\n", "The following method extracts particular date fields from a complete datetime for the purpose of constructing categoricals.  You should always consider this feature extraction step when working with date-time. Without expanding your date-time into these additional fields, you can't capture any trend/cyclical behavior as a function of time at any of these granularities."]}, {"block": 24, "type": "code", "linesLength": 2, "startIndex": 64, "lines": ["add_datepart(df_raw, 'saledate')\n", "df_raw.saleYear.head()"]}, {"block": 25, "type": "markdown", "linesLength": 1, "startIndex": 66, "lines": ["The categorical variables are currently stored as strings, which is inefficient, and doesn't provide the numeric coding required for a random forest. Therefore we call `train_cats` to convert strings to pandas categories."]}, {"block": 26, "type": "code", "linesLength": 1, "startIndex": 67, "lines": ["train_cats(df_raw)"]}, {"block": 27, "type": "markdown", "linesLength": 1, "startIndex": 68, "lines": ["We can specify the order to use for categorical variables if we wish:"]}, {"block": 28, "type": "code", "linesLength": 1, "startIndex": 69, "lines": ["df_raw.UsageBand.cat.categories"]}, {"block": 29, "type": "code", "linesLength": 1, "startIndex": 70, "lines": ["df_raw.UsageBand.cat.set_categories(['High', 'Medium', 'Low'], ordered=True, inplace=True)"]}, {"block": 30, "type": "markdown", "linesLength": 1, "startIndex": 71, "lines": ["We're still not quite done - for instance we have lots of missing values, wish we can't pass directly to a random forest."]}, {"block": 31, "type": "code", "linesLength": 1, "startIndex": 72, "lines": ["display_all(df_raw.isnull().sum().sort_index()/len(df_raw))"]}, {"block": 32, "type": "markdown", "linesLength": 1, "startIndex": 73, "lines": ["But let's save this file for now, since it's already in format can we be stored and accessed efficiently."]}, {"block": 33, "type": "code", "linesLength": 2, "startIndex": 74, "lines": ["os.makedirs('tmp', exist_ok=True)\n", "df_raw.to_feather('tmp/raw')"]}, {"block": 34, "type": "markdown", "linesLength": 1, "startIndex": 76, "lines": ["### Pre-processing"]}, {"block": 35, "type": "markdown", "linesLength": 1, "startIndex": 77, "lines": ["In the future we can simply read it from this fast format."]}, {"block": 36, "type": "code", "linesLength": 1, "startIndex": 78, "lines": ["df_raw = pd.read_feather('tmp/raw')"]}, {"block": 37, "type": "markdown", "linesLength": 1, "startIndex": 79, "lines": ["We'll replace categories with their numeric codes, missing continuous values with a value higher than the column max, and split the dependent variable into a separate variable."]}, {"block": 38, "type": "code", "linesLength": 1, "startIndex": 80, "lines": ["df_trn, y_trn = proc_df(df_raw, 'SalePrice')"]}, {"block": 39, "type": "markdown", "linesLength": 14, "startIndex": 81, "lines": ["Possibly **the most important idea** in machine learning is that of having separate training & validation data sets.\n", "\n", "As motivation, suppose you don't divide up your data, but instead use all of it.  And suppose you have lots of parameters:\n", "\n", "This is called over-fitting.  A validation set helps prevent this problem.\n", "\n", "<img src=\"images/overfitting2.png\" alt=\"\" style=\"width: 70%\"/>\n", "<center>\n", "[Underfitting and Overfitting](https://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted)\n", "</center>\n", "\n", "The error for the pictured data points is lowest for the model on the far right (the blue curve passes through the red points almost perfectly), yet it's not the best choice.  Why is that?  If you were to gather some new data points, they most likely would not be on that curve in the graph on the right, but would be closer to the curve in the middle graph.\n", "\n", "This illustrates how using all our data can lead to **overfitting**."]}, {"block": 40, "type": "code", "linesLength": 8, "startIndex": 95, "lines": ["def split_vals(a,n): return a[:n], a[n:]\n", "\n", "n_valid = 12000  # same as Kaggle's test set size\n", "n_trn = len(df_trn)-n_valid\n", "X_train, X_valid = split_vals(df_trn.values, n_trn)\n", "y_train, y_valid = split_vals(y_trn, n_trn)\n", "\n", "X_train.shape, y_train.shape, X_valid.shape"]}, {"block": 41, "type": "markdown", "linesLength": 1, "startIndex": 103, "lines": ["## Random Forests"]}, {"block": 42, "type": "markdown", "linesLength": 1, "startIndex": 104, "lines": ["### First model"]}, {"block": 43, "type": "markdown", "linesLength": 1, "startIndex": 105, "lines": ["We now have something we can pass to a random forest!"]}, {"block": 44, "type": "code", "linesLength": 4, "startIndex": 106, "lines": ["def print_score(m):\n", "    res = [m.score(X_train, y_train), m.score(X_valid, y_valid)]\n", "    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n", "    print(res)"]}, {"block": 45, "type": "code", "linesLength": 3, "startIndex": 110, "lines": ["m = RandomForestRegressor(n_jobs=-1)\n", "%time m.fit(X_train, y_train)\n", "print_score(m)"]}, {"block": 46, "type": "markdown", "linesLength": 1, "startIndex": 113, "lines": ["An r^2 in the high-80's isn't too bad, but we can see from the validation set score that we're over-fitting badly. To understand this issue, let's simplify things down to a single small tree."]}, {"block": 47, "type": "markdown", "linesLength": 1, "startIndex": 114, "lines": ["*todo* define r^2"]}, {"block": 48, "type": "markdown", "linesLength": 1, "startIndex": 115, "lines": ["### Speeding things up"]}, {"block": 49, "type": "code", "linesLength": 3, "startIndex": 116, "lines": ["df_trn, y_trn = proc_df(df_raw, 'SalePrice', subset=30000)\n", "X_train, _ = split_vals(df_trn.values, 20000)\n", "y_train, _ = split_vals(y_trn, 20000)"]}, {"block": 50, "type": "code", "linesLength": 3, "startIndex": 119, "lines": ["m = RandomForestRegressor(n_jobs=-1)\n", "%time m.fit(X_train, y_train)\n", "print_score(m)"]}, {"block": 51, "type": "markdown", "linesLength": 1, "startIndex": 122, "lines": ["### Single tree"]}, {"block": 52, "type": "code", "linesLength": 3, "startIndex": 123, "lines": ["m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)\n", "m.fit(X_train, y_train)\n", "print_score(m)"]}, {"block": 53, "type": "code", "linesLength": 1, "startIndex": 126, "lines": ["draw_tree(m.estimators_[0], df_trn)"]}, {"block": 54, "type": "markdown", "linesLength": 1, "startIndex": 127, "lines": ["Let's see what happens if we create a bigger tree."]}, {"block": 55, "type": "code", "linesLength": 3, "startIndex": 128, "lines": ["m = RandomForestRegressor(n_estimators=1, bootstrap=False, n_jobs=-1)\n", "m.fit(X_train, y_train)\n", "print_score(m)"]}, {"block": 56, "type": "markdown", "linesLength": 1, "startIndex": 131, "lines": ["The training set result looks great! But the validation set is worse than our original model. This is why we need to use *bagging* of multiple trees to get more generalizable results."]}, {"block": 57, "type": "markdown", "linesLength": 1, "startIndex": 132, "lines": ["### Bagging"]}, {"block": 58, "type": "markdown", "linesLength": 1, "startIndex": 133, "lines": ["To learn about bagging in random forests, let's start with our basic model again."]}, {"block": 59, "type": "code", "linesLength": 3, "startIndex": 134, "lines": ["m = RandomForestRegressor(n_jobs=-1)\n", "m.fit(X_train, y_train)\n", "print_score(m)"]}, {"block": 60, "type": "markdown", "linesLength": 1, "startIndex": 137, "lines": ["We'll grab the predictions for each individual tree, and look at one example."]}, {"block": 61, "type": "code", "linesLength": 2, "startIndex": 138, "lines": ["preds = np.stack([t.predict(X_valid) for t in m.estimators_])\n", "preds[:,0], np.mean(preds[:,0]), y_valid[0]"]}, {"block": 62, "type": "code", "linesLength": 1, "startIndex": 140, "lines": ["plt.plot([metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(10)]);"]}, {"block": 63, "type": "markdown", "linesLength": 1, "startIndex": 141, "lines": ["The shape of this curve suggests that adding more trees isn't going to help us much. Let's check. (Compare this to our original model on a sample)"]}, {"block": 64, "type": "code", "linesLength": 3, "startIndex": 142, "lines": ["m = RandomForestRegressor(n_estimators=20, n_jobs=-1)\n", "m.fit(X_train, y_train)\n", "print_score(m)"]}, {"block": 65, "type": "code", "linesLength": 3, "startIndex": 145, "lines": ["m = RandomForestRegressor(n_estimators=40, n_jobs=-1)\n", "m.fit(X_train, y_train)\n", "print_score(m)"]}, {"block": 66, "type": "markdown", "linesLength": 1, "startIndex": 148, "lines": ["### Subsampling"]}, {"block": 67, "type": "markdown", "linesLength": 1, "startIndex": 149, "lines": ["Let's return to using our full dataset, to try a better way to speed up our analysis."]}, {"block": 68, "type": "code", "linesLength": 3, "startIndex": 150, "lines": ["df_trn, y_trn = proc_df(df_raw, 'SalePrice')\n", "X_train, X_valid = split_vals(df_trn.values, n_trn)\n", "y_train, y_valid = split_vals(y_trn, n_trn)"]}, {"block": 69, "type": "markdown", "linesLength": 1, "startIndex": 153, "lines": ["Rather than limit the total amount of data that our model can access, let's instead limit it to a *different* random subset per tree. That way, given enough trees, the model can still see *all* the data, but for each individual tree it'll be just as fast as if we had cut down our dataset as before."]}, {"block": 70, "type": "code", "linesLength": 1, "startIndex": 154, "lines": ["set_rf_samples(20000)"]}, {"block": 71, "type": "code", "linesLength": 3, "startIndex": 155, "lines": ["m = RandomForestRegressor(n_jobs=-1)\n", "%time m.fit(X_train, y_train)\n", "print_score(m)"]}, {"block": 72, "type": "markdown", "linesLength": 1, "startIndex": 158, "lines": ["Since each additional tree allows the model to see more data, this approach can make additional trees more useful."]}, {"block": 73, "type": "code", "linesLength": 3, "startIndex": 159, "lines": ["m = RandomForestRegressor(n_estimators=40, n_jobs=-1)\n", "m.fit(X_train, y_train)\n", "print_score(m)"]}, {"block": 74, "type": "markdown", "linesLength": 1, "startIndex": 162, "lines": ["### Out-of-bag (OOB) score"]}, {"block": 75, "type": "markdown", "linesLength": 7, "startIndex": 163, "lines": ["Is our validation set worse than our training set because we're over-fitting, or because the validation set is for a different time period, or a bit of both? With the existing information we've shown, we can't tell. However, random forests have a very clever trick called *out-of-bag (OOB) error* which can handle this (and more!)\n", "\n", "The idea is to calculate error on the training set, but only include the trees in the calculation of a row's error where that row was *not* included in training that tree. This allows us to see whether the model is over-fitting, without needing a separate validation set.\n", "\n", "This also has the benefit of allowing us to see whether our model generalizes, even if we only have a small amount of data so want to avoid separating some out to create a validation set.\n", "\n", "This is as simple as adding one more parameter to our model constructor. We print the OOB error last in our `print_score` function below."]}, {"block": 76, "type": "code", "linesLength": 3, "startIndex": 170, "lines": ["m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n", "m.fit(X_train, y_train)\n", "print_score(m)"]}, {"block": 77, "type": "markdown", "linesLength": 1, "startIndex": 173, "lines": ["This shows that our validation set time difference is making an impact, as is model over-fitting."]}, {"block": 78, "type": "markdown", "linesLength": 1, "startIndex": 174, "lines": ["### Reducing over-fitting"]}, {"block": 79, "type": "markdown", "linesLength": 1, "startIndex": 175, "lines": ["We revert to using a full bootstrap sample in order to show the impact of other over-fitting avoidance methods."]}, {"block": 80, "type": "code", "linesLength": 1, "startIndex": 176, "lines": ["reset_rf_samples()"]}, {"block": 81, "type": "markdown", "linesLength": 1, "startIndex": 177, "lines": ["Let's get a baseline for this full set to compare to."]}, {"block": 82, "type": "code", "linesLength": 3, "startIndex": 178, "lines": ["m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n", "m.fit(X_train, y_train)\n", "print_score(m)"]}, {"block": 83, "type": "markdown", "linesLength": 4, "startIndex": 181, "lines": ["Another way to reduce over-fitting is to grow our trees less deeply. We do this by specifying (with `min_samples_leaf`) that we require some minimum number of rows in every leaf node. This has two benefits:\n", "\n", "- There are less decision rules for each leaf node; simpler models should generalize better\n", "- The predictions are made by averaging more rows in the leaf node, resulting in less volatility"]}, {"block": 84, "type": "code", "linesLength": 3, "startIndex": 185, "lines": ["m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, n_jobs=-1, oob_score=True)\n", "m.fit(X_train, y_train)\n", "print_score(m)"]}, {"block": 85, "type": "markdown", "linesLength": 1, "startIndex": 188, "lines": ["We can also increase the amount of variation amongst the trees by not only use a sample of rows for each tree, but to also using a sample of *columns* for each *split*. We do this by specifying `max_features`, which is the proportion of features to randomly select from at each split."]}, {"block": 86, "type": "code", "linesLength": 3, "startIndex": 189, "lines": ["m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n", "m.fit(X_train, y_train)\n", "print_score(m)"]}, {"block": 87, "type": "markdown", "linesLength": 2, "startIndex": 192, "lines": ["The sklearn docs [show an example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html) of different `max_features` methods with increasing numbers of trees - as you see, using a subset of features on each split requires using more trees, but results in better models:\n", "![sklearn max_features chart](http://scikit-learn.org/stable/_images/sphx_glr_plot_ensemble_oob_001.png)"]}, {"block": 88, "type": "markdown", "linesLength": 1, "startIndex": 194, "lines": ["### Feature importance"]}, {"block": 89, "type": "code", "linesLength": 2, "startIndex": 195, "lines": ["df_trn, y_trn = proc_df(df_raw, 'SalePrice')\n", "X_train, X_valid = split_vals(df_trn.values, n_trn)"]}, {"block": 90, "type": "markdown", "linesLength": 1, "startIndex": 197, "lines": ["It's not normally enough to just to know that a model can make accurate predictions - we also want to know *how* it's making predictions. The most important way to see this is with *feature importances*."]}, {"block": 91, "type": "code", "linesLength": 3, "startIndex": 198, "lines": ["m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n", "m.fit(X_train, y_train)\n", "print_score(m)"]}, {"block": 92, "type": "code", "linesLength": 1, "startIndex": 201, "lines": ["fi = rf_feat_importance(m, df_trn); fi[:10]"]}, {"block": 93, "type": "code", "linesLength": 1, "startIndex": 202, "lines": ["fi.plot('cols', 'imp', figsize=(7,7), legend=False);"]}, {"block": 94, "type": "code", "linesLength": 1, "startIndex": 203, "lines": ["def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)"]}, {"block": 95, "type": "code", "linesLength": 1, "startIndex": 204, "lines": ["plot_fi(fi[:30]);"]}, {"block": 96, "type": "code", "linesLength": 1, "startIndex": 205, "lines": ["to_keep = fi[fi.imp>0.004].cols; len(to_keep)"]}, {"block": 97, "type": "code", "linesLength": 2, "startIndex": 206, "lines": ["df_keep = df_trn[to_keep].copy()\n", "X_train, X_valid = df_keep.values[:n_trn], df_keep.values[n_trn:]"]}, {"block": 98, "type": "code", "linesLength": 4, "startIndex": 208, "lines": ["m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5,\n", "                          n_jobs=-1, oob_score=True)\n", "m.fit(X_train, y_train)\n", "print_score(m)"]}, {"block": 99, "type": "code", "linesLength": 2, "startIndex": 212, "lines": ["fi = rf_feat_importance(m, df_keep)\n", "plot_fi(fi);"]}, {"block": 100, "type": "code", "linesLength": 3, "startIndex": 214, "lines": ["ignore=['saleDay', 'saleYear']\n", "df = df_keep.drop(ignore, axis=1).copy()\n", "X_train, X_valid = split_vals(df.values, n_trn)"]}, {"block": 101, "type": "code", "linesLength": 4, "startIndex": 217, "lines": ["m = RandomForestRegressor(n_estimators=64, min_samples_leaf=4, max_features=0.6,\n", "                          n_jobs=-1, oob_score=True)\n", "m.fit(X_train, y_train)\n", "print_score(m)"]}, {"block": 102, "type": "markdown", "linesLength": 1, "startIndex": 221, "lines": ["TODO: write about How to decide which are categorical and which are continuous"]}, {"block": 103, "type": "code", "linesLength": 2, "startIndex": 222, "lines": ["df_trn2, y_trn = proc_df(df_raw, 'SalePrice', max_n_cat=6)\n", "X_train, X_valid = split_vals(df_trn2.values, n_trn)"]}, {"block": 104, "type": "code", "linesLength": 3, "startIndex": 224, "lines": ["m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1, oob_score=True)\n", "m.fit(X_train, y_train)\n", "print_score(m)"]}, {"block": 105, "type": "code", "linesLength": 2, "startIndex": 227, "lines": ["fi = rf_feat_importance(m, df_trn2)\n", "plot_fi(fi[:25]);"]}, {"block": 106, "type": "code", "linesLength": 1, "startIndex": 229, "lines": ["df.plot('YearMade', 'saleElapsed', 'scatter', alpha=0.01, figsize=(10,8));"]}, {"block": 107, "type": "code", "linesLength": 1, "startIndex": 230, "lines": ["df.loc[df.YearMade<1900, 'YearMade'] = 2020"]}, {"block": 108, "type": "code", "linesLength": 1, "startIndex": 231, "lines": ["df.plot('YearMade', 'saleElapsed', 'scatter', alpha=0.01, figsize=(10,8));"]}, {"block": 109, "type": "code", "linesLength": 2, "startIndex": 232, "lines": ["df_trn2['age'] = df_trn2.saleYear-df_trn2.YearMade\n", "X_train, X_valid = df_trn2.values[:n_trn], df_trn2.values[n_trn:]"]}, {"block": 110, "type": "code", "linesLength": 3, "startIndex": 234, "lines": ["m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1, oob_score=True)\n", "m.fit(X_train, y_train)\n", "print_score(m)"]}, {"block": 111, "type": "code", "linesLength": 2, "startIndex": 237, "lines": ["fi = rf_feat_importance(m, df_trn2)\n", "plot_fi(fi[:25]);"]}, {"block": 112, "type": "markdown", "linesLength": 1, "startIndex": 239, "lines": ["### Removing redundent features"]}, {"block": 113, "type": "markdown", "linesLength": 1, "startIndex": 240, "lines": ["One thing that makes this harder to interpret is that there seem to be some variables with very similar meanings. Let's try to remove redundent features."]}, {"block": 114, "type": "code", "linesLength": 1, "startIndex": 241, "lines": ["from scipy.cluster import hierarchy as hc"]}, {"block": 115, "type": "code", "linesLength": 6, "startIndex": 242, "lines": ["corr = np.round(scipy.stats.spearmanr(df_keep).correlation, 4)\n", "corr_condensed = hc.distance.squareform(1-corr)\n", "z = hc.linkage(corr_condensed, method='average')\n", "fig = plt.figure(figsize=(16,10))\n", "dendrogram = hc.dendrogram(z, labels=df_keep.columns, orientation='left', leaf_font_size=16)\n", "plt.show()"]}, {"block": 116, "type": "markdown", "linesLength": 1, "startIndex": 248, "lines": ["Let's try removing some of these related features to see if the model can be simplified without impacting the accuracy. When we're comparing a few different models we can use sampling to speed up model creation, since the comparisons will be equally valid (even if the models are a little less accurate)."]}, {"block": 117, "type": "code", "linesLength": 1, "startIndex": 249, "lines": ["set_rf_samples(50000)"]}, {"block": 118, "type": "code", "linesLength": 5, "startIndex": 250, "lines": ["def get_oob(df):\n", "    m = RandomForestRegressor(n_estimators=30, min_samples_leaf=5, max_features=0.6, n_jobs=-1, oob_score=True)\n", "    x, _ = split_vals(df.values, n_trn)\n", "    m.fit(x, y_train)\n", "    return m.oob_score_"]}, {"block": 119, "type": "markdown", "linesLength": 1, "startIndex": 255, "lines": ["Here's our baseline."]}, {"block": 120, "type": "code", "linesLength": 1, "startIndex": 256, "lines": ["get_oob(df_keep)"]}, {"block": 121, "type": "markdown", "linesLength": 1, "startIndex": 257, "lines": ["Now we try removing each variable one at a time."]}, {"block": 122, "type": "code", "linesLength": 2, "startIndex": 258, "lines": ["for c in ('fiModelDesc', 'fiBaseModel', 'saleYear', 'saleElapsed', 'Hydraulics_Flow', 'Grouser_Tracks', 'Coupler_System'):\n", "    print(c, get_oob(df_keep.drop(c, axis=1)))"]}, {"block": 123, "type": "markdown", "linesLength": 1, "startIndex": 260, "lines": ["It looks like we can try one from each group for removal. Let's see what that does."]}, {"block": 124, "type": "code", "linesLength": 3, "startIndex": 261, "lines": ["to_drop = ['fiBaseModel', 'saleYear', 'Grouser_Tracks']\n", "for c in ('fiModelDesc', 'saleElapsed', 'Hydraulics_Flow', 'Coupler_System'):\n", "    print(c, get_oob(df_keep.drop(to_drop+[c], axis=1)))"]}, {"block": 125, "type": "markdown", "linesLength": 1, "startIndex": 264, "lines": ["...and try removing one more from the largest cluster:"]}, {"block": 126, "type": "code", "linesLength": 3, "startIndex": 265, "lines": ["to_drop = ['fiBaseModel', 'saleYear', 'Grouser_Tracks', 'Hydraulics_Flow']\n", "for c in ('fiModelDesc', 'saleElapsed', 'Coupler_System'):\n", "    print(c, get_oob(df_keep.drop(to_drop+[c], axis=1)))"]}, {"block": 127, "type": "markdown", "linesLength": 1, "startIndex": 268, "lines": ["Looking good! Let's use this dataframe from here."]}, {"block": 128, "type": "code", "linesLength": 1, "startIndex": 269, "lines": ["df_keep.drop(to_drop, axis=1, inplace=True)"]}, {"block": 129, "type": "markdown", "linesLength": 1, "startIndex": 270, "lines": ["And let's see how this model looks on the full dataset:"]}, {"block": 130, "type": "code", "linesLength": 1, "startIndex": 271, "lines": ["reset_rf_samples()"]}, {"block": 131, "type": "code", "linesLength": 4, "startIndex": 272, "lines": ["m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n", "X_train, X_valid = split_vals(df_keep.values, n_trn)\n", "m.fit(X_train, y_train)\n", "print_score(m)"]}, {"block": 132, "type": "code", "linesLength": 2, "startIndex": 276, "lines": ["fi = rf_feat_importance(m, df_keep)\n", "plot_fi(fi);"]}, {"block": 133, "type": "markdown", "linesLength": 1, "startIndex": 278, "lines": ["### Partial dependence"]}, {"block": 134, "type": "markdown", "linesLength": 1, "startIndex": 279, "lines": ["*todo* add interactions (see R code)"]}, {"block": 135, "type": "markdown", "linesLength": 1, "startIndex": 280, "lines": ["*todo* extrapolate future dates better. use RF to decide if something is in train or validation. note: it's a classification problem!"]}, {"block": 136, "type": "markdown", "linesLength": 1, "startIndex": 281, "lines": ["*todo* confidence based on tree variance"]}, {"block": 137, "type": "markdown", "linesLength": 1, "startIndex": 282, "lines": ["### Functions, parameters, and training"]}, {"block": 138, "type": "markdown", "linesLength": 5, "startIndex": 283, "lines": ["A **function** takes inputs and returns outputs. For instance, $f(x) = 3x + 5$ is an example of a function.  If we input $2$, the output is $3\\times 2 + 5 = 11$, or if we input $-1$, the output is $3\\times (-1) + 5 = 2$\n", "\n", "Functions have **parameters**. The above function $f$ is $ax + b$, with parameters a and b set to $a=3$ and $b=5$.\n", "\n", "Machine learning is often about learning the best values for those parameters.  For instance, suppose we have the data points on the chart below.  What values should we choose for $a$ and $b$?"]}, {"block": 139, "type": "markdown", "linesLength": 1, "startIndex": 288, "lines": ["<img src=\"images/sgd2.gif\" alt=\"\" style=\"width: 70%\"/>"]}, {"block": 140, "type": "markdown", "linesLength": 5, "startIndex": 289, "lines": ["In the above gif fast.ai Practical Deep Learning for Coders course, [intro to SGD notebook](https://github.com/fastai/courses/blob/master/deeplearning1/nbs/sgd-intro.ipynb)), an algorithm called stochastic gradient descent is being used to learn the best parameters to fit the line to the data (note: in the gif, the algorithm is stopping before the absolute best parameters are found).  This process is called **training** or **fitting**.\n", "\n", "Most datasets will not be well-represented by a line.  We could use a more complicated function, such as $g(x) = ax^2 + bx + c + \\sin d$.  Now we have 4 parameters to learn: $a$, $b$, $c$, and $d$.  This function is more flexible than $f(x) = ax + b$ and will be able to accurately model more datasets.\n", "\n", "Neural networks take this to an extreme, and are infinitely flexible.  They often have thousands, or even hundreds of thousands of parameters.  However the core idea is the same as above.  The neural network is a function, and we will learn the best parameters for modeling our data."]}, {"block": 141, "type": "markdown", "linesLength": 1, "startIndex": 294, "lines": ["### Imports"]}, {"block": 142, "type": "code", "linesLength": 3, "startIndex": 295, "lines": ["from fastai.imports import *\n", "from fastai.torch_imports import *\n", "from fastai.io import *"]}, {"block": 143, "type": "markdown", "linesLength": 1, "startIndex": 298, "lines": ["## Data"]}, {"block": 144, "type": "markdown", "linesLength": 1, "startIndex": 299, "lines": ["Today we will be working with MNIST, a classic data set of hand-written digits.  Solutions to this problem are used by banks to automatically recognize the amounts on checks, and by the postal service to automatically recognize zip codes on mail."]}, {"block": 145, "type": "markdown", "linesLength": 1, "startIndex": 300, "lines": ["<img src=\"images/mnist.png\" alt=\"\" style=\"width: 60%\"/>"]}, {"block": 146, "type": "markdown", "linesLength": 5, "startIndex": 301, "lines": ["A matrix can represent an image, by creating a grid where each entry corresponds to a different pixel.\n", "\n", "<img src=\"images/digit.gif\" alt=\"digit\" style=\"width: 55%\"/>\n", "  (Source: [Adam Geitgey\n", "](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721))\n"]}, {"block": 147, "type": "markdown", "linesLength": 1, "startIndex": 306, "lines": ["### Download"]}, {"block": 148, "type": "markdown", "linesLength": 1, "startIndex": 307, "lines": ["Let's download, unzip, and format the data."]}, {"block": 149, "type": "code", "linesLength": 1, "startIndex": 308, "lines": ["path = '../data/'"]}, {"block": 150, "type": "code", "linesLength": 2, "startIndex": 309, "lines": ["import os\n", "os.makedirs(path, exist_ok=True)"]}, {"block": 151, "type": "code", "linesLength": 5, "startIndex": 311, "lines": ["URL='http://deeplearning.net/data/mnist/'\n", "FILENAME='mnist.pkl.gz'\n", "\n", "def load_mnist(filename):\n", "    return pickle.load(gzip.open(filename, 'rb'), encoding='latin-1')"]}, {"block": 152, "type": "code", "linesLength": 2, "startIndex": 316, "lines": ["get_data(URL+FILENAME, path+FILENAME)\n", "((x, y), (x_valid, y_valid), _) = load_mnist(path+FILENAME)"]}, {"block": 153, "type": "markdown", "linesLength": 1, "startIndex": 318, "lines": ["### Normalize"]}, {"block": 154, "type": "markdown", "linesLength": 1, "startIndex": 319, "lines": ["Many machine learning algorithms behave better when the data is *normalized*, that is when the mean is 0 and the standard deviation is 1. We will subtract off the mean and standard deviation from our training set in order to normalize the data:"]}, {"block": 155, "type": "code", "linesLength": 2, "startIndex": 320, "lines": ["mean = x.mean()\n", "std = x.std()"]}, {"block": 156, "type": "code", "linesLength": 2, "startIndex": 322, "lines": ["x=(x-mean)/std\n", "x.mean(), x.std()"]}, {"block": 157, "type": "markdown", "linesLength": 1, "startIndex": 324, "lines": ["Note that for consistency (with the parameters we learn when training), we subtract the mean and standard deviation of our training set from our validation set. "]}, {"block": 158, "type": "code", "linesLength": 2, "startIndex": 325, "lines": ["x_valid = (x_valid-mean)/std\n", "x_valid.mean(), x_valid.std()"]}, {"block": 159, "type": "markdown", "linesLength": 1, "startIndex": 327, "lines": ["### Look at the data"]}, {"block": 160, "type": "markdown", "linesLength": 1, "startIndex": 328, "lines": ["In any sort of data science work, it's important to look at your data, to make sure you understand the format, how it's stored, what type of values it holds, etc. To make it easier to work with, let's reshape it into 2d images from the flattened 1d format."]}, {"block": 161, "type": "markdown", "linesLength": 1, "startIndex": 329, "lines": ["#### Helper methods"]}, {"block": 162, "type": "code", "linesLength": 7, "startIndex": 330, "lines": ["%matplotlib inline\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "def show(img, title=None):\n", "    plt.imshow(img, interpolation='none', cmap=\"gray\")\n", "    if title is not None: plt.title(title)"]}, {"block": 163, "type": "code", "linesLength": 8, "startIndex": 337, "lines": ["def plots(ims, figsize=(12,6), rows=2, titles=None):\n", "    f = plt.figure(figsize=figsize)\n", "    cols = len(ims)//rows\n", "    for i in range(len(ims)):\n", "        sp = f.add_subplot(rows, cols, i+1)\n", "        sp.axis('Off')\n", "        if titles is not None: sp.set_title(titles[i], fontsize=16)\n", "        plt.imshow(ims[i], interpolation='none', cmap='gray')"]}, {"block": 164, "type": "markdown", "linesLength": 1, "startIndex": 345, "lines": ["#### Plots "]}, {"block": 165, "type": "code", "linesLength": 1, "startIndex": 346, "lines": ["x_valid.shape"]}, {"block": 166, "type": "code", "linesLength": 1, "startIndex": 347, "lines": ["x_imgs = np.reshape(x_valid, (-1,28,28)); x_imgs.shape"]}, {"block": 167, "type": "code", "linesLength": 1, "startIndex": 348, "lines": ["show(x_imgs[0], y_valid[0])"]}, {"block": 168, "type": "code", "linesLength": 1, "startIndex": 349, "lines": ["y_valid.shape"]}, {"block": 169, "type": "markdown", "linesLength": 1, "startIndex": 350, "lines": ["It's the digit 3!  And that's stored in the y value:"]}, {"block": 170, "type": "code", "linesLength": 1, "startIndex": 351, "lines": ["y_valid[0]"]}, {"block": 171, "type": "markdown", "linesLength": 1, "startIndex": 352, "lines": ["We can look at part of an image:"]}, {"block": 172, "type": "code", "linesLength": 1, "startIndex": 353, "lines": ["x_imgs[0,10:15,10:15]"]}, {"block": 173, "type": "code", "linesLength": 1, "startIndex": 354, "lines": ["show(x_imgs[0,10:15,10:15])"]}, {"block": 174, "type": "code", "linesLength": 1, "startIndex": 355, "lines": ["plots(x_imgs[:8], titles=y_valid[:8])"]}, {"block": 175, "type": "markdown", "linesLength": 1, "startIndex": 356, "lines": ["## The Most Important Machine Learning Concepts"]}, {"block": 176, "type": "markdown", "linesLength": 1, "startIndex": 357, "lines": ["### Functions, parameters, and training"]}, {"block": 177, "type": "markdown", "linesLength": 5, "startIndex": 358, "lines": ["A **function** takes inputs and returns outputs. For instance, $f(x) = 3x + 5$ is an example of a function.  If we input $2$, the output is $3\\times 2 + 5 = 11$, or if we input $-1$, the output is $3\\times (-1) + 5 = 2$\n", "\n", "Functions have **parameters**. The above function $f$ is $ax + b$, with parameters a and b set to $a=3$ and $b=5$.\n", "\n", "Machine learning is often about learning the best values for those parameters.  For instance, suppose we have the data points on the chart below.  What values should we choose for $a$ and $b$?"]}, {"block": 178, "type": "markdown", "linesLength": 1, "startIndex": 363, "lines": ["<img src=\"images/sgd2.gif\" alt=\"\" style=\"width: 70%\"/>"]}, {"block": 179, "type": "markdown", "linesLength": 5, "startIndex": 364, "lines": ["In the above gif fast.ai Practical Deep Learning for Coders course, [intro to SGD notebook](https://github.com/fastai/courses/blob/master/deeplearning1/nbs/sgd-intro.ipynb)), an algorithm called stochastic gradient descent is being used to learn the best parameters to fit the line to the data (note: in the gif, the algorithm is stopping before the absolute best parameters are found).  This process is called **training** or **fitting**.\n", "\n", "Most datasets will not be well-represented by a line.  We could use a more complicated function, such as $g(x) = ax^2 + bx + c + \\sin d$.  Now we have 4 parameters to learn: $a$, $b$, $c$, and $d$.  This function is more flexible than $f(x) = ax + b$ and will be able to accurately model more datasets.\n", "\n", "Neural networks take this to an extreme, and are infinitely flexible.  They often have thousands, or even hundreds of thousands of parameters.  However the core idea is the same as above.  The neural network is a function, and we will learn the best parameters for modeling our data."]}, {"block": 180, "type": "markdown", "linesLength": 1, "startIndex": 369, "lines": ["### Training & Validation data sets"]}, {"block": 181, "type": "markdown", "linesLength": 3, "startIndex": 370, "lines": ["Possibly **the most important idea** in machine learning is that of having separate training & validation data sets.\n", "\n", "As motivation, suppose you don't divide up your data, but instead use all of it.  And suppose you have lots of parameters:"]}, {"block": 182, "type": "markdown", "linesLength": 1, "startIndex": 373, "lines": ["This is called over-fitting.  A validation set helps prevent this problem."]}, {"block": 183, "type": "markdown", "linesLength": 4, "startIndex": 374, "lines": ["<img src=\"images/overfitting2.png\" alt=\"\" style=\"width: 70%\"/>\n", "<center>\n", "[Underfitting and Overfitting](https://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted)\n", "</center>"]}, {"block": 184, "type": "markdown", "linesLength": 3, "startIndex": 378, "lines": ["The error for the pictured data points is lowest for the model on the far right (the blue curve passes through the red points almost perfectly), yet it's not the best choice.  Why is that?  If you were to gather some new data points, they most likely would not be on that curve in the graph on the right, but would be closer to the curve in the middle graph.\n", "\n", "This illustrates how using all our data can lead to **overfitting**."]}, {"block": 185, "type": "markdown", "linesLength": 1, "startIndex": 381, "lines": ["## Neural Net (with nn.torch)"]}, {"block": 186, "type": "markdown", "linesLength": 1, "startIndex": 382, "lines": ["### Imports "]}, {"block": 187, "type": "code", "linesLength": 6, "startIndex": 383, "lines": ["from fastai.metrics import *\n", "from fastai.model import *\n", "from fastai.dataset import *\n", "from fastai.core import *\n", "\n", "import torch.nn as nn"]}, {"block": 188, "type": "markdown", "linesLength": 1, "startIndex": 389, "lines": ["###  Neural networks "]}, {"block": 189, "type": "markdown", "linesLength": 1, "startIndex": 390, "lines": ["We will use fastai's ImageClassifierData, which holds our training and validation sets and will provide batches of that data in a form ready for use by a PyTorch model."]}, {"block": 190, "type": "code", "linesLength": 1, "startIndex": 391, "lines": ["md = ImageClassifierData.from_arrays(path, (x,y), (x_valid, y_valid))"]}, {"block": 191, "type": "markdown", "linesLength": 1, "startIndex": 392, "lines": ["We will begin with the highest level abstraction: using a neural net defined by PyTorch's Sequential class.  "]}, {"block": 192, "type": "code", "linesLength": 5, "startIndex": 393, "lines": ["net = nn.Sequential(\n", "    nn.Linear(28*28, 256),\n", "    nn.ReLU(),\n", "    nn.Linear(256, 10)\n", ").cuda()"]}, {"block": 193, "type": "markdown", "linesLength": 1, "startIndex": 398, "lines": ["Each input is a vector of size $28\\times 28$ pixels and our output is of size $10$ (since there are 10 digits: 0, 1, ..., 9). "]}, {"block": 194, "type": "markdown", "linesLength": 1, "startIndex": 399, "lines": ["We use the output of the final layer to generate our predictions.  Often for classification problems (like MNIST digit classification), the final layer has the same number of outputs as there are classes.  In that case, this is 10: one for each digit from 0 to 9.  These can be converted to comparative probabilities.  For instance, it may be determined that a particular hand-written image is 80% likely to be a 4, 18% likely to be a 9, and 2% likely to be a 3.  In our case, we are not interested in viewing the probabilites, and just want to see what the most likely guess is."]}, {"block": 195, "type": "markdown", "linesLength": 1, "startIndex": 400, "lines": ["### Layers"]}, {"block": 196, "type": "markdown", "linesLength": 1, "startIndex": 401, "lines": ["Sequential defines layers of our network, so let's talk about layers. Neural networks consist of **linear layers alternating with non-linear layers**.  This creates functions which are incredibly flexible.  Deeper layers are able to capture more complex patterns."]}, {"block": 197, "type": "markdown", "linesLength": 5, "startIndex": 402, "lines": ["Layer 1 of a convolutional neural network:\n", "<img src=\"images/zeiler1.png\" alt=\"pytorch\" style=\"width: 40%\"/>\n", "<center>\n", "[Matthew Zeiler and Rob Fergus](http://www.matthewzeiler.com/wp-content/uploads/2017/07/arxive2013.pdf)\n", "</center>"]}, {"block": 198, "type": "markdown", "linesLength": 5, "startIndex": 407, "lines": ["Layer 2:\n", "<img src=\"images/zeiler2.png\" alt=\"pytorch\" style=\"width: 90%\"/>\n", "<center>\n", "[Matthew Zeiler and Rob Fergus](http://www.matthewzeiler.com/wp-content/uploads/2017/07/arxive2013.pdf)\n", "</center>"]}, {"block": 199, "type": "markdown", "linesLength": 5, "startIndex": 412, "lines": ["Deeper layers can learn about more complicated shapes (although we are only using 2 layers in our network):\n", "<img src=\"images/zeiler4.png\" alt=\"pytorch\" style=\"width: 90%\"/>\n", "<center>\n", "[Matthew Zeiler and Rob Fergus](http://www.matthewzeiler.com/wp-content/uploads/2017/07/arxive2013.pdf)\n", "</center>"]}, {"block": 200, "type": "markdown", "linesLength": 1, "startIndex": 417, "lines": ["### Training the network "]}, {"block": 201, "type": "code", "linesLength": 1, "startIndex": 418, "lines": ["net"]}, {"block": 202, "type": "code", "linesLength": 1, "startIndex": 419, "lines": ["784*256 + 256*10"]}, {"block": 203, "type": "markdown", "linesLength": 4, "startIndex": 420, "lines": ["Next we will set a few inputs for our *fit* method:\n", "- **Optimizer**: algorithm for finding the minimum. typically these are variations on *stochastic gradient descent*, involve taking a step that appears to be the right direction based on the change in the function.\n", "- **Loss**: what function is the optimizer trying to minimize?  We need to say how we're defining the error.\n", "- **Metrics**: other calculations you want printed out as you train"]}, {"block": 204, "type": "code", "linesLength": 3, "startIndex": 424, "lines": ["loss=F.cross_entropy\n", "metrics=[accuracy]\n", "opt=optim.Adam(net.parameters())"]}, {"block": 205, "type": "markdown", "linesLength": 1, "startIndex": 427, "lines": ["*Fitting* is the process by which the neural net learns the best parameters for the dataset."]}, {"block": 206, "type": "code", "linesLength": 1, "startIndex": 428, "lines": ["fit(net, md, epochs=1, crit=loss, opt=opt, metrics=metrics)"]}, {"block": 207, "type": "markdown", "linesLength": 3, "startIndex": 429, "lines": ["GPUs are great at handling lots of data at once (otherwise don't get performance benefit).  We break the data up into **batches**, and that specifies how many samples from our dataset we want to send to the GPU at a time.  The fastai library defaults to a batch size of 64.  On each iteration of the training loop, the error on 1 batch of data will be calculated, and the optimizer will update the parameters based on that.\n", "\n", "An **epoch** is completed once each data sample has been used once in the training loop."]}, {"block": 208, "type": "markdown", "linesLength": 1, "startIndex": 432, "lines": ["Now that we have the parameters for our model, we can make predictions on our validation set."]}, {"block": 209, "type": "code", "linesLength": 1, "startIndex": 433, "lines": ["preds = predict(net, md.val_dl)"]}, {"block": 210, "type": "code", "linesLength": 1, "startIndex": 434, "lines": ["preds = preds.max(1)[1]"]}, {"block": 211, "type": "markdown", "linesLength": 1, "startIndex": 435, "lines": ["Let's see how some of our preditions look!"]}, {"block": 212, "type": "code", "linesLength": 1, "startIndex": 436, "lines": ["plots(x_imgs[:8], titles=preds[:8])"]}, {"block": 213, "type": "markdown", "linesLength": 1, "startIndex": 437, "lines": ["These predictions are pretty good!"]}, {"block": 214, "type": "markdown", "linesLength": 1, "startIndex": 438, "lines": ["## Coding the Neural Net ourselves"]}, {"block": 215, "type": "markdown", "linesLength": 1, "startIndex": 439, "lines": ["Recall that above we used PyTorch's `Sequential` to define a neural network with a linear layer, a non-linear layer (`ReLU`), and then another linear layer."]}, {"block": 216, "type": "code", "linesLength": 6, "startIndex": 440, "lines": ["# Our code from above\n", "net = nn.Sequential(\n", "    nn.Linear(28*28, 256),\n", "    nn.ReLU(),\n", "    nn.Linear(256, 10)\n", ").cuda()"]}, {"block": 217, "type": "markdown", "linesLength": 5, "startIndex": 446, "lines": ["It turns out that `Linear` is defined by a matrix multiplication and then an addition.  Let's try defining this ourselves. This will allow us to see exactly where matrix multiplication is used (we will dive in to how matrix multiplication works in teh next section).  \n", "\n", "Just as Numpy has `np.matmul` for matrix multiplication (in Python 3, this is equivalent to the `@` operator), PyTorch has `torch.matmul`.  \n", "\n", "PyTorch class has two things: constructor (says parameters) and a forward method (how to calculate prediction using those parameters)  The method `forward` describes how the neural net converts inputs to outputs."]}, {"block": 218, "type": "markdown", "linesLength": 1, "startIndex": 451, "lines": ["In PyTorch, the optimizer knows to try to optimize any attribute of type **Parameter**."]}, {"block": 219, "type": "code", "linesLength": 1, "startIndex": 452, "lines": ["def get_weights(*dims): return nn.Parameter(torch.randn(*dims)/dims[0])"]}, {"block": 220, "type": "code", "linesLength": 14, "startIndex": 453, "lines": ["class SimpleMnist(nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.l1_w = get_weights(28*28, 256)  # Layer 1 weights\n", "        self.l1_b = get_weights(256)         # Layer 1 bias\n", "        self.l2_w = get_weights(256, 10)     # Layer 2 weights\n", "        self.l2_b = get_weights(10)          # Layer 2 bias\n", "\n", "    def forward(self, x):\n", "        x = x.view(x.size(0), -1)\n", "        x = torch.matmul(x, self.l1_w) + self.l1_b  # Linear Layer\n", "        x = x * (x > 0).float()                     # Non-linear Layer\n", "        x = torch.matmul(x, self.l2_w) + self.l2_b  # Linear Layer\n", "        return x"]}, {"block": 221, "type": "markdown", "linesLength": 1, "startIndex": 467, "lines": ["We create our neural net and the optimizer.  (We will use the same loss and metrics from above)."]}, {"block": 222, "type": "code", "linesLength": 2, "startIndex": 468, "lines": ["net2 = SimpleMnist().cuda()\n", "opt=optim.Adam(net2.parameters())"]}, {"block": 223, "type": "code", "linesLength": 1, "startIndex": 470, "lines": ["fit(net2, md, epochs=1, crit=loss, opt=opt, metrics=metrics)"]}, {"block": 224, "type": "markdown", "linesLength": 1, "startIndex": 471, "lines": ["Now we can check our predictions:"]}, {"block": 225, "type": "code", "linesLength": 2, "startIndex": 472, "lines": ["preds = predict(net2, md.val_dl).max(1)[1]\n", "plots(x_imgs[:8], titles=preds[:8])"]}, {"block": 226, "type": "markdown", "linesLength": 1, "startIndex": 474, "lines": ["## what torch.matmul (matrix multiplication) is doing"]}, {"block": 227, "type": "markdown", "linesLength": 1, "startIndex": 475, "lines": ["Now let's dig in to what we were doing with `torch.matmul`: matrix multiplication.  First, let's start with a simpler building block: **broadcasting**."]}, {"block": 228, "type": "markdown", "linesLength": 1, "startIndex": 476, "lines": ["### Element-wise operations "]}, {"block": 229, "type": "markdown", "linesLength": 5, "startIndex": 477, "lines": ["Broadcasting and element-wise operations are supported in the same way by both numpy and pytorch.\n", "\n", "Operators (+,-,\\*,/,>,<,==) are usually element-wise.\n", "\n", "Examples of element-wise operations:"]}, {"block": 230, "type": "code", "linesLength": 2, "startIndex": 482, "lines": ["a = np.array([10, 6, -4])\n", "b = np.array([2, 8, 7])"]}, {"block": 231, "type": "code", "linesLength": 1, "startIndex": 484, "lines": ["a + b"]}, {"block": 232, "type": "code", "linesLength": 1, "startIndex": 485, "lines": ["a < b"]}, {"block": 233, "type": "markdown", "linesLength": 1, "startIndex": 486, "lines": ["### Broadcasting"]}, {"block": 234, "type": "markdown", "linesLength": 13, "startIndex": 487, "lines": ["The term **broadcasting** describes how arrays with different shapes are treated during arithmetic operations.  The term broadcasting was first used by Numpy, although is now used in other libraries such as [Tensorflow](https://www.tensorflow.org/performance/xla/broadcasting) and Matlab; the rules can vary by library.\n", "\n", "From the [Numpy Documentation](https://docs.scipy.org/doc/numpy-1.10.0/user/basics.broadcasting.html):\n", "\n", "    The term broadcasting describes how numpy treats arrays with \n", "    different shapes during arithmetic operations. Subject to certain \n", "    constraints, the smaller array is \u201cbroadcast\u201d across the larger \n", "    array so that they have compatible shapes. Broadcasting provides a \n", "    means of vectorizing array operations so that looping occurs in C\n", "    instead of Python. It does this without making needless copies of \n", "    data and usually leads to efficient algorithm implementations.\n", "    \n", "In addition to the efficiency of broadcasting, it allows developers to write less code, which typically leads to fewer errors."]}, {"block": 235, "type": "markdown", "linesLength": 1, "startIndex": 500, "lines": ["*This section was adapted from [Chapter 4](http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/4.%20Compressed%20Sensing%20of%20CT%20Scans%20with%20Robust%20Regression.ipynb#4.-Compressed-Sensing-of-CT-Scans-with-Robust-Regression) of the fast.ai [Computational Linear Algebra](https://github.com/fastai/numerical-linear-algebra) course.*"]}, {"block": 236, "type": "markdown", "linesLength": 1, "startIndex": 501, "lines": ["#### Broadcasting with a scalar"]}, {"block": 237, "type": "code", "linesLength": 1, "startIndex": 502, "lines": ["a"]}, {"block": 238, "type": "code", "linesLength": 1, "startIndex": 503, "lines": ["a > 0"]}, {"block": 239, "type": "markdown", "linesLength": 9, "startIndex": 504, "lines": ["How are we able to do a > 0?  0 is being **broadcast** to have the same dimensions as a.\n", "\n", "Remember above when we normalized our dataset by subtracting the mean (a scalar) from the entire data set (a matrix) and dividing by the standard deviation (another scalar)?  \n", "\n", "    x=(x-mean)/std\n", "\n", "We were using broadcasting!\n", "\n", "Other examples of broadcasting with a scalar:"]}, {"block": 240, "type": "code", "linesLength": 1, "startIndex": 513, "lines": ["a + 1"]}, {"block": 241, "type": "code", "linesLength": 1, "startIndex": 514, "lines": ["m = np.array([[1, 2, 3], [4,5,6], [7,8,9]]); m"]}, {"block": 242, "type": "code", "linesLength": 1, "startIndex": 515, "lines": ["m * 2"]}, {"block": 243, "type": "markdown", "linesLength": 1, "startIndex": 516, "lines": ["#### Broadcasting a vector to a matrix"]}, {"block": 244, "type": "markdown", "linesLength": 1, "startIndex": 517, "lines": ["We can also broadcast a vector to a matrix:"]}, {"block": 245, "type": "code", "linesLength": 1, "startIndex": 518, "lines": ["c = np.array([10,20,30]); c"]}, {"block": 246, "type": "code", "linesLength": 1, "startIndex": 519, "lines": ["m + c"]}, {"block": 247, "type": "markdown", "linesLength": 1, "startIndex": 520, "lines": ["Although numpy does this automatically, you can also use the `broadcast_to` method:"]}, {"block": 248, "type": "code", "linesLength": 1, "startIndex": 521, "lines": ["np.broadcast_to(c, (3,3))"]}, {"block": 249, "type": "code", "linesLength": 1, "startIndex": 522, "lines": ["c.shape"]}, {"block": 250, "type": "markdown", "linesLength": 1, "startIndex": 523, "lines": ["The numpy `expand_dims` method lets us convert the 1-dimensional array `c` into a 2-dimensional array (although one of those dimensions has value 1)."]}, {"block": 251, "type": "code", "linesLength": 1, "startIndex": 524, "lines": ["np.expand_dims(c,0).shape"]}, {"block": 252, "type": "code", "linesLength": 1, "startIndex": 525, "lines": ["m + np.expand_dims(c,0)"]}, {"block": 253, "type": "code", "linesLength": 1, "startIndex": 526, "lines": ["np.expand_dims(c,1).shape"]}, {"block": 254, "type": "code", "linesLength": 1, "startIndex": 527, "lines": ["m + np.expand_dims(c,1)"]}, {"block": 255, "type": "code", "linesLength": 1, "startIndex": 528, "lines": ["np.broadcast_to(np.expand_dims(c,1), (3,3))"]}, {"block": 256, "type": "markdown", "linesLength": 1, "startIndex": 529, "lines": ["#### Broadcasting Rules"]}, {"block": 257, "type": "markdown", "linesLength": 4, "startIndex": 530, "lines": ["When operating on two arrays, Numpy/PyTorch compares their shapes element-wise. It starts with the **trailing dimensions**, and works its way forward. Two dimensions are **compatible** when\n", "\n", "- they are equal, or\n", "- one of them is 1"]}, {"block": 258, "type": "markdown", "linesLength": 5, "startIndex": 534, "lines": ["Arrays do not need to have the same number of dimensions. For example, if you have a $256 \\times 256 \\times 3$ array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:\n", "\n", "    Image  (3d array): 256 x 256 x 3\n", "    Scale  (1d array):             3\n", "    Result (3d array): 256 x 256 x 3"]}, {"block": 259, "type": "markdown", "linesLength": 1, "startIndex": 539, "lines": ["The [numpy documentation](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html#general-broadcasting-rules) includes several examples of what dimensions can and can not be broadcast together."]}, {"block": 260, "type": "markdown", "linesLength": 1, "startIndex": 540, "lines": ["### Matrix Multiplication"]}, {"block": 261, "type": "markdown", "linesLength": 1, "startIndex": 541, "lines": ["We are going to use broadcasting to define matrix multiplication."]}, {"block": 262, "type": "markdown", "linesLength": 1, "startIndex": 542, "lines": ["#### Matrix-Vector Multiplication"]}, {"block": 263, "type": "code", "linesLength": 1, "startIndex": 543, "lines": ["m, c"]}, {"block": 264, "type": "code", "linesLength": 1, "startIndex": 544, "lines": ["m @ c  # np.matmul(m, c)"]}, {"block": 265, "type": "markdown", "linesLength": 1, "startIndex": 545, "lines": ["We get the same answer using `torch.matmul`:"]}, {"block": 266, "type": "code", "linesLength": 1, "startIndex": 546, "lines": ["torch.matmul(torch.from_numpy(m), torch.from_numpy(c))"]}, {"block": 267, "type": "markdown", "linesLength": 1, "startIndex": 547, "lines": ["The following is **NOT** matrix multiplication.  What is it?"]}, {"block": 268, "type": "code", "linesLength": 1, "startIndex": 548, "lines": ["m * c"]}, {"block": 269, "type": "code", "linesLength": 1, "startIndex": 549, "lines": ["(m * c).sum(axis=1)"]}, {"block": 270, "type": "code", "linesLength": 1, "startIndex": 550, "lines": ["c"]}, {"block": 271, "type": "code", "linesLength": 1, "startIndex": 551, "lines": ["np.broadcast_to(c, (3,3))"]}, {"block": 272, "type": "markdown", "linesLength": 1, "startIndex": 552, "lines": ["From a machine learning perspective, matrix multiplication is a way of creating features by saying how much we want to weight each input column.  **Different features are different weighted averages of the input columns**. "]}, {"block": 273, "type": "markdown", "linesLength": 1, "startIndex": 553, "lines": ["The website [matrixmultiplication.xyz](http://matrixmultiplication.xyz/) provides a nice visualization of matrix multiplcation"]}, {"block": 274, "type": "markdown", "linesLength": 1, "startIndex": 554, "lines": ["Draw a picture"]}, {"block": 275, "type": "code", "linesLength": 1, "startIndex": 555, "lines": ["n = np.array([[10,40],[20,0],[30,-5]]); n"]}, {"block": 276, "type": "code", "linesLength": 1, "startIndex": 556, "lines": ["m @ n"]}, {"block": 277, "type": "code", "linesLength": 1, "startIndex": 557, "lines": ["(m * n[:,0]).sum(axis=1)"]}, {"block": 278, "type": "code", "linesLength": 1, "startIndex": 558, "lines": ["(m * n[:,1]).sum(axis=1)"]}, {"block": 279, "type": "markdown", "linesLength": 1, "startIndex": 559, "lines": ["## Homework: another use of broadcasting"]}, {"block": 280, "type": "markdown", "linesLength": 8, "startIndex": 560, "lines": ["If you want to test your understanding of the above tutorial.  I encourage you to work through it again, only this time use **CIFAR 10**, a dataset that consists of 32x32 *color* images in 10 different categories.  Color images have an extra dimension, containing RGB values, compared to black & white images.\n", "\n", "<img src=\"images/cifar10.png\" alt=\"\" style=\"width: 70%\"/>\n", "<center>\n", "(source: [Cifar 10](https://www.cs.toronto.edu/~kriz/cifar.html))\n", "</center>\n", "\n", "Fortunately, broadcasting will make it relatively easy to add this extra dimension (for color RGB), but you will have to make some changes to the code."]}, {"block": 281, "type": "markdown", "linesLength": 1, "startIndex": 568, "lines": ["## Other applications of Matrix and Tensor Products"]}, {"block": 282, "type": "markdown", "linesLength": 1, "startIndex": 569, "lines": ["Here are some other examples of where matrix multiplication arises.  This material is taken from [Chapter 1](http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb) of my [Computational Linear Algebra](https://github.com/fastai/numerical-linear-algebra) course. "]}, {"block": 283, "type": "markdown", "linesLength": 1, "startIndex": 570, "lines": ["#### Matrix-Vector Products:"]}, {"block": 284, "type": "markdown", "linesLength": 7, "startIndex": 571, "lines": ["The matrix below gives the probabilities of moving from 1 health state to another in 1 year.  If the current health states for a group are:\n", "- 85% asymptomatic\n", "- 10% symptomatic\n", "- 5% AIDS\n", "- 0% death\n", "\n", "what will be the % in each health state in 1 year?"]}, {"block": 285, "type": "markdown", "linesLength": 1, "startIndex": 578, "lines": ["<img src=\"images/markov_health.jpg\" alt=\"floating point\" style=\"width: 80%\"/>(Source: [Concepts of Markov Chains](https://www.youtube.com/watch?v=0Il-y_WLTo4))"]}, {"block": 286, "type": "markdown", "linesLength": 1, "startIndex": 579, "lines": ["#### Answer"]}, {"block": 287, "type": "code", "linesLength": 1, "startIndex": 580, "lines": ["import numpy as np"]}, {"block": 288, "type": "code", "linesLength": 1, "startIndex": 581, "lines": ["#Exercise: Use Numpy to compute the answer to the above\n"]}, {"block": 289, "type": "markdown", "linesLength": 1, "startIndex": 582, "lines": ["#### Matrix-Matrix Products"]}, {"block": 290, "type": "markdown", "linesLength": 1, "startIndex": 583, "lines": ["<img src=\"images/shop.png\" alt=\"floating point\" style=\"width: 100%\"/>(Source: [Several Simple Real-world Applications of Linear Algebra Tools](https://www.mff.cuni.cz/veda/konference/wds/proc/pdf06/WDS06_106_m8_Ulrychova.pdf))"]}, {"block": 291, "type": "markdown", "linesLength": 1, "startIndex": 584, "lines": ["#### Answer"]}, {"block": 292, "type": "code", "linesLength": 1, "startIndex": 585, "lines": ["#Exercise: Use Numpy to compute the answer to the above\n"]}, {"block": 293, "type": "markdown", "linesLength": 1, "startIndex": 586, "lines": ["## End"]}, {"block": 294, "type": "code", "linesLength": 0, "startIndex": 587, "lines": []}, {"block": 295, "type": "markdown", "linesLength": 1, "startIndex": 587, "lines": ["A Tensor is a *multi-dimensional matrix containing elements of a single data type*: a group of data, all with the same type (e.g. A Tensor could store a 4 x 4 x 6 matrix of 32-bit signed integers)."]}]