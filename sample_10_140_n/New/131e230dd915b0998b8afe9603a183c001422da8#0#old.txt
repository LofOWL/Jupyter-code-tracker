[{"block": 0, "type": "code", "linesLength": 8, "startIndex": 0, "lines": ["%reload_ext autoreload\n", "%autoreload 2\n", "%matplotlib inline\n", "\n", "from fastai.io import *\n", "from fastai.conv_learner import *\n", "\n", "from fastai.column_data import *"]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 8, "lines": ["## Setup"]}, {"block": 2, "type": "markdown", "linesLength": 1, "startIndex": 9, "lines": ["We're going to download the collected works of Nietzsche to use as our data for this class."]}, {"block": 3, "type": "code", "linesLength": 1, "startIndex": 10, "lines": ["PATH='data/nietzsche/'"]}, {"block": 4, "type": "code", "linesLength": 3, "startIndex": 11, "lines": ["get_data(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\", f'{PATH}nietzsche.txt')\n", "text = open(f'{PATH}nietzsche.txt').read()\n", "print('corpus length:', len(text))"]}, {"block": 5, "type": "code", "linesLength": 1, "startIndex": 14, "lines": ["text[:400]"]}, {"block": 6, "type": "code", "linesLength": 3, "startIndex": 15, "lines": ["chars = sorted(list(set(text)))\n", "vocab_size = len(chars)+1\n", "print('total chars:', vocab_size)"]}, {"block": 7, "type": "markdown", "linesLength": 1, "startIndex": 18, "lines": ["Sometimes it's useful to have a zero value in the dataset, e.g. for padding"]}, {"block": 8, "type": "code", "linesLength": 3, "startIndex": 19, "lines": ["chars.insert(0, \"\\0\")\n", "\n", "''.join(chars[1:-6])"]}, {"block": 9, "type": "markdown", "linesLength": 1, "startIndex": 22, "lines": ["Map from chars to indices and back again"]}, {"block": 10, "type": "code", "linesLength": 2, "startIndex": 23, "lines": ["char_indices = dict((c, i) for i, c in enumerate(chars))\n", "indices_char = dict((i, c) for i, c in enumerate(chars))"]}, {"block": 11, "type": "markdown", "linesLength": 1, "startIndex": 25, "lines": ["*idx* will be the data we use from now own - it simply converts all the characters to their index (based on the mapping above)"]}, {"block": 12, "type": "code", "linesLength": 3, "startIndex": 26, "lines": ["idx = [char_indices[c] for c in text]\n", "\n", "idx[:10]"]}, {"block": 13, "type": "code", "linesLength": 1, "startIndex": 29, "lines": ["''.join(indices_char[i] for i in idx[:70])"]}, {"block": 14, "type": "markdown", "linesLength": 1, "startIndex": 30, "lines": ["## Three char model"]}, {"block": 15, "type": "markdown", "linesLength": 1, "startIndex": 31, "lines": ["### Create inputs"]}, {"block": 16, "type": "markdown", "linesLength": 1, "startIndex": 32, "lines": ["Create a list of every 4th character, starting at the 0th, 1st, 2nd, then 3rd characters"]}, {"block": 17, "type": "code", "linesLength": 5, "startIndex": 33, "lines": ["cs=3\n", "c1_dat = [idx[i]   for i in range(0, len(idx)-1-cs, cs)]\n", "c2_dat = [idx[i+1] for i in range(0, len(idx)-1-cs, cs)]\n", "c3_dat = [idx[i+2] for i in range(0, len(idx)-1-cs, cs)]\n", "c4_dat = [idx[i+3] for i in range(0, len(idx)-1-cs, cs)]"]}, {"block": 18, "type": "markdown", "linesLength": 1, "startIndex": 38, "lines": ["Our inputs"]}, {"block": 19, "type": "code", "linesLength": 3, "startIndex": 39, "lines": ["x1 = np.stack(c1_dat[:-2])\n", "x2 = np.stack(c2_dat[:-2])\n", "x3 = np.stack(c3_dat[:-2])"]}, {"block": 20, "type": "markdown", "linesLength": 1, "startIndex": 42, "lines": ["Our output"]}, {"block": 21, "type": "code", "linesLength": 1, "startIndex": 43, "lines": ["y = np.stack(c4_dat[:-2])"]}, {"block": 22, "type": "markdown", "linesLength": 1, "startIndex": 44, "lines": ["The first 4 inputs and outputs"]}, {"block": 23, "type": "code", "linesLength": 1, "startIndex": 45, "lines": ["x1[:4], x2[:4], x3[:4]"]}, {"block": 24, "type": "code", "linesLength": 1, "startIndex": 46, "lines": ["y[:4]"]}, {"block": 25, "type": "code", "linesLength": 1, "startIndex": 47, "lines": ["x1.shape, y.shape"]}, {"block": 26, "type": "markdown", "linesLength": 1, "startIndex": 48, "lines": ["### Create and train model"]}, {"block": 27, "type": "markdown", "linesLength": 1, "startIndex": 49, "lines": ["Pick a size for our hidden state"]}, {"block": 28, "type": "code", "linesLength": 1, "startIndex": 50, "lines": ["n_hidden = 256"]}, {"block": 29, "type": "markdown", "linesLength": 1, "startIndex": 51, "lines": ["The number of latent factors to create (i.e. the size of the embedding matrix)"]}, {"block": 30, "type": "code", "linesLength": 1, "startIndex": 52, "lines": ["n_fac = 42"]}, {"block": 31, "type": "code", "linesLength": 25, "startIndex": 53, "lines": ["class Char3Model(nn.Module):\n", "    def __init__(self, vocab_size, n_fac):\n", "        super().__init__()\n", "        self.e = nn.Embedding(vocab_size, n_fac)\n", "\n", "        # The 'green arrow' from our diagram - the layer operation from input to hidden\n", "        self.l_in = nn.Linear(n_fac, n_hidden)\n", "\n", "        # The 'orange arrow' from our diagram - the layer operation from hidden to hidden\n", "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n", "        \n", "        # The 'blue arrow' from our diagram - the layer operation from hidden to output\n", "        self.l_out = nn.Linear(n_hidden, vocab_size)\n", "        \n", "    def forward(self, c1, c2, c3):\n", "        in1 = F.relu(self.l_in(self.e(c1)))\n", "        in2 = F.relu(self.l_in(self.e(c2)))\n", "        in3 = F.relu(self.l_in(self.e(c3)))\n", "        \n", "        h = V(torch.zeros(in1.size()).cuda())\n", "        h = F.tanh(self.l_hidden(h+in1))\n", "        h = F.tanh(self.l_hidden(h+in2))\n", "        h = F.tanh(self.l_hidden(h+in3))\n", "        \n", "        return F.log_softmax(self.l_out(h))"]}, {"block": 32, "type": "code", "linesLength": 1, "startIndex": 78, "lines": ["md = ColumnarModelData.from_arrays('.', [-1], np.stack([x1,x2,x3], axis=1), y, bs=512)"]}, {"block": 33, "type": "code", "linesLength": 1, "startIndex": 79, "lines": ["m = Char3Model(vocab_size, n_fac).cuda()"]}, {"block": 34, "type": "code", "linesLength": 3, "startIndex": 80, "lines": ["it = iter(md.trn_dl)\n", "*xs,yt = next(it)\n", "t = m(*V(xs))"]}, {"block": 35, "type": "code", "linesLength": 1, "startIndex": 83, "lines": ["opt = optim.Adam(m.parameters(), 1e-2)"]}, {"block": 36, "type": "code", "linesLength": 1, "startIndex": 84, "lines": ["fit(m, md, 1, opt, F.nll_loss)"]}, {"block": 37, "type": "code", "linesLength": 1, "startIndex": 85, "lines": ["set_lrs(opt, 0.001)"]}, {"block": 38, "type": "code", "linesLength": 1, "startIndex": 86, "lines": ["fit(m, md, 1, opt, F.nll_loss)"]}, {"block": 39, "type": "markdown", "linesLength": 1, "startIndex": 87, "lines": ["### Test model"]}, {"block": 40, "type": "code", "linesLength": 5, "startIndex": 88, "lines": ["def get_next(inp):\n", "    idxs = T(np.array([char_indices[c] for c in inp]))\n", "    p = m(*VV(idxs))\n", "    i = np.argmax(to_np(p))\n", "    return chars[i]"]}, {"block": 41, "type": "code", "linesLength": 1, "startIndex": 93, "lines": ["get_next('y. ')"]}, {"block": 42, "type": "code", "linesLength": 1, "startIndex": 94, "lines": ["get_next('ppl')"]}, {"block": 43, "type": "code", "linesLength": 1, "startIndex": 95, "lines": ["get_next(' th')"]}, {"block": 44, "type": "code", "linesLength": 1, "startIndex": 96, "lines": ["get_next('and')"]}, {"block": 45, "type": "markdown", "linesLength": 1, "startIndex": 97, "lines": ["## Our first RNN!"]}, {"block": 46, "type": "markdown", "linesLength": 1, "startIndex": 98, "lines": ["### Create inputs"]}, {"block": 47, "type": "markdown", "linesLength": 1, "startIndex": 99, "lines": ["This is the size of our unrolled RNN."]}, {"block": 48, "type": "code", "linesLength": 1, "startIndex": 100, "lines": ["cs=8"]}, {"block": 49, "type": "markdown", "linesLength": 1, "startIndex": 101, "lines": ["For each of 0 through 7, create a list of every 8th character with that starting point. These will be the 8 inputs to out model."]}, {"block": 50, "type": "code", "linesLength": 1, "startIndex": 102, "lines": ["c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(len(idx)-cs-1)]"]}, {"block": 51, "type": "markdown", "linesLength": 1, "startIndex": 103, "lines": ["Then create a list of the next character in each of these series. This will be the labels for our model."]}, {"block": 52, "type": "code", "linesLength": 1, "startIndex": 104, "lines": ["c_out_dat = [idx[j+cs] for j in range(len(idx)-cs-1)]"]}, {"block": 53, "type": "code", "linesLength": 1, "startIndex": 105, "lines": ["xs = np.stack(c_in_dat, axis=0)"]}, {"block": 54, "type": "code", "linesLength": 1, "startIndex": 106, "lines": ["xs.shape"]}, {"block": 55, "type": "code", "linesLength": 1, "startIndex": 107, "lines": ["y = np.stack(c_out_dat)"]}, {"block": 56, "type": "markdown", "linesLength": 1, "startIndex": 108, "lines": ["So each column below is one series of 8 characters from the text."]}, {"block": 57, "type": "code", "linesLength": 1, "startIndex": 109, "lines": ["xs[:cs,:cs]"]}, {"block": 58, "type": "markdown", "linesLength": 1, "startIndex": 110, "lines": ["...and this is the next character after each sequence."]}, {"block": 59, "type": "code", "linesLength": 1, "startIndex": 111, "lines": ["y[:cs]"]}, {"block": 60, "type": "markdown", "linesLength": 1, "startIndex": 112, "lines": ["### Create and train model"]}, {"block": 61, "type": "code", "linesLength": 1, "startIndex": 113, "lines": ["val_idx = get_cv_idxs(len(idx)-cs-1)"]}, {"block": 62, "type": "code", "linesLength": 1, "startIndex": 114, "lines": ["md = ColumnarModelData.from_arrays('.', val_idx, xs, y, bs=512)"]}, {"block": 63, "type": "code", "linesLength": 17, "startIndex": 115, "lines": ["class CharLoopModel(nn.Module):\n", "    # This is an RNN!\n", "    def __init__(self, vocab_size, n_fac):\n", "        super().__init__()\n", "        self.e = nn.Embedding(vocab_size, n_fac)\n", "        self.l_in = nn.Linear(n_fac, n_hidden)\n", "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n", "        self.l_out = nn.Linear(n_hidden, vocab_size)\n", "        \n", "    def forward(self, *cs):\n", "        bs = cs[0].size(0)\n", "        h = V(torch.zeros(bs, n_hidden).cuda())\n", "        for c in cs:\n", "            inp = F.relu(self.l_in(self.e(c)))\n", "            h = F.tanh(self.l_hidden(h+inp))\n", "        \n", "        return F.log_softmax(self.l_out(h), dim=-1)"]}, {"block": 64, "type": "code", "linesLength": 2, "startIndex": 132, "lines": ["m = CharLoopModel(vocab_size, n_fac).cuda()\n", "opt = optim.Adam(m.parameters(), 1e-2)"]}, {"block": 65, "type": "code", "linesLength": 1, "startIndex": 134, "lines": ["fit(m, md, 1, opt, F.nll_loss)"]}, {"block": 66, "type": "code", "linesLength": 1, "startIndex": 135, "lines": ["set_lrs(opt, 0.001)"]}, {"block": 67, "type": "code", "linesLength": 1, "startIndex": 136, "lines": ["fit(m, md, 1, opt, F.nll_loss)"]}, {"block": 68, "type": "code", "linesLength": 17, "startIndex": 137, "lines": ["class CharLoopConcatModel(nn.Module):\n", "    def __init__(self, vocab_size, n_fac):\n", "        super().__init__()\n", "        self.e = nn.Embedding(vocab_size, n_fac)\n", "        self.l_in = nn.Linear(n_fac+n_hidden, n_hidden)\n", "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n", "        self.l_out = nn.Linear(n_hidden, vocab_size)\n", "        \n", "    def forward(self, *cs):\n", "        bs = cs[0].size(0)\n", "        h = V(torch.zeros(bs, n_hidden).cuda())\n", "        for c in cs:\n", "            inp = torch.cat((h, self.e(c)), 1)\n", "            inp = F.relu(self.l_in(inp))\n", "            h = F.tanh(self.l_hidden(inp))\n", "        \n", "        return F.log_softmax(self.l_out(h), dim=-1)"]}, {"block": 69, "type": "code", "linesLength": 2, "startIndex": 154, "lines": ["m = CharLoopConcatModel(vocab_size, n_fac).cuda()\n", "opt = optim.Adam(m.parameters(), 1e-3)"]}, {"block": 70, "type": "code", "linesLength": 3, "startIndex": 156, "lines": ["it = iter(md.trn_dl)\n", "*xs,yt = next(it)\n", "t = m(*V(xs))"]}, {"block": 71, "type": "code", "linesLength": 1, "startIndex": 159, "lines": ["fit(m, md, 1, opt, F.nll_loss)"]}, {"block": 72, "type": "code", "linesLength": 1, "startIndex": 160, "lines": ["set_lrs(opt, 1e-4)"]}, {"block": 73, "type": "code", "linesLength": 1, "startIndex": 161, "lines": ["fit(m, md, 1, opt, F.nll_loss)"]}, {"block": 74, "type": "markdown", "linesLength": 1, "startIndex": 162, "lines": ["### Test model"]}, {"block": 75, "type": "code", "linesLength": 5, "startIndex": 163, "lines": ["def get_next(inp):\n", "    idxs = T(np.array([char_indices[c] for c in inp]))\n", "    p = m(*VV(idxs))\n", "    i = np.argmax(to_np(p))\n", "    return chars[i]"]}, {"block": 76, "type": "code", "linesLength": 1, "startIndex": 168, "lines": ["get_next('for thos')"]}, {"block": 77, "type": "code", "linesLength": 1, "startIndex": 169, "lines": ["get_next('part of ')"]}, {"block": 78, "type": "code", "linesLength": 1, "startIndex": 170, "lines": ["get_next('queens a')"]}, {"block": 79, "type": "markdown", "linesLength": 1, "startIndex": 171, "lines": ["## RNN with pytorch"]}, {"block": 80, "type": "code", "linesLength": 14, "startIndex": 172, "lines": ["class CharRnn(nn.Module):\n", "    def __init__(self, vocab_size, n_fac):\n", "        super().__init__()\n", "        self.e = nn.Embedding(vocab_size, n_fac)\n", "        self.rnn = nn.RNN(n_fac, n_hidden)\n", "        self.l_out = nn.Linear(n_hidden, vocab_size)\n", "        \n", "    def forward(self, *cs):\n", "        bs = cs[0].size(0)\n", "        h = V(torch.zeros(1, bs, n_hidden))\n", "        inp = self.e(torch.stack(cs))\n", "        outp,h = self.rnn(inp, h)\n", "        \n", "        return F.log_softmax(self.l_out(outp[-1]), dim=-1)"]}, {"block": 81, "type": "code", "linesLength": 2, "startIndex": 186, "lines": ["m = CharRnn(vocab_size, n_fac).cuda()\n", "opt = optim.Adam(m.parameters(), 1e-3)"]}, {"block": 82, "type": "code", "linesLength": 2, "startIndex": 188, "lines": ["it = iter(md.trn_dl)\n", "*xs,yt = next(it)"]}, {"block": 83, "type": "code", "linesLength": 2, "startIndex": 190, "lines": ["t = m.e(V(torch.stack(xs)))\n", "t.size()"]}, {"block": 84, "type": "code", "linesLength": 3, "startIndex": 192, "lines": ["ht = V(torch.zeros(1, 512,n_hidden))\n", "outp, hn = m.rnn(t, ht)\n", "outp.size(), hn.size()"]}, {"block": 85, "type": "code", "linesLength": 1, "startIndex": 195, "lines": ["t = m(*V(xs)); t.size()"]}, {"block": 86, "type": "code", "linesLength": 1, "startIndex": 196, "lines": ["fit(m, md, 4, opt, F.nll_loss)"]}, {"block": 87, "type": "code", "linesLength": 1, "startIndex": 197, "lines": ["set_lrs(opt, 1e-4)"]}, {"block": 88, "type": "code", "linesLength": 1, "startIndex": 198, "lines": ["fit(m, md, 2, opt, F.nll_loss)"]}, {"block": 89, "type": "markdown", "linesLength": 1, "startIndex": 199, "lines": ["### Test model"]}, {"block": 90, "type": "code", "linesLength": 5, "startIndex": 200, "lines": ["def get_next(inp):\n", "    idxs = T(np.array([char_indices[c] for c in inp]))\n", "    p = m(*VV(idxs))\n", "    i = np.argmax(to_np(p))\n", "    return chars[i]"]}, {"block": 91, "type": "code", "linesLength": 1, "startIndex": 205, "lines": ["get_next('for thos')"]}, {"block": 92, "type": "code", "linesLength": 7, "startIndex": 206, "lines": ["def get_next_n(inp, n):\n", "    res = inp\n", "    for i in range(n):\n", "        c = get_next(inp)\n", "        res += c\n", "        inp = inp[1:]+c\n", "    return res"]}, {"block": 93, "type": "code", "linesLength": 1, "startIndex": 213, "lines": ["get_next_n('for thos', 40)"]}, {"block": 94, "type": "markdown", "linesLength": 1, "startIndex": 214, "lines": ["## Multi-output model"]}, {"block": 95, "type": "markdown", "linesLength": 1, "startIndex": 215, "lines": ["### Setup"]}, {"block": 96, "type": "markdown", "linesLength": 1, "startIndex": 216, "lines": ["Let's take non-overlapping sets of characters this time"]}, {"block": 97, "type": "code", "linesLength": 1, "startIndex": 217, "lines": ["c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(0, len(idx)-cs-1, cs)]"]}, {"block": 98, "type": "markdown", "linesLength": 1, "startIndex": 218, "lines": ["Then create the exact same thing, offset by 1, as our labels"]}, {"block": 99, "type": "code", "linesLength": 1, "startIndex": 219, "lines": ["c_out_dat = [[idx[i+j] for i in range(cs)] for j in range(1, len(idx)-cs, cs)]"]}, {"block": 100, "type": "code", "linesLength": 2, "startIndex": 220, "lines": ["xs = np.stack(c_in_dat)\n", "xs.shape"]}, {"block": 101, "type": "code", "linesLength": 2, "startIndex": 222, "lines": ["ys = np.stack(c_out_dat)\n", "ys.shape"]}, {"block": 102, "type": "code", "linesLength": 1, "startIndex": 224, "lines": ["xs[:cs,:cs]"]}, {"block": 103, "type": "code", "linesLength": 1, "startIndex": 225, "lines": ["ys[:cs,:cs]"]}, {"block": 104, "type": "markdown", "linesLength": 1, "startIndex": 226, "lines": ["### Create and train model"]}, {"block": 105, "type": "code", "linesLength": 1, "startIndex": 227, "lines": ["val_idx = get_cv_idxs(len(xs)-cs-1)"]}, {"block": 106, "type": "code", "linesLength": 1, "startIndex": 228, "lines": ["md = ColumnarModelData.from_arrays('.', val_idx, xs, ys, bs=512)"]}, {"block": 107, "type": "code", "linesLength": 13, "startIndex": 229, "lines": ["class CharSeqRnn(nn.Module):\n", "    def __init__(self, vocab_size, n_fac):\n", "        super().__init__()\n", "        self.e = nn.Embedding(vocab_size, n_fac)\n", "        self.rnn = nn.RNN(n_fac, n_hidden)\n", "        self.l_out = nn.Linear(n_hidden, vocab_size)\n", "        \n", "    def forward(self, *cs):\n", "        bs = cs[0].size(0)\n", "        h = V(torch.zeros(1, bs, n_hidden))\n", "        inp = self.e(torch.stack(cs))\n", "        outp,h = self.rnn(inp, h)\n", "        return F.log_softmax(self.l_out(outp), dim=-1)"]}, {"block": 108, "type": "code", "linesLength": 2, "startIndex": 242, "lines": ["m = CharSeqRnn(vocab_size, n_fac).cuda()\n", "opt = optim.Adam(m.parameters(), 1e-3)"]}, {"block": 109, "type": "code", "linesLength": 2, "startIndex": 244, "lines": ["it = iter(md.trn_dl)\n", "*xst,yt = next(it)"]}, {"block": 110, "type": "code", "linesLength": 4, "startIndex": 246, "lines": ["def nll_loss_seq(inp, targ):\n", "    sl,bs,nh = inp.size()\n", "    targ = targ.transpose(0,1).contiguous().view(-1)\n", "    return F.nll_loss(inp.view(-1,nh), targ)"]}, {"block": 111, "type": "code", "linesLength": 1, "startIndex": 250, "lines": ["fit(m, md, 4, opt, nll_loss_seq)"]}, {"block": 112, "type": "code", "linesLength": 1, "startIndex": 251, "lines": ["set_lrs(opt, 1e-4)"]}, {"block": 113, "type": "code", "linesLength": 1, "startIndex": 252, "lines": ["fit(m, md, 1, opt, nll_loss_seq)"]}, {"block": 114, "type": "markdown", "linesLength": 1, "startIndex": 253, "lines": ["### Identity init!"]}, {"block": 115, "type": "code", "linesLength": 2, "startIndex": 254, "lines": ["m = CharSeqRnn(vocab_size, n_fac).cuda()\n", "opt = optim.Adam(m.parameters(), 1e-2)"]}, {"block": 116, "type": "code", "linesLength": 1, "startIndex": 256, "lines": ["m.rnn.weight_hh_l0.data.copy_(torch.eye(n_hidden))"]}, {"block": 117, "type": "code", "linesLength": 1, "startIndex": 257, "lines": ["fit(m, md, 4, opt, nll_loss_seq)"]}, {"block": 118, "type": "code", "linesLength": 1, "startIndex": 258, "lines": ["set_lrs(opt, 1e-3)"]}, {"block": 119, "type": "code", "linesLength": 1, "startIndex": 259, "lines": ["fit(m, md, 4, opt, nll_loss_seq)"]}, {"block": 120, "type": "markdown", "linesLength": 1, "startIndex": 260, "lines": ["## Stateful model"]}, {"block": 121, "type": "markdown", "linesLength": 1, "startIndex": 261, "lines": ["### Setup"]}, {"block": 122, "type": "code", "linesLength": 13, "startIndex": 262, "lines": ["from torchtext import vocab, data\n", "\n", "from fastai.nlp import *\n", "from fastai.lm_rnn import *\n", "\n", "PATH='data/nietzsche/'\n", "\n", "TRN_PATH = 'trn/'\n", "VAL_PATH = 'val/'\n", "TRN = f'{PATH}{TRN_PATH}'\n", "VAL = f'{PATH}{VAL_PATH}'\n", "\n", "%ls {PATH}"]}, {"block": 123, "type": "code", "linesLength": 1, "startIndex": 275, "lines": ["%ls {PATH}trn"]}, {"block": 124, "type": "code", "linesLength": 7, "startIndex": 276, "lines": ["TEXT = data.Field(lower=True, tokenize=list)\n", "bs=64; bptt=8; n_fac=42; n_hidden=256\n", "\n", "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n", "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)\n", "\n", "len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"]}, {"block": 125, "type": "markdown", "linesLength": 1, "startIndex": 283, "lines": ["### RNN"]}, {"block": 126, "type": "code", "linesLength": 17, "startIndex": 284, "lines": ["class CharSeqStatefulRnn(nn.Module):\n", "    def __init__(self, vocab_size, n_fac, bs):\n", "        self.vocab_size = vocab_size\n", "        super().__init__()\n", "        self.e = nn.Embedding(vocab_size, n_fac)\n", "        self.rnn = nn.RNN(n_fac, n_hidden)\n", "        self.l_out = nn.Linear(n_hidden, vocab_size)\n", "        self.init_hidden(bs)\n", "        \n", "    def forward(self, cs):\n", "        bs = cs[0].size(0)\n", "        if self.h.size(1) != bs: self.init_hidden(bs)\n", "        outp,h = self.rnn(self.e(cs), self.h)\n", "        self.h = repackage_var(h)\n", "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n", "    \n", "    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))"]}, {"block": 127, "type": "code", "linesLength": 2, "startIndex": 301, "lines": ["m = CharSeqStatefulRnn(md.nt, n_fac, 512).cuda()\n", "opt = optim.Adam(m.parameters(), 1e-3)"]}, {"block": 128, "type": "code", "linesLength": 1, "startIndex": 303, "lines": ["fit(m, md, 4, opt, F.nll_loss)"]}, {"block": 129, "type": "code", "linesLength": 3, "startIndex": 304, "lines": ["set_lrs(opt, 1e-4)\n", "\n", "fit(m, md, 4, opt, F.nll_loss)"]}, {"block": 130, "type": "markdown", "linesLength": 1, "startIndex": 307, "lines": ["### RNN loop"]}, {"block": 131, "type": "code", "linesLength": 4, "startIndex": 308, "lines": ["# From the pytorch source\n", "\n", "def RNNCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n", "    return F.tanh(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))"]}, {"block": 132, "type": "code", "linesLength": 22, "startIndex": 312, "lines": ["class CharSeqStatefulRnn2(nn.Module):\n", "    def __init__(self, vocab_size, n_fac, bs):\n", "        super().__init__()\n", "        self.vocab_size = vocab_size\n", "        self.e = nn.Embedding(vocab_size, n_fac)\n", "        self.rnn = nn.RNNCell(n_fac, n_hidden)\n", "        self.l_out = nn.Linear(n_hidden, vocab_size)\n", "        self.init_hidden(bs)\n", "        \n", "    def forward(self, cs):\n", "        bs = cs[0].size(0)\n", "        if self.h.size(1) != bs: self.init_hidden(bs)\n", "        outp = []\n", "        o = self.h\n", "        for c in cs: \n", "            o = self.rnn(self.e(c), o)\n", "            outp.append(o)\n", "        outp = self.l_out(torch.stack(outp))\n", "        self.h = repackage_var(o)\n", "        return F.log_softmax(outp, dim=-1).view(-1, self.vocab_size)\n", "    \n", "    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))"]}, {"block": 133, "type": "code", "linesLength": 2, "startIndex": 334, "lines": ["m = CharSeqStatefulRnn2(md.nt, n_fac, 512).cuda()\n", "opt = optim.Adam(m.parameters(), 1e-3)"]}, {"block": 134, "type": "code", "linesLength": 1, "startIndex": 336, "lines": ["fit(m, md, 4, opt, F.nll_loss)"]}, {"block": 135, "type": "markdown", "linesLength": 1, "startIndex": 337, "lines": ["### GRU"]}, {"block": 136, "type": "code", "linesLength": 17, "startIndex": 338, "lines": ["class CharSeqStatefulGRU(nn.Module):\n", "    def __init__(self, vocab_size, n_fac, bs):\n", "        super().__init__()\n", "        self.vocab_size = vocab_size\n", "        self.e = nn.Embedding(vocab_size, n_fac)\n", "        self.rnn = nn.GRU(n_fac, n_hidden)\n", "        self.l_out = nn.Linear(n_hidden, vocab_size)\n", "        self.init_hidden(bs)\n", "        \n", "    def forward(self, cs):\n", "        bs = cs[0].size(0)\n", "        if self.h.size(1) != bs: self.init_hidden(bs)\n", "        outp,h = self.rnn(self.e(cs), self.h)\n", "        self.h = repackage_var(h)\n", "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n", "    \n", "    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))"]}, {"block": 137, "type": "code", "linesLength": 12, "startIndex": 355, "lines": ["# From the pytorch source code - for reference\n", "\n", "def GRUCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n", "    gi = F.linear(input, w_ih, b_ih)\n", "    gh = F.linear(hidden, w_hh, b_hh)\n", "    i_r, i_i, i_n = gi.chunk(3, 1)\n", "    h_r, h_i, h_n = gh.chunk(3, 1)\n", "\n", "    resetgate = F.sigmoid(i_r + h_r)\n", "    inputgate = F.sigmoid(i_i + h_i)\n", "    newgate = F.tanh(i_n + resetgate * h_n)\n", "    return newgate + inputgate * (hidden - newgate)"]}, {"block": 138, "type": "code", "linesLength": 3, "startIndex": 367, "lines": ["m = CharSeqStatefulGRU(md.nt, n_fac, 512).cuda()\n", "\n", "opt = optim.Adam(m.parameters(), 1e-3)"]}, {"block": 139, "type": "code", "linesLength": 1, "startIndex": 370, "lines": ["fit(m, md, 6, opt, F.nll_loss)"]}, {"block": 140, "type": "code", "linesLength": 1, "startIndex": 371, "lines": ["set_lrs(opt, 1e-4)"]}, {"block": 141, "type": "code", "linesLength": 1, "startIndex": 372, "lines": ["fit(m, md, 3, opt, F.nll_loss)"]}, {"block": 142, "type": "markdown", "linesLength": 1, "startIndex": 373, "lines": ["### Putting it all together: LSTM"]}, {"block": 143, "type": "code", "linesLength": 3, "startIndex": 374, "lines": ["from fastai import sgdr\n", "\n", "n_hidden=512"]}, {"block": 144, "type": "code", "linesLength": 19, "startIndex": 377, "lines": ["class CharSeqStatefulLSTM(nn.Module):\n", "    def __init__(self, vocab_size, n_fac, bs, nl):\n", "        super().__init__()\n", "        self.vocab_size,self.nl = vocab_size,nl\n", "        self.e = nn.Embedding(vocab_size, n_fac)\n", "        self.rnn = nn.LSTM(n_fac, n_hidden, nl, dropout=0.5)\n", "        self.l_out = nn.Linear(n_hidden, vocab_size)\n", "        self.init_hidden(bs)\n", "        \n", "    def forward(self, cs):\n", "        bs = cs[0].size(0)\n", "        if self.h[0].size(1) != bs: self.init_hidden(bs)\n", "        outp,h = self.rnn(self.e(cs), self.h)\n", "        self.h = repackage_var(h)\n", "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n", "    \n", "    def init_hidden(self, bs):\n", "        self.h = (V(torch.zeros(self.nl, bs, n_hidden)),\n", "                  V(torch.zeros(self.nl, bs, n_hidden)))"]}, {"block": 145, "type": "code", "linesLength": 2, "startIndex": 396, "lines": ["m = CharSeqStatefulLSTM(md.nt, n_fac, 512, 2).cuda()\n", "lo = LayerOptimizer(optim.Adam, m, 1e-2, 1e-5)"]}, {"block": 146, "type": "code", "linesLength": 1, "startIndex": 398, "lines": ["os.makedirs(f'{PATH}models', exist_ok=True)"]}, {"block": 147, "type": "code", "linesLength": 1, "startIndex": 399, "lines": ["fit(m, md, 2, lo.opt, F.nll_loss)"]}, {"block": 148, "type": "code", "linesLength": 3, "startIndex": 400, "lines": ["on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_{cycle}')\n", "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n", "fit(m, md, 2**4-1, lo.opt, F.nll_loss, callbacks=cb)"]}, {"block": 149, "type": "code", "linesLength": 3, "startIndex": 403, "lines": ["on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_{cycle}')\n", "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n", "fit(m, md, 2**6-1, lo.opt, F.nll_loss, callbacks=cb)"]}, {"block": 150, "type": "code", "linesLength": 0, "startIndex": 406, "lines": []}, {"block": 151, "type": "markdown", "linesLength": 1, "startIndex": 406, "lines": ["### Test"]}, {"block": 152, "type": "code", "linesLength": 5, "startIndex": 407, "lines": ["def get_next(inp):\n", "    idxs = TEXT.numericalize(inp)\n", "    p = m(VV(idxs.transpose(0,1)))\n", "    r = torch.multinomial(p[-1].exp(), 1)\n", "    return TEXT.vocab.itos[to_np(r)[0]]"]}, {"block": 153, "type": "code", "linesLength": 1, "startIndex": 412, "lines": ["get_next('for thos')"]}, {"block": 154, "type": "code", "linesLength": 7, "startIndex": 413, "lines": ["def get_next_n(inp, n):\n", "    res = inp\n", "    for i in range(n):\n", "        c = get_next(inp)\n", "        res += c\n", "        inp = inp[1:]+c\n", "    return res"]}, {"block": 155, "type": "code", "linesLength": 1, "startIndex": 420, "lines": ["print(get_next_n('for thos', 400))"]}, {"block": 156, "type": "code", "linesLength": 0, "startIndex": 421, "lines": []}]