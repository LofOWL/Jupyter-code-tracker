[{"block": 0, "type": "markdown", "linesLength": 3, "startIndex": 0, "lines": ["<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n", "\n", "<i>Licensed under the MIT License.</i>"]}, {"block": 1, "type": "markdown", "linesLength": 24, "startIndex": 3, "lines": ["# Train SAR on MovieLens with Azure Machine Learning (Python, CPU)\n", "\n", "This notebook provides an exmaple of how to train SAR on remote compute resources. Details of SAR algorithm can be found in [SAR Python CPU Movielens](https://github.com/Microsoft/Recommenders/blob/master/notebooks/00_quick_start/sar_movielens.ipynb) notebook. \n", "\n", "This notebook provides an example of how to utilize and evaluate SAR in Python on a CPU.\n", "\n", "This notebook showcases what are the changes required to run local script on AzureML targets. AzureML comes with auto-scaling and gpu options and can greatly accelerate model training. There're a few more advanced AzureML notebooks in this repo. \n", "\n", "SAR is a fast scalable adaptive algorithm for personalized recommendations based on user transaction history. It produces easily explainable / interpretable recommendations and handles \"cold item\" and \"semi-cold user\" scenarios. SAR is a kind of neighborhood based algorithm (as discussed in [Recommender Systems by Aggarwal](https://dl.acm.org/citation.cfm?id=2931100)) which is intended for ranking top items for each user. \n", "\n", "SAR recommends items that are most ***similar*** to the ones that the user already has an existing ***affinity*** for. Two items are ***similar*** if the users who have interacted with one item are also likely to have interacted with another. A user has an ***affinity*** to an item if they have interacted with it in the past.\n", "\n", "### Advantages of SAR:\n", "- High accuracy for an easy to train and deploy algorithm\n", "- Fast training, only requiring simple counting to construct matrices used at prediction time. \n", "- Fast scoring, only involving multiplication of the similarity matric with an affinity vector\n", "\n", "### Advantages of using AML:\n", "- Run large dataset quickly with scaling capability\n", "\n", "### Notes to use SAR properly:\n", "- Since it does not use item or user features, it can be at a disadvantage against algorithms that do.\n", "- It's memory-hungry, requiring the creation of an $mxm$ sparse square matrix (where $m$ is the number of items). This can also be a problem for many matrix factorization algorithms.\n", "- SAR favors an implicit rating scenario and it does not predict ratings."]}, {"block": 2, "type": "markdown", "linesLength": 22, "startIndex": 27, "lines": ["# 0 Setup environment\n", "### 1 Configure AzureML workspace and create experiment\n", "**AzureML workspace** is a foundational block in the cloud that you use to experiment, train, and deploy machine learning models via AzureML service. In this notebook, we 1) create a workspace from [**Azure portal**](https://portal.azure.com) and 2) configure from this notebook.\n", "\n", "You can find more details about the setup and configure processes from the following links:\n", "* [Quickstart with Azure portal](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-get-started)\n", "* [Quickstart with Python SDK](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python)\n", "\n", "#### 1.1 Create a workspace\n", "1. Sign in to the [Azure portal](https://portal.azure.com) by using the credentials for the Azure subscription you use.\n", "2. Select **Create a resource** menu, search for **Machine Learning service workspace** select **Create** button.\n", "3. In the **ML service workspace** pane, configure your workspace with entering the *workspace name* and *resource group* (or **create new** resource group if you don't have one already), and select **Create**. It can take a few moments to create the workspace.\n", "\n", "#### 1.2 Configure\n", "To configure this notebook to communicate with the workspace, type in your Azure subscription id, the resource group name and workspace name to `<subscription-id>`, `<resource-group>`, `<workspace-name>` in the above notebook cell. Alternatively, you can create a *.\\\\aml_config\\\\config.json* file with the following contents:\n", "```\n", "{\n", "    \"subscription_id\": \"<subscription-id>\",\n", "    \"resource_group\": \"<resource-group>\",\n", "    \"workspace_name\": \"<workspace-name>\"\n", "}\n", "```"]}, {"block": 3, "type": "code", "linesLength": 26, "startIndex": 49, "lines": ["# set the environment path to find Recommenders\n", "import sys\n", "sys.path.append(\"../../\")\n", "import os\n", "import shutil\n", "\n", "from reco_utils.dataset import movielens\n", "\n", "import azureml\n", "from azureml.core import Workspace, Run, Experiment\n", "from azureml.core.compute import ComputeTarget, AmlCompute\n", "from azureml.train.estimator import Estimator\n", "from azureml.widgets import RunDetails\n", "\n", "# AzureML workspace info. Note, will look up \"aml_config\\config.json\" first, then fall back to use this\n", "subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"<my-subscription-id>\")\n", "resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"<my-resource-group>\")\n", "workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"<my-workspace-name>\")\n", "workspace_region = os.getenv(\"WORKSPACE_REGION\", default=\"eastus2\")\n", "\n", "# Remote compute (cluster) configuration. If you want to save the cost more, set these to small.\n", "VM_SIZE = 'STANDARD_D2_V2'\n", "VM_PRIORITY = 'lowpriority'\n", "# Cluster nodes\n", "MIN_NODES = 4\n", "MAX_NODES = 8"]}, {"block": 4, "type": "markdown", "linesLength": 1, "startIndex": 75, "lines": ["Now let's see if everything is ready!"]}, {"block": 5, "type": "code", "linesLength": 21, "startIndex": 76, "lines": ["# Connect to a workspace\n", "try:\n", "    ws = Workspace.from_config()\n", "except aml.exceptions.UserErrorException:\n", "    try:\n", "        ws = Workspace(\n", "            subscription_id=subscription_id,\n", "            resource_group=resource_group,\n", "            workspace_name=workspace_name\n", "        )\n", "        ws.write_config()\n", "    except:\n", "        ws = None\n", "\n", "if ws is None:\n", "    raise ValueError(\n", "        \"\"\"Cannot access the AzureML workspace w/ the config info provided.\n", "        Please check if you entered the correct id, group name and workspace name\"\"\"\n", "    )\n", "else:\n", "    print(\"AzureML workspace name: \", ws.name)"]}, {"block": 6, "type": "markdown", "linesLength": 4, "startIndex": 97, "lines": ["### 2 Create Remote Compute Target\n", "We create a cpu cluster as our **remote compute target**. If a cluster with the same name is already exist in your workspace, the script will load it instead. You can see [this document](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets) to learn more about setting up a compute target on different locations.\n", "\n", "This notebook selects 'STANDARD_NC6' virtual machine (VM) and sets it's priority as lowpriority to save the cost."]}, {"block": 7, "type": "code", "linesLength": 19, "startIndex": 101, "lines": ["CLUSTER_NAME = 'cpucluster'\n", "\n", "try:\n", "    compute_target = ComputeTarget(workspace=ws, name=CLUSTER_NAME)\n", "    print(\"Found existing compute target\")\n", "except:\n", "    print(\"Creating a new compute target...\")\n", "    compute_config = AmlCompute.provisioning_configuration(\n", "        vm_size=VM_SIZE,\n", "        vm_priority=VM_PRIORITY,\n", "        min_nodes=MIN_NODES,\n", "        max_nodes=MAX_NODES\n", "    )\n", "    # create the cluster\n", "    compute_target = ComputeTarget.create(ws, CLUSTER_NAME, compute_config)\n", "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n", "\n", "# Use the 'status' property to get a detailed status for the current cluster. \n", "print(compute_target.status.serialize())"]}, {"block": 8, "type": "code", "linesLength": 5, "startIndex": 120, "lines": ["# top k items to recommend\n", "TOP_K = 10\n", "\n", "# Select Movielens data size: 100k, 1m, 10m, or 20m\n", "MOVIELENS_DATA_SIZE = '1m'"]}, {"block": 9, "type": "markdown", "linesLength": 4, "startIndex": 125, "lines": ["### 3 Download dataset and upload to data store\n", "Now make the data accessible remotely by uploading that data from your local machine into Azure so it can be accessed for remote training. The datastore is a convenient construct associated with your workspace for you to upload/download data, and interact with it from your remote compute targets. It is backed by Azure blob storage account.\n", "\n", "The data files are uploaded into a directory named `data` at the root of the datastore. Here's detailed documentation about [datastore](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data)."]}, {"block": 10, "type": "code", "linesLength": 20, "startIndex": 129, "lines": ["DATA_DIR = 'aml_data'\n", "TARGET_DIR = 'movielens'\n", "#os.makedirs('./data', exist_ok = True)\n", "\n", "os.makedirs(DATA_DIR, exist_ok=True)\n", "print(MOVIELENS_DATA_SIZE)\n", "\n", "# download dataset\n", "data = movielens.load_pandas_df(\n", "    size=MOVIELENS_DATA_SIZE,\n", "    header=['UserId','MovieId','Rating','Timestamp']\n", ")\n", "\n", "# upload dataset to workspace datastore\n", "data_file_name = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_data.pkl\"\n", "data.to_pickle(os.path.join(DATA_DIR, data_file_name))\n", "\n", "ds = ws.get_default_datastore()\n", "\n", "ds.upload(src_dir=DATA_DIR, target_path=TARGET_DIR, overwrite=True, show_progress=True)"]}, {"block": 11, "type": "markdown", "linesLength": 3, "startIndex": 149, "lines": ["# 1 Prepare training script\n", "### Create a directory\n", "Create a directory to deliver the necessary code from your computer to the remote resource."]}, {"block": 12, "type": "code", "linesLength": 2, "startIndex": 152, "lines": ["script_folder = './movielens-sar'\n", "os.makedirs(script_folder, exist_ok=True)"]}, {"block": 13, "type": "markdown", "linesLength": 2, "startIndex": 154, "lines": ["### Create a training script\n", "To submit the job to the cluster, first create a training script. Run the following code to create the training script called `train.py` in the directory you just created. This training adds a regularization rate to the training algorithm, so produces a slightly different model than the local version."]}, {"block": 14, "type": "code", "linesLength": 96, "startIndex": 156, "lines": ["%%writefile $script_folder/train.py\n", "\n", "import argparse\n", "import os\n", "import numpy as np\n", "import pandas as pd\n", "import itertools\n", "import logging\n", "import time\n", "\n", "from azureml.core import Run\n", "from sklearn.externals import joblib\n", "\n", "from reco_utils.dataset import movielens\n", "from reco_utils.dataset.python_splitters import python_random_split\n", "from reco_utils.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n", "from reco_utils.recommender.sar.sar_singlenode import SARSingleNode\n", "\n", "TARGET_DIR = 'movielens'\n", "OUTPUT_FILE_NAME = 'outputs/movielens_sar_model.pkl'\n", "\n", "\n", "# get hold of the current run\n", "run = Run.get_context()\n", "\n", "# let user feed in 2 parameters, the location of the data files (from datastore), and the regularization rate of the logistic regression model\n", "parser = argparse.ArgumentParser()\n", "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n", "parser.add_argument('--data-file', type=str, dest='data_file', help='data file name')\n", "parser.add_argument('--top-k', type=int, dest='top_k', default=10, help='top k items to recommend')\n", "parser.add_argument('--data-size', type=str, dest='data_size', default=10, help='Movielens data size: 100k, 1m, 10m, or 20m')\n", "args = parser.parse_args()\n", "\n", "data_pickle_path = os.path.join(args.data_folder, args.data_file)\n", "\n", "data = pd.read_pickle(path=data_pickle_path)\n", "\n", "train, test = python_random_split(data)\n", "\n", "# instantiate the SAR algorithm and set the index\n", "header = {\n", "    \"col_user\": \"UserId\",\n", "    \"col_item\": \"MovieId\",\n", "    \"col_rating\": \"Rating\",\n", "    \"col_timestamp\": \"Timestamp\",\n", "}\n", "\n", "logging.basicConfig(level=logging.DEBUG, \n", "                    format='%(asctime)s %(levelname)-8s %(message)s')\n", "\n", "model = SARSingleNode(\n", "    remove_seen=True, similarity_type=\"jaccard\", \n", "    time_decay_coefficient=30, time_now=None, timedecay_formula=True, **header\n", ")\n", "\n", "# train the SAR model\n", "start_time = time.time()\n", "\n", "model.fit(train)\n", "\n", "train_time = time.time() - start_time\n", "run.log(name=\"Training time\", value=train_time)\n", "\n", "start_time = time.time()\n", "\n", "top_k = model.recommend_k_items(test)\n", "\n", "test_time = time.time() - start_time\n", "run.log(name=\"Prediction time\", value=test_time)\n", "\n", "# TODO: remove this call when the model returns same type as input\n", "top_k['UserId'] = pd.to_numeric(top_k['UserId'])\n", "top_k['MovieId'] = pd.to_numeric(top_k['MovieId'])\n", "\n", "# evaluate\n", "eval_map = map_at_k(test, top_k, col_user=\"UserId\", col_item=\"MovieId\", \n", "                    col_rating=\"Rating\", col_prediction=\"prediction\", \n", "                    relevancy_method=\"top_k\", k=args.top_k)\n", "eval_ndcg = ndcg_at_k(test, top_k, col_user=\"UserId\", col_item=\"MovieId\", \n", "                      col_rating=\"Rating\", col_prediction=\"prediction\", \n", "                      relevancy_method=\"top_k\", k=args.top_k)\n", "eval_precision = precision_at_k(test, top_k, col_user=\"UserId\", col_item=\"MovieId\", \n", "                                col_rating=\"Rating\", col_prediction=\"prediction\", \n", "                                relevancy_method=\"top_k\", k=args.top_k)\n", "eval_recall = recall_at_k(test, top_k, col_user=\"UserId\", col_item=\"MovieId\", \n", "                          col_rating=\"Rating\", col_prediction=\"prediction\", \n", "                          relevancy_method=\"top_k\", k=args.top_k)\n", "\n", "run.log(\"map\", eval_map)\n", "run.log(\"ndcg\", eval_ndcg)\n", "run.log(\"precision\", eval_precision)\n", "run.log(\"recall\", eval_recall)\n", "\n", "os.makedirs('outputs', exist_ok=True)\n", "# note file saved in the outputs folder is automatically uploaded into experiment record\n", "joblib.dump(value=model, filename=OUTPUT_FILE_NAME)"]}, {"block": 15, "type": "code", "linesLength": 2, "startIndex": 252, "lines": ["shutil.rmtree('./movielens-sar/reco_utils/', ignore_errors=True)\n", "shutil.copytree('../../reco_utils/', './movielens-sar/reco_utils/')"]}, {"block": 16, "type": "markdown", "linesLength": 12, "startIndex": 254, "lines": ["# 2 Run training script\n", "### Create an estimator\n", "An estimator object is used to submit the run.  Create your estimator by running the following code to define:\n", "\n", "* The name of the estimator object, `est`\n", "* The directory that contains your scripts. All the files in this directory are uploaded into the cluster nodes for execution. \n", "* The compute target.  In this case you will use the AmlCompute you created\n", "* The training script name, train.py\n", "* Parameters required from the training script \n", "* Python packages needed for training\n", "\n", "In this tutorial, this target is AmlCompute. All files in the script folder are uploaded into the cluster nodes for execution. `ds.as_mount()` mounts a datastore on the remote compute and returns the folder. See documentation [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data#access-datastores-during-training)."]}, {"block": 17, "type": "code", "linesLength": 13, "startIndex": 266, "lines": ["script_params = {\n", "    '--data-folder': ds.as_mount(),\n", "    '--data-file': 'movielens/' + data_file_name,\n", "    '--top-k': TOP_K,\n", "    '--data-size': MOVIELENS_DATA_SIZE\n", "}\n", "\n", "est = Estimator(source_directory=script_folder,\n", "                script_params=script_params,\n", "                compute_target=compute_target,\n", "                entry_script='train.py',\n", "                conda_packages=['pandas'],\n", "                pip_packages=['sklearn'])"]}, {"block": 18, "type": "markdown", "linesLength": 2, "startIndex": 279, "lines": ["### Submit the job to the cluster\n", "Run the experiment by submitting the estimator object. You can check the status of current experiment in Azure Portal by clicking link below."]}, {"block": 19, "type": "code", "linesLength": 6, "startIndex": 281, "lines": ["# create experiment\n", "EXPERIMENT_NAME = 'movielens-sar'\n", "exp = Experiment(workspace=ws, name=EXPERIMENT_NAME)\n", "\n", "run = exp.submit(config=est)\n", "run"]}, {"block": 20, "type": "markdown", "linesLength": 4, "startIndex": 287, "lines": ["You should see simialar output as below\n", "![Experiment submit output](https://docs.microsoft.com/en-us/azure/machine-learning/service/media/quickstart-get-started/view_exp.png)\n", "Azure Portal looks like this\n", "![Azure Portal Experiment](https://docs.microsoft.com/en-us/azure/machine-learning/service/media/quickstart-get-started/web-results.png)"]}, {"block": 21, "type": "markdown", "linesLength": 6, "startIndex": 291, "lines": ["\n", "# 3 Monitor remote run\n", "\n", "### Jupyter widget\n", "\n", "Alternatively, watch the progress of the run with a Jupyter widget.  Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."]}, {"block": 22, "type": "code", "linesLength": 1, "startIndex": 297, "lines": ["RunDetails(run).show()"]}, {"block": 23, "type": "code", "linesLength": 0, "startIndex": 298, "lines": []}]