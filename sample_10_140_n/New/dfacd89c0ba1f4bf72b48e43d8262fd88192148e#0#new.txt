[{"block": 0, "type": "markdown", "linesLength": 10, "startIndex": 0, "lines": ["## Wikidata Knowledge Graph Extraction\n", "Many recommendation algorithms (DKN, RippleNet, KGCN) use Knowledge Graphs as an external source of information. We found that one of the bottlenecks to benchmark current algorithms like DKN, RippleNet or KGCN is that they used Microsoft Satori. As Satori is not open source, it's not possible to replicate the results found in the papers. The solution is using other open source KGs.\n", "\n", "The goal of this notebook is to provide examples of how to interact with Wikipedia queries and Wikidata to extract a Knowledge Graph that can be used with the mentioned algorithms.\n", "\n", "The steps covered are:\n", "- How to find a Wikidata entity (https://www.wikidata.org/wiki/Wikidata:Glossary/en from a text query\n", "- How to find surrounding entities of an entity \n", "- How to find the description of an entity\n", "- Create a KG for Movielens"]}, {"block": 1, "type": "code", "linesLength": 19, "startIndex": 10, "lines": ["# set the environment path to find Recommenders\n", "import sys\n", "sys.path.append(\"../../\")\n", "print(\"System version: {}\".format(sys.version))\n", "\n", "import pandas as pd\n", "from reco_utils.dataset.wikidata import (\n", "    find_wikidataID,\n", "    query_entity_links,\n", "    read_linked_entities,\n", "    query_entity_description\n", ")\n", "\n", "import networkx as nx\n", "import matplotlib.pyplot as plt\n", "from tqdm import tqdm\n", "\n", "from reco_utils.dataset import movielens\n", "from reco_utils.common.notebook_utils import is_jupyter"]}, {"block": 2, "type": "code", "linesLength": 4, "startIndex": 29, "lines": ["# Select MovieLens data size: 100k, 1m, 10m, or 20m\n", "MOVIELENS_DATA_SIZE = '100k'\n", "MOVIELENS_SAMPLE = False\n", "MOVIELENS_SAMPLE_SIZE = 5"]}, {"block": 3, "type": "code", "linesLength": 1, "startIndex": 33, "lines": ["KG_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + '_wikidata.csv'"]}, {"block": 4, "type": "markdown", "linesLength": 1, "startIndex": 34, "lines": ["## 1. Create a KG from linked entities in Wikidata"]}, {"block": 5, "type": "code", "linesLength": 1, "startIndex": 35, "lines": ["names = [\"The Godfather\", \"Al Pacino\", \"Tom Hanks\", \"Forrest Gump\", \"Julia Roberts\", \"\", \"My Best Friend's Wedding\"]"]}, {"block": 6, "type": "code", "linesLength": 14, "startIndex": 36, "lines": ["def wikidata_KG_from_list(names):\n", "    results_list = pd.DataFrame()\n", "    for n in names:\n", "        entity_id = find_wikidataID(n)\n", "        if entity_id != \"entityNotFound\":\n", "            json_links = query_entity_links(entity_id)\n", "            related_entities,related_names = read_linked_entities(json_links)\n", "            d = pd.DataFrame({\n", "                \"name\":n,\n", "                \"original_entity\":[entity_id]* len(related_entities),\n", "                \"linked_entities\":related_entities,\n", "                \"name_linked_entities\":related_names})\n", "            results_list = pd.concat([results_list, d])\n", "    return results_list"]}, {"block": 7, "type": "code", "linesLength": 3, "startIndex": 50, "lines": ["%%time\n", "results_list = wikidata_KG_from_list(names)\n", "results_list.head()"]}, {"block": 8, "type": "markdown", "linesLength": 1, "startIndex": 53, "lines": ["### Visualize KG using networkx"]}, {"block": 9, "type": "code", "linesLength": 1, "startIndex": 54, "lines": ["G = nx.from_pandas_edgelist(results_list, 'original_entity', 'linked_entities')"]}, {"block": 10, "type": "code", "linesLength": 5, "startIndex": 55, "lines": ["target_names = results_list[[\"linked_entities\", \"name_linked_entities\"]].drop_duplicates().rename(columns={\"linked_entities\": \"labels\", \"name_linked_entities\": \"name\"})\n", "source_names = results_list[[\"original_entity\", \"name\"]].drop_duplicates().rename(columns={\"original_entity\": \"labels\"})\n", "names = pd.concat([target_names, source_names])\n", "names = names.set_index(\"labels\")\n", "names = names.to_dict()[\"name\"]"]}, {"block": 11, "type": "code", "linesLength": 5, "startIndex": 60, "lines": ["plt.figure(figsize=(12,12)) \n", "pos = nx.spring_layout(G)\n", "nx.draw(G,pos, node_size=60,font_size=9, width = 0.2)\n", "nx.draw_networkx_labels(G, pos, names, font_size=9)\n", "plt.show()"]}, {"block": 12, "type": "markdown", "linesLength": 1, "startIndex": 65, "lines": ["## 2. Create an item description with short description and linked entitites"]}, {"block": 13, "type": "code", "linesLength": 2, "startIndex": 66, "lines": ["# Create entity description with small description and string of linked entities\n", "names = [\"The Godfather\", \"Al Pacino\"]"]}, {"block": 14, "type": "code", "linesLength": 14, "startIndex": 68, "lines": ["def wikidata_descriptions_from_list(names):\n", "    result_description = pd.DataFrame()\n", "    for n in names:\n", "        entity_id = find_wikidataID(n)\n", "        if entity_id != \"entityNotFound\":\n", "            json_links = query_entity_links(entity_id)\n", "            entity_description = query_entity_description(entity_id)\n", "            related_entities,related_names = read_linked_entities(json_links)\n", "            d = pd.DataFrame({\"name\": n,\n", "                              \"original_entity\": entity_id,\n", "                              \"description\":entity_description,\n", "                              \"related_names\":', '.join(related_names)}, index = [0])\n", "            result_description = pd.concat([result_description, d])\n", "    return result_description"]}, {"block": 15, "type": "code", "linesLength": 3, "startIndex": 82, "lines": ["%%time\n", "result_description = wikidata_descriptions_from_list(names)\n", "result_description.head(10)"]}, {"block": 16, "type": "markdown", "linesLength": 1, "startIndex": 85, "lines": ["## 3. Create a KG from the Movielens Dataset"]}, {"block": 17, "type": "code", "linesLength": 8, "startIndex": 86, "lines": ["# Obtain pairs of Movie Title - IDs from Movielens\n", "df = movielens.load_pandas_df(MOVIELENS_DATA_SIZE,\n", "                              ('UserId', 'ItemId', 'Rating', 'Timestamp'),\n", "                             title_col='Title',\n", "                             genres_col='Genres',\n", "                             year_col='Year'\n", "        )\n", "movies = df[[\"Title\", \"ItemId\"]].drop_duplicates().reset_index()"]}, {"block": 18, "type": "code", "linesLength": 1, "startIndex": 94, "lines": ["movies[\"Title\"][1:5]"]}, {"block": 19, "type": "code", "linesLength": 1, "startIndex": 95, "lines": ["movies.shape"]}, {"block": 20, "type": "code", "linesLength": 14, "startIndex": 96, "lines": ["def wikidata_KG_from_movielens(df):\n", "    result_linked = pd.DataFrame()\n", "    entity_id = find_wikidataID(df[\"Title\"] + \" film\")\n", "    if entity_id != \"entityNotFound\":\n", "        json_links = query_entity_links(entity_id)\n", "        related_entities,related_names = read_linked_entities(json_links)\n", "        d = pd.DataFrame({\"original_entity\":[entity_id]* len(related_entities),\n", "                          \"linked_entities\":related_entities,\n", "                          \"name_linked_entities\":related_names,\n", "                          \"movielens_title\": df[\"Title\"],\n", "                          \"movielens_id\": df[\"ItemId\"],\n", "                         })\n", "        result_linked = pd.concat([result_linked, d])\n", "    return result_linked"]}, {"block": 21, "type": "code", "linesLength": 3, "startIndex": 110, "lines": ["# For notebook testing\n", "if MOVIELENS_SAMPLE == True:\n", "    movies = movies.sample(MOVIELENS_SAMPLE_SIZE, random_state=123)"]}, {"block": 22, "type": "code", "linesLength": 2, "startIndex": 113, "lines": ["tqdm().pandas(desc=\"Number of movies completed\")\n", "result = pd.concat(list(movies.progress_apply(lambda x: wikidata_KG_from_movielens(x), axis=1)))"]}, {"block": 23, "type": "code", "linesLength": 1, "startIndex": 115, "lines": ["result[\"movielens_title\"].value_counts()"]}, {"block": 24, "type": "code", "linesLength": 1, "startIndex": 116, "lines": ["# result.to_csv(KG_FILE_NAME, index = False)"]}, {"block": 25, "type": "code", "linesLength": 5, "startIndex": 117, "lines": ["# Record results with papermill for tests - ignore this cell\n", "if is_jupyter():\n", "    # Record results with papermill for unit-tests\n", "    import papermill as pm\n", "    pm.record(\"lenght_result\", result.shape[0])"]}]