[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["# LightFM -  hybrid matrix factorisation on MovieLens (Python, CPU)"]}, {"block": 1, "type": "markdown", "linesLength": 6, "startIndex": 1, "lines": ["## Notes\n", "Last updated: 27-04-2020\n", "\n", "TO-DO:\n", "- update movielens in reco_utils to retrieve and join user metadata\n", "- extract model outputs to assess performance with ndcg_at_k"]}, {"block": 2, "type": "markdown", "linesLength": 79, "startIndex": 7, "lines": ["This notebook explains the concept of a hybrid matrix factorisation based model for recommendation, it also outlines the steps to construct a pure matrix factorisation and a hybrid models using the [LightFM](https://github.com/lyst/lightfm) package. It also demonstrates how to extract both user and item affinity from a fitted hybrid model.\n", "\n", "## 1. Hybrid matrix factorisation model\n", "\n", "### 1.1 Background\n", "\n", "In general, most recommendation models can be divided into two categories:\n", "- Content based model,\n", "- Collaborative filtering model.\n", "\n", "The content-based model recommends based on similarity of the items and/or users using their description/metadata/profile. On the other hand, collaborative filtering model (discussion is limited to matrix factorisation approach in this notebook) computes the latent factors of the users and items. It works based on the assumption that if a group of people expressed similar opinions on an item, these peole would tend to have similar opinions on other items. For further background and detailed explanation between these two approaches, the reader can refer to machine learning literatures [3, 4].\n", "\n", "The choice between the two models is largely based on the data availability. For example, the collaborative filtering model is usually adopted and effective when sufficient ratings/feedbacks have been recorded for a group of users and items.\n", "\n", "However, if there is a lack of ratings, content based model can be used provided that the metadata of the users and items are available. This is also the common approach to address the cold-start issues, where there are insufficient historical collaborative interactions available to model new users and/or items.\n", "\n", "<!-- In addition, most collaborative filtering models only consume explicit ratings e.g. movie \n", "\n", "**NOTE** add stuff about implicit and explicit ratings -->\n", "\n", "### 1.2 Hybrid matrix factorisation algorithm\n", "\n", "In view of the above problems, there have been a number of proposals to address the cold-start issues by combining both content-based and collaborative filtering approaches. The hybrid matrix factorisation model is among one of the solutions proposed [1].  \n", "\n", "In general, most hybrid approaches proposed different ways of assessing and/or combining the feature data in conjunction with the collaborative information.\n", "\n", "### 1.3 LightFM package \n", "\n", "LightFM is a Python implementation of a hybrid recommendation algorithms for both implicit and explicit feedbacks [1].\n", "\n", "It is a hybrid content-collaborative model which represents users and items as linear combinations of their content features\u2019 latent factors. The model learns **embeddings or latent representations of the users and items in such a way that it encodes user preferences over items**. These representations produce scores for every item for a given user; items scored highly are more likely to be interesting to the user.\n", "\n", "The user and item embeddings are estimated for every feature, and these features are then added together to be the final representations for users and items. \n", "\n", "For example, for user i, the model retrieves the i-th row of the feature matrix to find the features with non-zero weights. The embeddings for these features will then be added together to become the user representation e.g. if user 10 has weight 1 in the 5th column of the user feature matrix, and weight 3 in the 20th column, the user 10\u2019s representation is the sum of embedding for the 5th and the 20th features multiplying their corresponding weights. The representation for each items is computed in the same approach. \n", "\n", "#### 1.3.1 Modelling approach\n", "\n", "Let $U$ be the set of users and $I$ be the set of items, and each user can be described by a set of user features $f_{u} \\subset F^{U}$ whilst each items can be described by item features $f_{i} \\subset F^{I}$. Both $F^{U}$ and $F^{I}$ are all the features which fully describe all users and items. \n", "\n", "The LightFM model operates based binary feedbacks, the ratings will be normalised into two groups. The user-item interaction pairs $(u,i) \\in U\\times I$ are the union of positive (favourable reviews) $S^+$ and negative interactions (negative reviews) $S^-$ for explicit ratings. For implicit feedbacks, these can be the observed and not observed interactions respectively.\n", "\n", "For each user and item feature, their embeddings are $e_{f}^{U}$ and $e_{f}^{I}$ respectively. Furthermore, each feature is also has a scalar bias term ($b_U^f$ for user and $b_I^f$ for item features). The embedding (latent representation) of user $u$ and item $i$ are the sum of its respective features\u2019 latent vectors:\n", "\n", "$$ \n", "q_{u} = \\sum_{j \\in f_{u}} e_{j}^{U}\n", "$$\n", "\n", "$$\n", "p_{i} = \\sum_{j \\in f_{i}} e_{j}^{I}\n", "$$\n", "\n", "Similarly the biases for user $u$ and item $i$ are the sum of its respective bias vectors. These variables capture the variation in behaviour across users and items:\n", "\n", "$$\n", "b_{u} = \\sum_{j \\in f_{u}} b_{j}^{U}\n", "$$\n", "\n", "$$\n", "b_{i} = \\sum_{j \\in f_{i}} b_{j}^{I}\n", "$$\n", "\n", "In LightFM, the representation for each user/item is a linear weighted sum of its feature vectors.\n", "\n", "The prediction for user $u$ and item $i$ can be modelled as sigmoid of the dot product of user and item vectors, adjusted by its feature biases as follows:\n", "\n", "$$\n", "\\hat{r}_{ui} = \\sigma (q_{u} \\cdot p_{i} + b_{u} + b_{i})\n", "$$\n", "\n", "As the LightFM is constructed to predict binary outcomes e.g. $S^+$ and $S^-$, the function $\\sigma()$ is based on the [sigmoid function](https://mathworld.wolfram.com/SigmoidFunction.html). \n", "\n", "The LightFM algorithm estimates interaction latent vectors and bias for features. For model fitting, the cost function of the model consists of maximising the likelihood of data conditional on the parameters described above using stochastic gradient descent. The likelihood can be expressed as follows:\n", "\n", "$$\n", "L = \\prod_{(u,i) \\in S+}\\hat{r}_{ui} \\times \\prod_{(u,i) \\in S-}1 - \\hat{r}_{ui}\n", "$$\n", "\n", "Note that if the feature latent vectors are not available, the algorithm will be behaves like a [logistic matrix factorisation model](http://stanford.edu/~rezab/nips2014workshop/submits/logmat.pdf)."]}, {"block": 3, "type": "markdown", "linesLength": 1, "startIndex": 86, "lines": ["## 2. Movie recommender with LightFM using only explicit feedbacks"]}, {"block": 4, "type": "markdown", "linesLength": 1, "startIndex": 87, "lines": ["### 2.1 Import libraries"]}, {"block": 5, "type": "code", "linesLength": 20, "startIndex": 88, "lines": ["import sys\n", "sys.path.append(\"../../\")\n", "import os\n", "\n", "import itertools\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "import lightfm\n", "from lightfm import LightFM\n", "from lightfm.data import Dataset\n", "from lightfm import cross_validation\n", "from lightfm.evaluation import precision_at_k, recall_at_k, auc_score\n", "\n", "from reco_utils.dataset import movielens\n", "from reco_utils.recommender.lightfm.lightfm_utils import (\n", "    model_perf_plots, compare_metric, track_model_metrics, \n", "    similar_users, similar_items)"]}, {"block": 6, "type": "code", "linesLength": 21, "startIndex": 108, "lines": ["# Select MovieLens data size\n", "MOVIELENS_DATA_SIZE = '100k'\n", "\n", "# default number of recommendations\n", "k = 10\n", "# percentage of data used for testing\n", "test_percentage = 0.25\n", "# model learning rate\n", "learning_rate = 0.25\n", "# no of latent factors\n", "no_components = 50\n", "# no of epochs to fit model\n", "no_epochs = 250\n", "# no of threads to fit model\n", "no_threads = 8\n", "# regularisation for both user and item features\n", "item_alpha=1e-6\n", "user_alpha=1e-6\n", "\n", "# seed for pseudonumber generations\n", "seedno = 42"]}, {"block": 7, "type": "code", "linesLength": 2, "startIndex": 129, "lines": ["print(\"System version: {}\".format(sys.version))\n", "print(\"LightFM version: {}\".format(lightfm.__version__))"]}, {"block": 8, "type": "markdown", "linesLength": 1, "startIndex": 131, "lines": ["### 2.2 Retrieve data"]}, {"block": 9, "type": "code", "linesLength": 7, "startIndex": 132, "lines": ["data = movielens.load_pandas_df(\n", "    size=MOVIELENS_DATA_SIZE,\n", "    genres_col='genre',\n", "    header=[\"userID\", \"itemID\", \"rating\"]\n", ")\n", "# quick look at the data\n", "data.sample(5)"]}, {"block": 10, "type": "markdown", "linesLength": 1, "startIndex": 139, "lines": ["### 2.3 Prepare data"]}, {"block": 11, "type": "markdown", "linesLength": 1, "startIndex": 140, "lines": ["Before fitting the LightFM model, we need to create an instance of `Dataset` which holds the interaction matrix."]}, {"block": 12, "type": "code", "linesLength": 1, "startIndex": 141, "lines": ["dataset = Dataset()"]}, {"block": 13, "type": "markdown", "linesLength": 1, "startIndex": 142, "lines": ["The `fit` method creates the user/item id mappings."]}, {"block": 14, "type": "code", "linesLength": 6, "startIndex": 143, "lines": ["dataset.fit(users=data['userID'], \n", "            items=data['itemID'])\n", "\n", "# quick check to determine the number of unique users and items in the data\n", "num_users, num_topics = dataset.interactions_shape()\n", "print(f'Num users: {num_users}, num_topics: {num_topics}.')"]}, {"block": 15, "type": "markdown", "linesLength": 1, "startIndex": 149, "lines": ["Next is to build the interaction matrix. The `build_interactions` method returns 2 COO sparse matrices, namely the `interactions` and `weights` matrices."]}, {"block": 16, "type": "code", "linesLength": 1, "startIndex": 150, "lines": ["(interactions, weights) = dataset.build_interactions(data.iloc[:, 0:3].values)"]}, {"block": 17, "type": "markdown", "linesLength": 5, "startIndex": 151, "lines": ["LightLM works slightly differently compared to other packages as it expects the train and test sets to have same dimension. Therefore the conventional train test split will not work.\n", "\n", "The package has included the `cross_validation.random_train_test_split` method to split the interaction data and splits it into two disjoint training and test sets. \n", "\n", "However, note that **it does not validate the interactions in the test set to guarantee all items and users have historical interactions in the training set**. Therefore this may result into a partial cold-start problem in the test set."]}, {"block": 18, "type": "code", "linesLength": 3, "startIndex": 156, "lines": ["train_interactions, test_interactions = cross_validation.random_train_test_split(\n", "    interactions, test_percentage=test_percentage,\n", "    random_state=np.random.RandomState(seedno))"]}, {"block": 19, "type": "markdown", "linesLength": 1, "startIndex": 159, "lines": ["Double check the size of both the train and test sets."]}, {"block": 20, "type": "code", "linesLength": 2, "startIndex": 160, "lines": ["print(f\"Shape of train interactions: {train_interactions.shape}\")\n", "print(f\"Shape of test interactions: {test_interactions.shape}\")"]}, {"block": 21, "type": "markdown", "linesLength": 1, "startIndex": 162, "lines": ["### 2.4 Fit & evaluate the LightFM model"]}, {"block": 22, "type": "markdown", "linesLength": 4, "startIndex": 163, "lines": ["In this notebook, the LightFM model will be using the weighted Approximate-Rank Pairwise (WARP) as the loss. Further explanation on the topic can be found [here](https://making.lyst.com/lightfm/docs/examples/warp_loss.html#learning-to-rank-using-the-warp-loss).\n", "\n", "\n", "In general, it maximises the rank of positive examples by repeatedly sampling negative examples until a rank violation has been located. This approach is recommended when only positive interactions are present."]}, {"block": 23, "type": "code", "linesLength": 3, "startIndex": 167, "lines": ["model1 = LightFM(loss='warp', no_components=no_components, \n", "                 learning_rate=learning_rate,                 \n", "                 random_state=np.random.RandomState(seedno))"]}, {"block": 24, "type": "markdown", "linesLength": 1, "startIndex": 170, "lines": ["The performance of the model will be tracked using 3 metrics, namely: AUC, Precision and Recall. The progression of these metrics during the fitting process is shown as follows:"]}, {"block": 25, "type": "code", "linesLength": 4, "startIndex": 171, "lines": ["output1, _ = track_model_metrics(model=model1, \n", "                              train_interactions=train_interactions, \n", "                              test_interactions=test_interactions, k=k,\n", "                              no_epochs=no_epochs, no_threads=no_threads)"]}, {"block": 26, "type": "markdown", "linesLength": 7, "startIndex": 175, "lines": ["Based on the model's AUC score, it seems the model is levelling off its performance quite early on ~50 epochs. The model then slowly improves as training progresses. As there is a gap between training and testing AUC, \n", "\n", "However, referring to both the Precision and Recall metric, the test performance in both cases plateaued out fairly early on, suggesting the model might be overfitting.\n", "\n", "There are a few approaches to fix this (not part of objectives in this notebook) such as:\n", "- changing model parameters,\n", "- include more data."]}, {"block": 27, "type": "markdown", "linesLength": 1, "startIndex": 182, "lines": ["### 2.5 Direct model fitting"]}, {"block": 28, "type": "markdown", "linesLength": 3, "startIndex": 183, "lines": ["If the reader refer to the `track_model_metrics` function within the `reco_utils` folder, he/she will it was constructed using the `fit_partial` method. This is because this method facilitates the performance extraction at each epoch.\n", "\n", "If the reader is confident with the hyperparameters, the model can be fitted directly using the `fit` method as follows:"]}, {"block": 29, "type": "code", "linesLength": 6, "startIndex": 186, "lines": ["model1_direct = LightFM(loss='warp', no_components=no_components, \n", "                 learning_rate=learning_rate,                 \n", "                 random_state=np.random.RandomState(seedno))\n", "\n", "model1_direct.fit(interactions=train_interactions, epochs=no_epochs,\n", "                  num_threads=no_threads)"]}, {"block": 30, "type": "markdown", "linesLength": 3, "startIndex": 192, "lines": ["Once the model has been fitted, we can double check both `partial_fit` and `fit` methods return models with similar performance. \n", "\n", "Firstly the recorded metrics from the `partial_fit` method can be extracted and stored under `metrics_partial_fit` variable as follows:"]}, {"block": 31, "type": "code", "linesLength": 2, "startIndex": 195, "lines": ["metrics_fit_partial = output1[\n", "    (output1['epoch'] == (no_epochs-1)) & (output1['stage'] == 'test') ]['value'].reset_index(drop=True)"]}, {"block": 32, "type": "markdown", "linesLength": 1, "startIndex": 197, "lines": ["Then the performance metrics of these two models can be retrieved and compared as follows:"]}, {"block": 33, "type": "code", "linesLength": 7, "startIndex": 198, "lines": ["pd.DataFrame([x for x in zip(metrics_fit_partial.values, \n", "                             [auc_score(model1_direct, test_interactions).mean(), \n", "                              precision_at_k(model1_direct, test_interactions, k=k).mean(), \n", "                              recall_at_k(model1_direct, test_interactions, k=k).mean()])],\n", "             columns=['fit_partial','fit'], \n", "             index=['AUC', 'Precision', 'Recall']\n", ")"]}, {"block": 34, "type": "markdown", "linesLength": 5, "startIndex": 205, "lines": ["There are some minor differences between these two models which could be due to implementation of the parallel computation, specifically in the `fit_partial` method, where at each epoch there was essentially a single threaded computation with `epoch=1`. \n", "\n", "As a result, the fitted models might have slight differences in performance due to the different gradient updating with `adagrad` compared with `fit` where parallel computation was possible.\n", "\n", "This section is highlight the reason why `fit_partial` was used and how the `fit` can be used if necessary. For the rest of this notebook the `track_model_metrics` function will be used in order to examining the fitting process."]}, {"block": 35, "type": "markdown", "linesLength": 1, "startIndex": 210, "lines": ["## 3. Movie recommender with LightFM using explicit feedbacks and additional item and user features"]}, {"block": 36, "type": "markdown", "linesLength": 1, "startIndex": 211, "lines": ["As the LightFM was designed to incorporates both user and item metadata, the model can be extended to include additional features such as movie genres and user occupations."]}, {"block": 37, "type": "markdown", "linesLength": 1, "startIndex": 212, "lines": ["### 3.1 Extract and prepare movie genres"]}, {"block": 38, "type": "markdown", "linesLength": 1, "startIndex": 213, "lines": ["In this notebook, the movie's genres will be used as the item metadata. As the genres have already been loaded during the initial data import, it can be processed directly as follows:"]}, {"block": 39, "type": "code", "linesLength": 2, "startIndex": 214, "lines": ["# split the genre based on the separator\n", "movie_genre = [x.split('|') for x in data['genre']]"]}, {"block": 40, "type": "code", "linesLength": 4, "startIndex": 216, "lines": ["# retrieve the all the unique genres in the data\n", "all_movie_genre = sorted(list(set(itertools.chain.from_iterable(movie_genre))))\n", "# quick look at the all the genres within the data\n", "all_movie_genre"]}, {"block": 41, "type": "markdown", "linesLength": 1, "startIndex": 220, "lines": ["### 3.2 Retrieve and prepare movie genres"]}, {"block": 42, "type": "markdown", "linesLength": 1, "startIndex": 221, "lines": ["Further user features can be included as part of the model fitting process. In this notebook, **only the occupation of each user will be included** but the feature list can be extended easily.\n"]}, {"block": 43, "type": "markdown", "linesLength": 1, "startIndex": 222, "lines": ["#### 3.2.1 Retrieve and merge data"]}, {"block": 44, "type": "markdown", "linesLength": 1, "startIndex": 223, "lines": ["The user features can be retrieved directly from the grouplens website and merged with the existing data as follows:"]}, {"block": 45, "type": "code", "linesLength": 9, "startIndex": 224, "lines": ["user_feature_URL = 'http://files.grouplens.org/datasets/movielens/ml-100k/u.user'\n", "user_data = pd.read_table(user_feature_URL, \n", "              sep='|', header=None)\n", "user_data.columns = ['userID','age','gender','occupation','zipcode']\n", "\n", "# merging user feature with existing data\n", "new_data = data.merge(user_data[['userID','occupation']], left_on='userID', right_on='userID')\n", "# quick look at the merged data\n", "new_data.sample(5)"]}, {"block": 46, "type": "markdown", "linesLength": 1, "startIndex": 233, "lines": ["#### 3.2.2 Extract and prepare user occupations"]}, {"block": 47, "type": "code", "linesLength": 2, "startIndex": 234, "lines": ["# retrieve all the unique occupations in the data\n", "all_occupations = sorted(list(set(new_data['occupation'])))"]}, {"block": 48, "type": "markdown", "linesLength": 1, "startIndex": 236, "lines": ["### 3.3 Prepare data and features"]}, {"block": 49, "type": "markdown", "linesLength": 1, "startIndex": 237, "lines": ["Similar to the previous model, the data is required to be converted into a `Dataset` instance and then create a user/item id mapping with the `fit` method."]}, {"block": 50, "type": "code", "linesLength": 5, "startIndex": 238, "lines": ["dataset2 = Dataset()\n", "dataset2.fit(data['userID'], \n", "            data['itemID'], \n", "            item_features=all_movie_genre,\n", "            user_features=all_occupations)"]}, {"block": 51, "type": "markdown", "linesLength": 1, "startIndex": 243, "lines": ["The movie genres are then converted into a item feature matrix using the `build_item_features` method as follows:"]}, {"block": 52, "type": "code", "linesLength": 2, "startIndex": 244, "lines": ["item_features = dataset2.build_item_features(\n", "    (x, y) for x,y in zip(data.itemID, movie_genre))"]}, {"block": 53, "type": "markdown", "linesLength": 1, "startIndex": 246, "lines": ["The user occupations are then converted into an user feature matrix using the `build_user_features` method as follows:"]}, {"block": 54, "type": "code", "linesLength": 2, "startIndex": 247, "lines": ["user_features = dataset2.build_user_features(\n", "    (x, [y]) for x,y in zip(new_data.userID, new_data['occupation']))"]}, {"block": 55, "type": "markdown", "linesLength": 1, "startIndex": 249, "lines": ["Once the item and user features matrices have been completed, the next steps are similar as before, which is to build the interaction matrix and split the interactions into train and test sets as follows:"]}, {"block": 56, "type": "code", "linesLength": 5, "startIndex": 250, "lines": ["(interactions2, weights) = dataset2.build_interactions(data.iloc[:, 0:3].values)\n", "\n", "train_interactions2, test_interactions2 = cross_validation.random_train_test_split(\n", "    interactions2, test_percentage=test_percentage,\n", "    random_state=np.random.RandomState(seedno))"]}, {"block": 57, "type": "markdown", "linesLength": 1, "startIndex": 255, "lines": ["### 3.3 Fit & evaluate the LightFM model with additional user and item features"]}, {"block": 58, "type": "markdown", "linesLength": 3, "startIndex": 256, "lines": ["The parameters of the second model will be similar to the first model to facilitates comparison.\n", "\n", "The model performance at each epoch is also tracked by the same metrics as before."]}, {"block": 59, "type": "code", "linesLength": 11, "startIndex": 259, "lines": ["model2 = LightFM(loss='warp', no_components=no_components, \n", "                 learning_rate=learning_rate, \n", "                 item_alpha=item_alpha,\n", "                 user_alpha=user_alpha,\n", "                 random_state=np.random.RandomState(seedno))\n", "\n", "output2, _ = track_model_metrics(model=model2, train_interactions=train_interactions2, \n", "                              test_interactions=test_interactions2, k=k,\n", "                              no_epochs=no_epochs, no_threads=no_threads, \n", "                              item_features=item_features,\n", "                              user_features=user_features)"]}, {"block": 60, "type": "markdown", "linesLength": 3, "startIndex": 270, "lines": ["Referring to the AUC figure above, it is rather interesting to see that with the inclusive of both user and item features, the model seems to have overfitted.\n", "\n", "On the other hand, the Precision and Recall traces have plateued fairly early on, in particular for Precision@K there seems to be a large difference between train and test results. This could be due to the different number of user-item interactions between these data sets, e.g. the number of user-item interactions in the test set is less than the train set such that if the available interactions were less than K (in this case K=10), the best Precision@K will be bounded by the maximum number of interactions over K."]}, {"block": 61, "type": "markdown", "linesLength": 1, "startIndex": 273, "lines": ["## 4. Performance comparison"]}, {"block": 62, "type": "markdown", "linesLength": 1, "startIndex": 274, "lines": ["As more features were included in this model, we can check whether movie genres actually helped in lifting the model's performance. The following plots look at the performance between the three models based on the test data set."]}, {"block": 63, "type": "code", "linesLength": 6, "startIndex": 275, "lines": ["for i in ['AUC', 'Precision', 'Recall']:\n", "    sns.set_palette(\"Set2\")\n", "    plt.figure()\n", "    sns.scatterplot(x=\"epoch\", y=\"value\", hue='data',\n", "                data=compare_metric(df_list = [output1, output2], metric=i)\n", "               ).set_title(f'{i} comparison using test set');"]}, {"block": 64, "type": "markdown", "linesLength": 5, "startIndex": 281, "lines": ["From the AUC metric plots above, the model2 (with additional user and movie metadata) started with better AUC but regresses as model fit progresses perhaps due to the fact WARP loss function was used. It prioritises the top items on the list of recommendations, hence sacrificing global metrics such as AUC which measures overall ranking.\n", "\n", "These can be evidenced by the Precision and Recall traces of model2, which are consistently better than model1. Note that model1 was essentially operating as a pure logistic matrix factorisation based model and interestingly from both Precision and Recall perspectives, the model1 seems to be experiencing variance issues as model fitting progresses.\n", "\n", "Readers who are interested could further investigate parameters such as learning rate and/or regularisation penalty as these preliminary models would certainly benefit from further hyperparameter studies."]}, {"block": 65, "type": "markdown", "linesLength": 1, "startIndex": 286, "lines": ["## 5. Similar users and items"]}, {"block": 66, "type": "markdown", "linesLength": 1, "startIndex": 287, "lines": ["As the LightFM package operates based on latent embeddings, these can be retrieved once the model has been fitted to assess user-user and/or item-item affinity."]}, {"block": 67, "type": "markdown", "linesLength": 1, "startIndex": 288, "lines": ["### 5.1 User affinity"]}, {"block": 68, "type": "markdown", "linesLength": 1, "startIndex": 289, "lines": ["The user-user affinity can be retrieved with the `get_user_representations` method from the fitted model as follows:"]}, {"block": 69, "type": "code", "linesLength": 2, "startIndex": 290, "lines": ["_, user_embeddings = model2.get_user_representations(features=user_features)\n", "user_embeddings"]}, {"block": 70, "type": "markdown", "linesLength": 1, "startIndex": 292, "lines": ["In order to retrieve the top N similar users, we can use the `similar_users` from `reco_utils`. For example, if we want to choose top 10 users most similar to the user 1:"]}, {"block": 71, "type": "code", "linesLength": 2, "startIndex": 293, "lines": ["similar_users(user_id=1, user_features=user_features, \n", "            model=model2)"]}, {"block": 72, "type": "markdown", "linesLength": 1, "startIndex": 295, "lines": ["### 5.2 Item affinity"]}, {"block": 73, "type": "markdown", "linesLength": 1, "startIndex": 296, "lines": ["Similar to the user affinity, the item-item affinity can be retrieved with the `get_item_representations` method using the fitted model."]}, {"block": 74, "type": "code", "linesLength": 2, "startIndex": 297, "lines": ["_, item_embeddings = model2.get_item_representations(features=item_features)\n", "item_embeddings"]}, {"block": 75, "type": "markdown", "linesLength": 1, "startIndex": 299, "lines": ["The function to retrieve the top N similar items is similar to similar_users() above. For example, if we want to choose top 10 items most similar to the item 10:"]}, {"block": 76, "type": "code", "linesLength": 2, "startIndex": 300, "lines": ["similar_items(item_id=10, item_features=item_features, \n", "            model=model2)"]}, {"block": 77, "type": "markdown", "linesLength": 1, "startIndex": 302, "lines": ["## 6. Conclusion"]}, {"block": 78, "type": "markdown", "linesLength": 5, "startIndex": 303, "lines": ["In this notebook, the background of hybrid matrix factorisation model has been explained together with a detailed example of LightFM's implementation. \n", "\n", "The process of incorporating additional user and item metadata has also been demonstrated with performance comparison. Furthermore, the calculation of both user and item affinity scores have also been demonstrated and extracted from the fitted model.\n", "\n", "This notebook remains a fairly simple treatment on the subject and hopefully could serve as a good foundation for the reader."]}, {"block": 79, "type": "markdown", "linesLength": 1, "startIndex": 308, "lines": ["## References"]}, {"block": 80, "type": "markdown", "linesLength": 4, "startIndex": 309, "lines": ["- [[1](https://arxiv.org/abs/1507.08439)]. Maciej Kula - Metadata Embeddings for User and Item Cold-start Recommendations, 2015. arXiv:1507.08439\n", "- [[2](https://making.lyst.com/lightfm/docs/home.html)]. LightFM documentation,\n", "- [3]. Charu C. Aggarwal - Recommender Systems: The Textbook, Springer, April 2016. ISBN 978-3-319-29659-3\n", "- [4]. Deepak K. Agarwal, Bee-Chung Chen - Statistical Methods for Recommender Systems, 2016. ISBN: 9781107036079 \n"]}]