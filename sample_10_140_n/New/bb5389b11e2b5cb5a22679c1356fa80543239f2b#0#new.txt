[{"block": 0, "type": "markdown", "linesLength": 3, "startIndex": 0, "lines": ["<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n", "\n", "<i>Licensed under the MIT License.</i>"]}, {"block": 1, "type": "markdown", "linesLength": 4, "startIndex": 3, "lines": ["# TF-IDF Content-Based Recommendation on the COVID-19 Open Research Dataset\n", "This demonstrates a simple implementation of Term Frequency Inverse Document Frequency (TF-IDF) content-based recommendation on the [COVID-19 Open Research Dataset](https://azure.microsoft.com/en-us/services/open-datasets/catalog/covid-19-open-research/), hosted through Azure Open Datasets.\n", "\n", "In this notebook, we will create a recommender which will return the top k recommended articles similar to any article of interest (query item) in the COVID-19 Open Reserach Dataset."]}, {"block": 2, "type": "code", "linesLength": 10, "startIndex": 7, "lines": ["# Set the environment path to find Recommenders\n", "import sys\n", "sys.path.append(\"../../\")\n", "\n", "# Import functions\n", "from reco_utils.dataset.covid_utils import get_blob_service,load_csv_from_blob,extract_public_domain,clean_dataframe,get_public_domain_text\n", "from reco_utils.recommender.tfidf.tfidf_utils import TfidfRecommender\n", "\n", "# Print version\n", "print(\"System version: {}\".format(sys.version))"]}, {"block": 3, "type": "markdown", "linesLength": 1, "startIndex": 17, "lines": ["Set the default parameters for accessing the dataset."]}, {"block": 4, "type": "code", "linesLength": 5, "startIndex": 18, "lines": ["# Parameters for the COVID-19 dataset in Azure Open Datasets\n", "azure_storage_account_name='azureopendatastorage'\n", "azure_storage_sas_token='sv=2019-02-02&ss=bfqt&srt=sco&sp=rlcup&se=2025-04-14T00:21:16Z&st=2020-04-13T16:21:16Z&spr=https&sig=JgwLYbdGruHxRYTpr5dxfJqobKbhGap8WUtKFadcivQ%3D'\n", "container_name='covid19temp'\n", "metadata_filename='metadata.csv'"]}, {"block": 5, "type": "markdown", "linesLength": 2, "startIndex": 23, "lines": ["### 1. Load the dataset into a dataframe\n", "Let's begin by loading the metadata file for the dataset into a Pandas dataframe. This file contains metadata about each of the scientific articles included in the full dataset."]}, {"block": 6, "type": "code", "linesLength": 3, "startIndex": 25, "lines": ["# Get metadata (may take around 1-2 min)\n", "blob_service = get_blob_service(azure_storage_account_name, azure_storage_sas_token, container_name)\n", "metadata = load_csv_from_blob(blob_service, container_name, metadata_filename)"]}, {"block": 7, "type": "markdown", "linesLength": 2, "startIndex": 28, "lines": ["### 2. Extract articles in the public domain\n", "The dataset contains articles using a variety of licenses. We will only be using articles that fall under the public domain ([cc0](https://creativecommons.org/publicdomain/zero/1.0/))."]}, {"block": 8, "type": "code", "linesLength": 2, "startIndex": 30, "lines": ["# View distribution of license types in the dataset\n", "metadata['license'].value_counts().plot(kind='bar', title='License')"]}, {"block": 9, "type": "code", "linesLength": 5, "startIndex": 32, "lines": ["# Extract metadata on public domain articles only\n", "metadata_public = extract_public_domain(metadata)\n", "\n", "# Clean dataframe\n", "metadata_public = clean_dataframe(metadata_public)"]}, {"block": 10, "type": "markdown", "linesLength": 1, "startIndex": 37, "lines": ["Let's look at the top few rows of this dataframe which contains metadata on public domain articles."]}, {"block": 11, "type": "code", "linesLength": 4, "startIndex": 38, "lines": ["# Preview metadata for public domain articles\n", "print('Number of articles in dataset: ' + str(len(metadata)))\n", "print('Number of articles in dataset that fall under the public domain (cc0): ' + str(len(metadata_public)))\n", "metadata_public.head()"]}, {"block": 12, "type": "markdown", "linesLength": 2, "startIndex": 42, "lines": ["### 3. Retrieve full article text\n", "Now that we have the metadata for the public domain articles as its own dataframe, let's retrieve the full text for each public domain scientific article."]}, {"block": 13, "type": "code", "linesLength": 2, "startIndex": 44, "lines": ["# Extract text from all public domain articles (may take 2-3 min)\n", "all_text = get_public_domain_text(metadata_public, blob_service, container_name)"]}, {"block": 14, "type": "markdown", "linesLength": 1, "startIndex": 46, "lines": ["Notice that **all_text** is the same as **metadata_public** but now has an additional column called **full_text** which contains the full text for each respective article."]}, {"block": 15, "type": "code", "linesLength": 2, "startIndex": 47, "lines": ["# Preview\n", "all_text.head()"]}, {"block": 16, "type": "markdown", "linesLength": 11, "startIndex": 49, "lines": ["### 4. Instantiate the recommender\n", "All functions for data preparation and recommendation are contained within the **TfidfRecommender** class we have imported. Prior to running these functions, we must create an object of this class.\n", "\n", "Select one of the following tokenization methods to use in the model:\n", "\n", "| tokenization_method | Description                                                                                                                      |\n", "|:--------------------|:---------------------------------------------------------------------------------------------------------------------------------|\n", "| 'none'              | No tokenization is applied. Each word is considered a token.                                                                     |\n", "| 'nltk'              | Simple stemming is applied using NLTK.                                                                                           |\n", "| 'bert'              | HuggingFace BERT word tokenization ('bert-base-cased') is applied.                                                               |\n", "| 'scibert'           | SciBERT word tokenization ('allenai/scibert_scivocab_cased') is applied.<br>This is recommended for scientific journal articles. |"]}, {"block": 17, "type": "code", "linesLength": 4, "startIndex": 60, "lines": ["# Create the recommender object\n", "recommender = TfidfRecommender(tokenization_method='scibert',\n", "                               id_col='cord_uid',\n", "                               k=5)"]}, {"block": 18, "type": "markdown", "linesLength": 4, "startIndex": 64, "lines": ["### 5. Prepare text for use in the TF-IDF recommender\n", "The raw text retrieved for each article requires basic cleaning prior to being used in the TF-IDF model.\n", "\n", "Let's look at the full_text from the first article in our dataframe as an example."]}, {"block": 19, "type": "code", "linesLength": 2, "startIndex": 68, "lines": ["# Preview the full scientific text from one example\n", "print(all_text['full_text'][0])"]}, {"block": 20, "type": "markdown", "linesLength": 3, "startIndex": 70, "lines": ["As seen above, there are some special characters (such as \u2022 \u25b2 \u25a0 \u2265 \u00b0) and punctuation which should be removed prior to using the text as input. Casing (capitalization) is preserved for [BERT-based tokenization methods](https://huggingface.co/transformers/model_doc/bert.html), but is removed for simple or no tokenization.\n", "\n", "Let's join together the **abstract** and **full_text** columns and clean them for future use in the TF-IDF model."]}, {"block": 21, "type": "code", "linesLength": 4, "startIndex": 73, "lines": ["# Assign columns to clean and combine\n", "cols_to_clean = ['abstract','full_text']\n", "clean_col = 'cleaned_text'\n", "df_clean = recommender.clean_dataframe(all_text, cols_to_clean, clean_col)"]}, {"block": 22, "type": "code", "linesLength": 2, "startIndex": 77, "lines": ["# Preview the dataframe with the cleaned text\n", "df_clean.head()"]}, {"block": 23, "type": "code", "linesLength": 2, "startIndex": 79, "lines": ["# Preview the cleaned version of the previous example\n", "df_clean[clean_col][0]"]}, {"block": 24, "type": "markdown", "linesLength": 1, "startIndex": 81, "lines": ["Let's also tokenize the cleaned text for use in the TF-IDF model. The tokens are stored within our TfidfRecommender object."]}, {"block": 25, "type": "code", "linesLength": 2, "startIndex": 82, "lines": ["# Tokenize text with tokenization_method specified in class instantiation\n", "tf, vectors_tokenized = recommender.tokenize_text(df_clean, text_col=clean_col)"]}, {"block": 26, "type": "markdown", "linesLength": 4, "startIndex": 84, "lines": ["### 6. Recommend articles using TF-IDF\n", "Let's now fit the recommender model to the processed data (tokens) and retrieve the top k recommended articles.\n", "\n", "When creating our object, we specified k=5 so the `recommend_top_k_items` function will return the top 5 recommendations for each public domain article."]}, {"block": 27, "type": "code", "linesLength": 5, "startIndex": 88, "lines": ["# Fit the TF-IDF vectorizer\n", "_ = recommender.fit_tfidf(tf, vectors_tokenized)\n", "\n", "# Get recommendations\n", "top_k_recommendations = recommender.recommend_top_k_items(df_clean)"]}, {"block": 28, "type": "markdown", "linesLength": 3, "startIndex": 93, "lines": ["In our recommendation table, each row representats a single recommendation.\n", "\n", "**cord_uid** and **title** correspond to the article that is being used to make recommendations from. **rec_rank** contains the recommdation's rank (e.g., rank of 1 means top recommendation). **rec_score** is the cosine similarity score between the query article and the recommended article. **rec_cord_uid** and **rec_title** correspond to the recommended article."]}, {"block": 29, "type": "code", "linesLength": 2, "startIndex": 96, "lines": ["# Preview the recommendations\n", "top_k_recommendations"]}, {"block": 30, "type": "markdown", "linesLength": 1, "startIndex": 98, "lines": ["Optionally, we can access the full recommendation dictionary, which contains full ranked lists for each public domain article."]}, {"block": 31, "type": "code", "linesLength": 7, "startIndex": 99, "lines": ["# Optionally view full recommendation list\n", "full_rec_list = recommender.recommendations\n", "\n", "article_of_interest = 'ej795nks'\n", "print('Number of recommended articles for ' +\n", "      article_of_interest + ': ' +\n", "      str(len(full_rec_list[article_of_interest])))"]}, {"block": 32, "type": "markdown", "linesLength": 1, "startIndex": 106, "lines": ["Optionally, we can also view the tokens and stop words which were used in the recommender."]}, {"block": 33, "type": "code", "linesLength": 5, "startIndex": 107, "lines": ["# Optionally view tokens\n", "tokens = recommender.get_tokens()\n", "\n", "# Preview 10 tokens\n", "print(list(tokens.keys())[:10])"]}, {"block": 34, "type": "code", "linesLength": 4, "startIndex": 112, "lines": ["# Preview just the first 10 stop words sorted alphabetically\n", "stop_words = list(recommender.get_stop_words())\n", "stop_words.sort()\n", "print(stop_words[:10])"]}, {"block": 35, "type": "markdown", "linesLength": 2, "startIndex": 116, "lines": ["### 7. Display top recommendations for article of interest\n", "Now that we have the recommendation table containing IDs for both query and recommended articles, we can easily return the full metadata for the top k recommendations for any given article."]}, {"block": 36, "type": "code", "linesLength": 1, "startIndex": 118, "lines": ["recommender.get_top_k_recommendations(metadata_public,'ej795nks')"]}, {"block": 37, "type": "markdown", "linesLength": 2, "startIndex": 119, "lines": ["### Conclusion\n", "In this notebook, we have demonstrated how to create a TF-IDF recommender to recommend the top k (in this case 5) articles similar in content to an article of interest (in this example, article with `cord_uid='ej795nks'`)."]}]