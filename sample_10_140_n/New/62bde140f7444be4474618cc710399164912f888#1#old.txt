[{"block": 0, "type": "markdown", "linesLength": 4, "startIndex": 0, "lines": ["<i>Copyright (c) Microsoft Corporation. All rights reserved.<br>\n", "Licensed under the MIT License.</i>\n", "<br><br>\n", "# Recommender Hyperparameter Tuning w/ AzureML"]}, {"block": 1, "type": "markdown", "linesLength": 19, "startIndex": 4, "lines": ["In this notebook, we show how to hyperparameter tune a recommender model by utilizing **Azure Machine Learning service*** ([AML or AzureML](https://azure.microsoft.com/en-us/services/machine-learning-service/)) in the context of movie recommendation. Note, to use AML, you will need Azure subscription.\n", "\n", "Here, we use [**wide-and-deep model**](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html) from TensorFlow high-level Estimator API.\n", "\n", "We present an overall process of utilizing AML by demonstrating some key steps while avoiding showing too much details. This notebook includes many useful links for those details instead.\n", "  \n", "<br>  \n", "\n", "For more details about the **wide-and-deep** model:\n", "* [Wide-Deep Quickstart notebook](../00_quick_start/wide_deep_model_movielens.ipynb)\n", "* [Original paper](https://arxiv.org/abs/1606.07792)\n", "* [TensorFlow API doc](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedRegressor)\n", "  \n", "Regarding **AuzreML**, please refer:\n", "* [Quickstart notebook](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python)\n", "* [Hyperdrive](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters)\n", "* [Tensorflow model tuning with hyperdrive](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-train-tensorflow)\n", "\n", "> \\* When you web-search \"Azure Machine Learning\", you will most likely to see mixed results of Azure Machine Learning (we call it AML) and Azure Machine Learning **Studio**. Please note they are different services where AML's focuses are on ML model management, tracking and hyperparameter tuning, while the [ML Studio](https://studio.azureml.net/)'s is to provide a high-level tool for 'easy-to-use' experience of ML designing and experimentation based on GUI.     "]}, {"block": 2, "type": "markdown", "linesLength": 12, "startIndex": 23, "lines": ["### Prerequisite\n", "To run this example, you'll need to install [`azureml-sdk`](https://pypi.org/project/azureml-sdk/).\n", "If you are using a [Data Science Virtual Machine (DSVM)](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-environment#dsvm) or [Azure Notebook](https://notebooks.azure.com/), `azureml-sdk` is already installed in it so you don't need to install the package.\n", "\n", "To install AML Python SDK*, run\n", "```\n", "pip install --upgrade azureml-sdk[notebooks]\n", "```\n", "\n", "More info about setting up AML environment can be found from [this link](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-environment).\n", "\n", "> \\* AML has a Databricks sdk `azureml-sdk[databricks]` but it doesn't support hyperparameter tuning on Databricks for now."]}, {"block": 3, "type": "markdown", "linesLength": 24, "startIndex": 35, "lines": ["### AML Workspace Configuration\n", "AML workspace is the foundational block in the cloud that you use to experiment, train, and deploy machine learning models. We 1) setup a workspace from Azure portal and 2) create a config file manually. The instructions here are based on AML documents about [Quickstart with Azure portal](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-get-started) and [Quickstart with Python SDK](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python) where you can find more details about the setup process with screen-shots.\n", "  \n", "<br>\n", "  \n", "#### Create a workspace\n", "1. Sign in to the [Azure portal](https://portal.azure.com) by using the credentials for the Azure subscription you use.\n", "2. Select **Create a resource** menu, search for **Machine Learning service workspace** select **Create** button.\n", "3. In the **ML service workspace** pane, configure your workspace with entering the *workspace name* and *resource group* (or **create new** resource group if you don't have one already), and select **Create**. It can take a few moments to create the workspace.\n", "  \n", "<br>\n", "  \n", "#### Make a configuration file\n", "To configure this notebook to communicate with your workspace easily, create a *.\\aml_config\\config.json* file with the following contents:\n", "```\n", "{\n", "    \"subscription_id\": \"<subscription-id>\",\n", "    \"resource_group\": \"<resource-group>\",\n", "    \"workspace_name\": \"<workspace-name>\"\n", "}\n", "```\n", "replacing `<subscription-id>`, `<resource-group>`, and `<workspace-name>` with the strings of your subscription id, resource group, and workspace name, respectively.\n", "\n", "Now let's see if everything is ready!"]}, {"block": 4, "type": "code", "linesLength": 19, "startIndex": 59, "lines": ["import sys\n", "sys.path.append(\"../../\")\n", "\n", "import os\n", "import shutil\n", "import itertools\n", "\n", "import pandas as pd\n", "import sklearn.preprocessing\n", "\n", "import azureml as aml\n", "import azureml.widgets\n", "import azureml.train.dnn\n", "import azureml.train.hyperdrive as hd\n", "\n", "from reco_utils.dataset import movielens\n", "from reco_utils.dataset.python_splitters import python_random_split\n", "\n", "print(\"Azure ML SDK Version:\", aml.core.VERSION)"]}, {"block": 5, "type": "code", "linesLength": 3, "startIndex": 78, "lines": ["# Connect to a workspace\n", "ws = aml.core.Workspace.from_config()\n", "print(\"AML workspace name: \", ws.name)"]}, {"block": 6, "type": "markdown", "linesLength": 4, "startIndex": 81, "lines": ["From the following cells, we\n", "1. Create a *remote compute target* (gpu-cluster) if it does not exist already,\n", "2. Mount a *data store* and upload the training set, and\n", "3. Run a hyperparameter tuning experiment."]}, {"block": 7, "type": "markdown", "linesLength": 5, "startIndex": 85, "lines": ["### Create a Remote Compute Target\n", "\n", "We create a gpu cluster for our remote compute target. The script will load the cluster if it already exists. You can see [this document](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets) to learn more about setting up a *compute target*.\n", "\n", "> Note, we create low priority cluster to save the cost."]}, {"block": 8, "type": "code", "linesLength": 19, "startIndex": 90, "lines": ["CLUSTER_NAME = 'gpu-cluster-16'\n", "\n", "try:\n", "    compute_target = aml.core.compute.ComputeTarget(workspace=ws, name=CLUSTER_NAME)\n", "    print(\"Found existing compute target\")\n", "except aml.core.compute_target.ComputeTargetException:\n", "    print(\"Creating a new compute target...\")\n", "    compute_config = aml.core.compute.AmlCompute.provisioning_configuration(\n", "        vm_size='STANDARD_NC6',\n", "        vm_priority='lowpriority',\n", "        min_nodes=4,\n", "        max_nodes=16\n", "    )\n", "    # create the cluster\n", "    compute_target = aml.core.compute.ComputeTarget.create(ws, CLUSTER_NAME, compute_config)\n", "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n", "\n", "# Use the 'status' property to get a detailed status for the current cluster. \n", "print(compute_target.status.serialize())"]}, {"block": 9, "type": "markdown", "linesLength": 3, "startIndex": 109, "lines": ["### Prepare Dataset\n", "1. Download data and split into training, evaluation, and testing sets\n", "2. Upload training and evaluation sets to the workspace's default **blob storage**"]}, {"block": 10, "type": "code", "linesLength": 10, "startIndex": 112, "lines": ["# Recommend top k items\n", "TOP_K = 10\n", "\n", "# Select Movielens data size: 100k, 1m, 10m, or 20m\n", "MOVIELENS_DATA_SIZE = '1m'\n", "\n", "USER_COL = 'UserId'\n", "ITEM_COL = 'MovieId'\n", "RATING_COL = 'Rating'\n", "ITEM_FEAT_COL = 'Genres'"]}, {"block": 11, "type": "code", "linesLength": 6, "startIndex": 122, "lines": ["data = movielens.load_pandas_df(\n", "    size=MOVIELENS_DATA_SIZE,\n", "    header=[USER_COL, ITEM_COL, RATING_COL],\n", "    genres_col='Genres_string'\n", ")\n", "data.head()"]}, {"block": 12, "type": "code", "linesLength": 6, "startIndex": 128, "lines": ["# Encode 'genres' into int array (multi-hot representation) to use as item features\n", "genres_encoder = sklearn.preprocessing.MultiLabelBinarizer()\n", "data[ITEM_FEAT_COL] = genres_encoder.fit_transform(\n", "    data['Genres_string'].apply(lambda s: s.split(\"|\"))\n", ").tolist()\n", "print(\"Genres:\", genres_encoder.classes_)"]}, {"block": 13, "type": "code", "linesLength": 7, "startIndex": 134, "lines": ["# Evaluation set for the hyper-parameter tuning should be separated from the test set.\n", "# In this example, we don't test the model.\n", "train, _ = python_random_split(\n", "    data.drop('Genres_string', axis=1),\n", "    ratio=0.75,\n", "    seed=123\n", ")"]}, {"block": 14, "type": "code", "linesLength": 14, "startIndex": 141, "lines": ["DATA_DIR = 'aml_data'\n", "os.makedirs(DATA_DIR, exist_ok=True)\n", "\n", "TRAIN_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_train.pkl\"\n", "train.to_pickle(os.path.join(DATA_DIR, TRAIN_FILE_NAME))\n", "\n", "# Note, all the files under DATA_DIR will be uploaded to the data store\n", "ds = ws.get_default_datastore()\n", "ds.upload(\n", "    src_dir=DATA_DIR,\n", "    target_path='data',\n", "    overwrite=True,\n", "    show_progress=True\n", ")"]}, {"block": 15, "type": "markdown", "linesLength": 1, "startIndex": 155, "lines": ["We also prepare a training script [wide_deep_training.py](../../reco_utils/aml/wide_deep_training.py) for the hyperparameter tuning, which will log our target metrics such as [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation) and/or [NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) to AML experiment so that we can track the metrics and optimize the primary metric via **hyperdrive**."]}, {"block": 16, "type": "code", "linesLength": 8, "startIndex": 156, "lines": ["SCRIPT_DIR = 'aml_script'\n", "dest_dir = os.path.join(SCRIPT_DIR, 'reco_utils')\n", "try:\n", "    shutil.copytree(os.path.join('..', '..', 'reco_utils'), dest_dir)\n", "except FileExistsError:\n", "    pass\n", "\n", "ENTRY_SCRIPT_NAME = 'reco_utils/aml/wide_deep_training.py'"]}, {"block": 17, "type": "markdown", "linesLength": 3, "startIndex": 164, "lines": ["Now we define a search space for the hyperparameters. All the parameter values will be passed to our training script.\n", "\n", "AML hyperdrive provides `RandomParameterSampling`, `GridParameterSampling`, and `BayesianParameterSampling`. Details about each approach are beyond the scope of this notebook and you can find them from [Azure doc](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters). Here, we use the Bayesian sampling."]}, {"block": 18, "type": "code", "linesLength": 38, "startIndex": 167, "lines": ["EXP_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_wide_deep_model\"\n", "METRICS = ['mae', 'ndcg@10']  # Put your primary metric at the first place\n", "NUM_EPOCHS = int(20000000 / len(train))\n", "\n", "script_params = {\n", "    '--datastore': ds.as_mount(),\n", "    '--train-datapath': \"data/\" + TRAIN_FILE_NAME,\n", "    '--user-col': USER_COL,\n", "    '--item-col': ITEM_COL,\n", "    '--item-feat-col': ITEM_FEAT_COL,\n", "    '--rating-col': RATING_COL,\n", "    '--metrics': METRICS,\n", "    '--model-type': 'wide_deep',\n", "    '--epochs': NUM_EPOCHS,\n", "}\n", "\n", "# hyperparameters search space\n", "hyper_params = {\n", "    '--batch-size': hd.choice(32, 64, 128, 256),\n", "    # Wide model hyperparameters\n", "    '--linear-optimizer': hd.choice('Ftrl', 'SGD'),\n", "    '--linear-optimizer-lr': hd.uniform(0.0005, 0.1),\n", "    '--l1-reg': hd.uniform(0.0, 0.1),\n", "    # Deep model hyperparameters\n", "    '--dnn-optimizer': hd.choice('Adagrad', 'Adam'),\n", "    '--dnn-optimizer-lr': hd.uniform(0.0005, 0.1),\n", "    '--dnn-user-embedding-dim': hd.choice(8, 32, 128),\n", "    '--dnn-item-embedding-dim': hd.choice(4, 16, 64),\n", "    '--dnn-hidden-layer-1': hd.choice(0, 32, 64, 128, 256, 512, 1024),\n", "    '--dnn-hidden-layer-2': hd.choice(0, 32, 64, 128, 256, 512, 1024),\n", "    '--dnn-hidden-layer-3': hd.choice(0, 32, 64, 128, 256, 512, 1024),\n", "    '--dnn-hidden-layer-4': hd.choice(32, 64, 128, 256, 512, 1024),\n", "    '--dnn-batch-norm': hd.choice(0, 1),  # False or True. Bayesian sampling only accept int, str or float\n", "    '--dropout': hd.uniform(0.0, 0.5),\n", "}\n", "\n", "# Note, BayesianParameterSampling only support choice, uniform, and quniform\n", "ps = hd.BayesianParameterSampling(hyper_params)"]}, {"block": 19, "type": "markdown", "linesLength": 10, "startIndex": 205, "lines": ["We use `azureml.train.dnn.TensorFlow`, a custom AML `Estimator` class which utilizes a preset docker image in the cluster (see more information from [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-train-tensorflow)).\n", "\n", "Once you submit the experiment, you can see the progress from the notebook by using `azureml.widgets.RunDetails`. You can directly check the details from the Azure portal as well. To get the link, run `run.get_portal_url()`.\n", "\n", "For RandomSampling, you can use early termnination policy\n", "```\n", "policy = hd.BanditPolicy(evaluation_interval=1, slack_factor=0.1, delay_evaluation=3)\n", "```\n", "\n", "> Since we will do hyperparameter tuning, we create a `HyperDriveRunConfig` and pass it to the experiment object. If you already know what hyperparameters to use and still want to utilize AML for other purposes (e.g. model management), you can set the hyperparameter values directly to `script_params` and run the experiment, `run = exp.submit(est)`, instead.  "]}, {"block": 20, "type": "code", "linesLength": 21, "startIndex": 215, "lines": ["est = azureml.train.dnn.TensorFlow(\n", "    source_directory=SCRIPT_DIR,\n", "    entry_script=ENTRY_SCRIPT_NAME,\n", "    script_params=script_params,\n", "    compute_target=compute_target,\n", "    use_gpu=True,\n", "    conda_packages=['pandas', 'scikit-learn'],\n", ")\n", "\n", "hd_config = hd.HyperDriveRunConfig(\n", "    estimator=est, \n", "    hyperparameter_sampling=ps,\n", "    primary_metric_name=METRICS[0],\n", "    primary_metric_goal=hd.PrimaryMetricGoal.MINIMIZE, \n", "    max_total_runs=100,\n", "    max_concurrent_runs=8\n", ")\n", "\n", "# Create an experiment to track the runs in the workspace\n", "exp = aml.core.Experiment(workspace=ws, name=EXP_NAME)\n", "run = exp.submit(config=hd_config)"]}, {"block": 21, "type": "code", "linesLength": 2, "startIndex": 236, "lines": ["azureml.widgets.RunDetails(run).show()\n", "run.wait_for_completion(show_output=True)"]}, {"block": 22, "type": "code", "linesLength": 5, "startIndex": 238, "lines": ["# Get best run and printout metrics\n", "best_run = run.get_best_run_by_primary_metric()\n", "\n", "best_run_metrics = best_run.get_metrics()\n", "parameter_values = best_run.get_details()['runDefinition']['Arguments']"]}, {"block": 23, "type": "code", "linesLength": 1, "startIndex": 243, "lines": ["print(parameter_values)"]}, {"block": 24, "type": "code", "linesLength": 5, "startIndex": 244, "lines": ["try:\n", "    shutil.rmtree(SCRIPT_DIR)\n", "    shutil.rmtree(DATA_DIR)\n", "except (PermissionError, FileNotFoundError):\n", "    pass"]}, {"block": 25, "type": "markdown", "linesLength": 6, "startIndex": 249, "lines": ["### References\n", "\n", "https://github.com/MtDersvan/tf_playground/blob/master/wide_and_deep_tutorial/wide_and_deep_export_r1.3.ipynb\n", "\n", "* [Fine-tune natural language processing models using Azure Machine Learning service](https://azure.microsoft.com/en-us/blog/fine-tune-natural-language-processing-models-using-azure-machine-learning-service/)\n", "* [Training, hyperparameter tune, and deploy with TensorFlow](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training-with-deep-learning/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb)\n"]}]