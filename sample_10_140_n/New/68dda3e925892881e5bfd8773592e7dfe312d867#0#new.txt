[{"block": 0, "type": "markdown", "linesLength": 3, "startIndex": 0, "lines": ["# Introduction: Prediction Intervals with Scikit-Learn\n", "\n", "In this notebook, we'll use the Gradient Boosting Regressor in Scikit-Learn to produce prediction intervals in addition to a single estimate of the target. Prediction intervals are useful when we want to show the uncertainty inherent in any prediction."]}, {"block": 1, "type": "markdown", "linesLength": 3, "startIndex": 3, "lines": ["## Imports \n", "\n", "We're using a pretty typical data science stack plus a few visualization and interact tools for making interactive visualizations. "]}, {"block": 2, "type": "code", "linesLength": 27, "startIndex": 6, "lines": ["# Data Manipulation\n", "import pandas as pd\n", "import numpy as np\n", "\n", "\n", "# Modeling\n", "from sklearn.base import BaseEstimator\n", "from sklearn.ensemble import GradientBoostingRegressor\n", "\n", "# File finding\n", "import glob\n", "files = glob.glob('data/*_energy_data.csv')\n", "\n", "# Interactivity\n", "from ipywidgets import interact, widgets\n", "\n", "# Visualization\n", "\n", "# Plotly\n", "import plotly.graph_objs as go\n", "from plotly.offline import iplot, plot, init_notebook_mode\n", "init_notebook_mode(connected=True)\n", "import plotly_express as px\n", "\n", "# cufflinks is a wrapper on plotly\n", "import cufflinks as cf\n", "cf.go_offline(connected=True)"]}, {"block": 3, "type": "markdown", "linesLength": 7, "startIndex": 33, "lines": ["## Explore Data\n", "\n", "For our modeling, we're going to be prediction building energy consumption. This is an important problem that we at Cortex Building Intelligence work on every day and it provides a good supervised, regression machine learning task. The energy data is measured every 15 minutes and includes 3 weather variables related to energy consumption: temperature, irradiance, and relative humidity. \n", "\n", "Although in a real application, we'd be using data from a database (study up on SQL), here we'll just load in the data from a csv. This is data from the DrivenData Energy Forecasting competition. You can find the original data [at DrivenData](https://www.drivendata.org/competitions/51/electricity-prediction-machine-learning/). I've cleaned up the datasets and extracted 8 features that allow us to predict the energy consumption fairly accurately. \n", "\n", "We're not going to spend any time on feature engineering or investigating the data, but know that should be a part of a data science pipeline."]}, {"block": 4, "type": "code", "linesLength": 2, "startIndex": 40, "lines": ["data = pd.read_csv(files[2], parse_dates=['timestamp'], index_col='timestamp').sort_index()\n", "data.head()"]}, {"block": 5, "type": "markdown", "linesLength": 3, "startIndex": 42, "lines": ["## Interactive Plot\n", "\n", "With `ipywidgets` and `plotly` (through `cufflinks`), it's easy to create an interact plot in a few lines of code. We can take a look at the energy consumption on different timescales."]}, {"block": 6, "type": "code", "linesLength": 41, "startIndex": 45, "lines": ["# Create a subset of data for plotting\n", "data_to_plot = data.loc[\"2015\"].copy()\n", "\n", "\n", "def plot_timescale(timescale, selection, theme):\n", "    \"\"\"\n", "    Plot the energy consumption on different timescales (day, week, month).\n", "    \n", "    :param timescale: the timescale to use\n", "    :param selection: the numeric value of the timescale selection (for example the 15th day\n", "    of the year or the 1st week of the year)\n", "    :param theme: aesthetics of plot\n", "    \"\"\"\n", "    # Subset based on timescale and selection\n", "    subset = data_to_plot.loc[\n", "        getattr(data_to_plot.index, timescale) == selection, \"energy\"\n", "    ].copy()\n", "\n", "    if subset.empty:\n", "        print(\"Choose another selection\")\n", "        return\n", "    \n", "    # Make an interactive plot\n", "    fig = subset.iplot(\n", "            title=f\"Energy for {selection} {timescale.title()}\", theme=theme, asFigure=True\n", "    )\n", "    fig['layout']['height'] = 900\n", "    fig['layout']['width'] = 1400\n", "    iplot(fig)\n", "    \n", "\n", "\n", "_ = interact(\n", "    plot_timescale,\n", "    timescale=widgets.RadioButtons(\n", "        options=[\"dayofyear\", \"week\", \"month\"], value=\"dayofyear\"\n", "    ),\n", "    # Selection \n", "    selection=widgets.IntSlider(value=16, min=0, max=365),\n", "    theme=widgets.Select(options=cf.themes.THEMES.keys(), value='ggplot')\n", ")"]}, {"block": 7, "type": "markdown", "linesLength": 1, "startIndex": 86, "lines": ["Clearly, there are different patterns in energy usage over the course of a day, week, and month. We can also look at longer timescales."]}, {"block": 8, "type": "code", "linesLength": 1, "startIndex": 87, "lines": ["data.loc['2015', 'energy'].iplot(layout=dict(title='2015 Energy Consumption', height=800))"]}, {"block": 9, "type": "markdown", "linesLength": 1, "startIndex": 88, "lines": ["Plotting this much data can make the notebook slow. Instead, we can resample the data and plot to see any long term trends."]}, {"block": 10, "type": "code", "linesLength": 1, "startIndex": 89, "lines": ["data.resample('12 H')['energy'].mean().iplot(layout=dict(title='Energy Data Resampled at 12 Hours', height=800))"]}, {"block": 11, "type": "markdown", "linesLength": 13, "startIndex": 90, "lines": ["# Predicting Intervals with the Gradient Boosting Regressor\n", "\n", "If this was a real application, we'd problem want to spend more time understanding the data and checking for outliers. However, the main focus here is to predict intervals so let's dive into the modeling.\n", "\n", "This code is based on an [example from Scikit-Learn](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html). The basic idea is straightforward:\n", "\n", "* For the lower prediction, use the `GradientBoostingRegressor` with `loss='quantile'` and `alpha=lower_quantile` (for example, 0.1)\n", "* For the upper prediction, use the `GradientBoostingRegressor` with `loss='quantile'` and `alpha=upper_quantile` (for example, 0.9)\n", "* For the middle prediction (generally taken to be the median), we have several options. We can use `GradientBoostingRegressor` with `loss='quantile'` and `alpha=0.5` or, we can use the same model with the default `loss=ls` (for least squares) which optimizes for the median.\n", "\n", "The loss refers to the metric which is optimized by the model. We won't get into the details right here (see Quantile Loss Explained below), but for more information on the quantile loss and regression, this article is probably the best place to start: https://towardsdatascience.com/quantile-regression-from-linear-models-to-trees-to-deep-learning-af3738b527c3. [The Wikipedia page on Quantile Regression](https://en.wikipedia.org/wiki/Quantile_regression) gets into slightly more detail. For the original explanation of the Gradient Boosting model, see [Friedman's 1999 paper](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf) \"Greedy Function Approximation: A Gradient Boosting Machine\". \n", "\n", "You don't need to know the details to implement the prediction intervals, although it can be useful to tuning the model. Now, let's make the predictions, and later in the notebook we'll take a high-level look at the loss function."]}, {"block": 12, "type": "code", "linesLength": 8, "startIndex": 103, "lines": ["# Train and test sets\n", "X_train = data.loc[\"2015\":\"2016\"].copy()\n", "X_test = data.loc[\"2017\":].copy()\n", "y_train = X_train.pop(\"energy\")\n", "y_test = X_test.pop(\"energy\")\n", "\n", "X_train.tail()\n", "X_test.head()"]}, {"block": 13, "type": "markdown", "linesLength": 1, "startIndex": 111, "lines": ["We're assuming we know all the weather in the test set, which isn't entirely realistic, but we'll use it for now!"]}, {"block": 14, "type": "markdown", "linesLength": 1, "startIndex": 112, "lines": ["### Build Models for Lower, Upper Quantile and Median"]}, {"block": 15, "type": "code", "linesLength": 18, "startIndex": 113, "lines": ["# Set lower and upper quantile\n", "LOWER_ALPHA = 0.15\n", "UPPER_ALPHA = 0.85\n", "\n", "N_ESTIMATORS = 100\n", "MAX_DEPTH = 5\n", "\n", "# Each model has to be separate\n", "\n", "lower_model = GradientBoostingRegressor(\n", "    loss=\"quantile\", alpha=LOWER_ALPHA, n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH\n", ")\n", "# The mid model will use the default\n", "mid_model = GradientBoostingRegressor(loss=\"ls\", n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH)\n", "\n", "upper_model = GradientBoostingRegressor(\n", "    loss=\"quantile\", alpha=UPPER_ALPHA, n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH\n", ")"]}, {"block": 16, "type": "markdown", "linesLength": 3, "startIndex": 131, "lines": ["### Train Models\n", "\n", "The models are trained based on optimizing for the specific loss function. This means __we have to build 3 separate models to predict the different objectives.__ A downside of this method is that it's a little slow, particularly because we can't parallelize training on the Scikit-Learn Gradient Boosting Regresssor. If you wanted, you could re-write this code to train each model on a separate processor (using `multiprocessing`.)"]}, {"block": 17, "type": "code", "linesLength": 3, "startIndex": 134, "lines": ["_ = lower_model.fit(X_train, y_train)\n", "_ = mid_model.fit(X_train, y_train)\n", "_ = upper_model.fit(X_train, y_train)"]}, {"block": 18, "type": "markdown", "linesLength": 3, "startIndex": 137, "lines": ["### Make Predictions\n", "\n", "With the models all trained, we now make predictions and record them with the true values."]}, {"block": 19, "type": "code", "linesLength": 6, "startIndex": 140, "lines": ["predictions = pd.DataFrame(y_test)\n", "predictions['lower'] = lower_model.predict(X_test)\n", "predictions['mid'] = mid_model.predict(X_test)\n", "predictions['upper'] = upper_model.predict(X_test)\n", "\n", "predictions.tail()"]}, {"block": 20, "type": "markdown", "linesLength": 3, "startIndex": 146, "lines": ["## Prediction Intervals Plot\n", "\n", "The best way to inspect these results is visually. Here we use plotly to make a filled area chart showing the prediction intervals."]}, {"block": 21, "type": "code", "linesLength": 100, "startIndex": 149, "lines": ["def plot_intervals(predictions, mid=False, start=None, stop=None, title=None):\n", "    \"\"\"\n", "    Function for plotting prediction intervals as filled area chart.\n", "    \n", "    :param predictions: dataframe of predictions with lower, upper, and actual columns (named for the target)\n", "    :param whether to show the mid prediction\n", "    :param start: optional parameter for subsetting start of predictions\n", "    :param stop: optional parameter for subsetting end of predictions\n", "    :param title: optional string title\n", "    \n", "    :return fig: plotly figure\n", "    \"\"\"\n", "    # Subset if required\n", "    predictions = (\n", "        predictions.loc[start:stop].copy()\n", "        if start is not None or stop is not None\n", "        else predictions.copy()\n", "    )\n", "    data = []\n", "\n", "    # Lower trace will fill to the upper trace\n", "    trace_low = go.Scatter(\n", "        x=predictions.index,\n", "        y=predictions[\"lower\"],\n", "        fill=\"tonexty\",\n", "        line=dict(color=\"darkblue\"),\n", "        fillcolor=\"rgba(173, 216, 230, 0.4)\",\n", "        showlegend=True,\n", "        name=\"lower\",\n", "    )\n", "    # Upper trace has no fill\n", "    trace_high = go.Scatter(\n", "        x=predictions.index,\n", "        y=predictions[\"upper\"],\n", "        fill=None,\n", "        line=dict(color=\"orange\"),\n", "        showlegend=True,\n", "        name=\"upper\",\n", "    )\n", "\n", "    # Must append high trace first so low trace fills to the high trace\n", "    data.append(trace_high)\n", "    data.append(trace_low)\n", "    \n", "    if mid:\n", "        trace_mid = go.Scatter(\n", "        x=predictions.index,\n", "        y=predictions[\"mid\"],\n", "        fill=None,\n", "        line=dict(color=\"green\"),\n", "        showlegend=True,\n", "        name=\"mid\",\n", "    )\n", "        data.append(trace_mid)\n", "\n", "    # Trace of actual values\n", "    trace_actual = go.Scatter(\n", "        x=predictions.index,\n", "        y=predictions[\"energy\"],\n", "        fill=None,\n", "        line=dict(color=\"black\"),\n", "        showlegend=True,\n", "        name=\"actual\",\n", "    )\n", "    data.append(trace_actual)\n", "\n", "    # Layout with some customization\n", "    layout = go.Layout(\n", "        height=900,\n", "        width=1400,\n", "        title=dict(text=\"Prediction Intervals\" if title is None else title),\n", "        yaxis=dict(title=dict(text=\"kWh\")),\n", "        xaxis=dict(\n", "            rangeselector=dict(\n", "                buttons=list(\n", "                    [\n", "                        dict(count=1, label=\"1d\", step=\"day\", stepmode=\"backward\"),\n", "                        dict(count=7, label=\"1w\", step=\"day\", stepmode=\"backward\"),\n", "                        dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n", "                        dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n", "                        dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n", "                        dict(step=\"all\"),\n", "                    ]\n", "                )\n", "            ),\n", "            rangeslider=dict(visible=True),\n", "            type=\"date\",\n", "        ),\n", "    )\n", "\n", "    fig = go.Figure(data=data, layout=layout)\n", "\n", "    # Make sure font is readable\n", "    fig[\"layout\"][\"font\"] = dict(size=20)\n", "    fig.layout.template = \"plotly_white\"\n", "    return fig\n", "\n", "\n", "# Example plot subsetted to one week\n", "fig = plot_intervals(predictions, start=\"2017-03-01\", stop=\"2017-03-08\")"]}, {"block": 22, "type": "code", "linesLength": 1, "startIndex": 249, "lines": ["iplot(fig)"]}, {"block": 23, "type": "markdown", "linesLength": 1, "startIndex": 250, "lines": ["Not too bad for a first try! Play around with the model parameters and the `alpha` parameters to see how it affects the plot."]}, {"block": 24, "type": "markdown", "linesLength": 5, "startIndex": 251, "lines": ["### Calculate Error\n", "\n", "With any machine learning model, we want to make predictions of the error. Quantifying the error of a prediction range can be tricky. We'll start off with the percentage of the time that the actual value falls in the range. However, one way to maximize this metric would be to just use extremely wide prediction intervals. Therefore, we want to penalize the model for making too wide prediction intervals. As a simple example (let me know ideas for a better metric) we can calculate the absolute error of the bottom and top lines, and divide by two to get an absolute error. We then take the average for the mean absolute error. We can also calculate the absolute error of the mid predictions.\n", "\n", "These are likely not the best metrics for all cases, so think about what your objective is before selecting a metric. For instance, you may want wide prediction intervals in which case you value the percent in bounds more than the error, but other times, you might want a narrow range of estimates."]}, {"block": 25, "type": "code", "linesLength": 15, "startIndex": 256, "lines": ["def calculate_error(predictions):\n", "    \"\"\"\n", "    Calculate the absolute error associated with prediction intervals\n", "    \n", "    :param predictions: dataframe of predictions\n", "    :return: None, modifies the prediction dataframe\n", "    \n", "    \"\"\"\n", "    predictions['absolute_error_lower'] = (predictions['lower'] - predictions['energy']).abs()\n", "    predictions['absolute_error_upper'] = (predictions['upper'] - predictions['energy']).abs()\n", "    \n", "    predictions['absolute_error_interval'] = (predictions['absolute_error_lower'] + predictions['absolute_error_upper']) / 2\n", "    predictions['absolute_error_mid'] = (predictions['mid'] - predictions['energy']).abs()\n", "    \n", "    predictions['in_bounds'] = predictions['energy'].between(left=predictions['lower'], right=predictions['upper'])"]}, {"block": 26, "type": "code", "linesLength": 3, "startIndex": 271, "lines": ["calculate_error(predictions)\n", "metrics = predictions[['absolute_error_lower', 'absolute_error_upper', 'absolute_error_interval', 'absolute_error_mid', 'in_bounds']].copy()\n", "metrics.describe()"]}, {"block": 27, "type": "markdown", "linesLength": 1, "startIndex": 274, "lines": ["We see the lower prediction has a smaller absolute error (in terms of the median). It's interesting the absolute error for the lower bound is actually less than that for the middle prediction! We can write a short function to display the metrics."]}, {"block": 28, "type": "code", "linesLength": 40, "startIndex": 275, "lines": ["def show_metrics(metrics):\n", "    \"\"\"\n", "    Make a boxplot of the metrics associated with prediction intervals\n", "    \n", "    :param metrics: dataframe of metrics produced from calculate error \n", "    :return fig: plotly figure\n", "    \"\"\"\n", "    percent_in_bounds = metrics['in_bounds'].mean() * 100\n", "    metrics_to_plot = metrics[[c for c in metrics if 'absolute_error' in c]]\n", "\n", "    # Rename the columns\n", "    metrics_to_plot.columns = [column.split('_')[-1].title() for column in metrics_to_plot]\n", "\n", "    # Create a boxplot of the metrics\n", "    fig = px.box(\n", "        metrics_to_plot.melt(var_name=\"metric\", value_name='Absolute Error'),\n", "        x=\"metric\",\n", "        y=\"Absolute Error\",\n", "        color='metric',\n", "        title=f\"Error Metrics Boxplots    In Bounds = {percent_in_bounds:.2f}%\",\n", "        height=800,\n", "        width=1000,\n", "        points=False,\n", "    )\n", "\n", "    # Create new data with no legends\n", "    d = []\n", "\n", "    for trace in fig.data:\n", "        # Remove legend for each trace\n", "        trace['showlegend'] = False\n", "        d.append(trace)\n", "\n", "    # Make the plot look a little better\n", "    fig.data = d\n", "    fig['layout']['font'] = dict(size=20)\n", "    return fig\n", "\n", "\n", "iplot(show_metrics(metrics))"]}, {"block": 29, "type": "code", "linesLength": 3, "startIndex": 315, "lines": ["# Example plot subsetted to one week\n", "fig = plot_intervals(predictions, mid=True, start=\"2017-03-01\", stop=\"2017-03-08\")\n", "iplot(fig)"]}, {"block": 30, "type": "markdown", "linesLength": 3, "startIndex": 318, "lines": ["# Prediction Interval Class\n", "\n", "To make this process repeatable, we can build our own estimator with a Scikit-Learn interface that fits and predicts all 3 models in one call each. This is a very simple class but can be extended based on your needs (feel free to show me ways to improve)."]}, {"block": 31, "type": "code", "linesLength": 85, "startIndex": 321, "lines": ["class GradientBoostingPredictionIntervals(BaseEstimator):\n", "    \"\"\"\n", "    Model that produces prediction intervals with a Scikit-Learn inteface\n", "    \n", "    :param lower_alpha: lower quantile for prediction, default=0.1\n", "    :param upper_alpha: upper quantile for prediction, default=0.9\n", "    :param **kwargs: additional keyword arguments for creating a GradientBoostingRegressor model\n", "    \"\"\"\n", "\n", "    def __init__(self, lower_alpha=0.1, upper_alpha=0.9, **kwargs):\n", "        self.lower_alpha = lower_alpha\n", "        self.upper_alpha = upper_alpha\n", "\n", "        # Three separate models\n", "        self.lower_model = GradientBoostingRegressor(\n", "            loss=\"quantile\", alpha=self.lower_alpha, **kwargs\n", "        )\n", "        self.mid_model = GradientBoostingRegressor(loss=\"ls\", **kwargs)\n", "        self.upper_model = GradientBoostingRegressor(\n", "            loss=\"quantile\", alpha=self.upper_alpha, **kwargs\n", "        )\n", "        self.predictions = None\n", "\n", "    def fit(self, X, y):\n", "        \"\"\"\n", "        Fit all three models\n", "            \n", "        :param X: train features\n", "        :param y: train targets\n", "        \n", "        TODO: parallelize this code across processors\n", "        \"\"\"\n", "        self.lower_model.fit(X_train, y_train)\n", "        self.mid_model.fit(X_train, y_train)\n", "        self.upper_model.fit(X_train, y_train)\n", "\n", "    def predict(self, X, y):\n", "        \"\"\"\n", "        Predict with all 3 models \n", "        \n", "        :param X: test features\n", "        :param y: test targets\n", "        :return predictions: dataframe of predictions\n", "        \n", "        TODO: parallelize this code across processors\n", "        \"\"\"\n", "        predictions = pd.DataFrame(y)\n", "        predictions[\"lower\"] = self.lower_model.predict(X)\n", "        predictions[\"mid\"] = self.mid_model.predict(X)\n", "        predictions[\"upper\"] = self.upper_model.predict(X)\n", "        self.predictions = predictions\n", "\n", "        return predictions\n", "\n", "    def plot_intervals(self, mid=False, start=None, stop=None):\n", "        \"\"\"\n", "        Plot the prediction intervals\n", "        \n", "        :param mid: boolean for whether to show the mid prediction\n", "        :param start: optional parameter for subsetting start of predictions\n", "        :param stop: optional parameter for subsetting end of predictions\n", "    \n", "        :return fig: plotly figure\n", "        \"\"\"\n", "\n", "        if self.predictions is None:\n", "            raise ValueError(\"This model has not yet made predictions.\")\n", "            return\n", "        \n", "        fig = plot_intervals(predictions, mid=mid, start=start, stop=stop)\n", "        return fig\n", "    \n", "    def calculate_and_show_errors(self):\n", "        \"\"\"\n", "        Calculate and display the errors associated with a set of prediction intervals\n", "        \n", "        :return fig: plotly boxplot of absolute error metrics\n", "        \"\"\"\n", "        if self.predictions is None:\n", "            raise ValueError(\"This model has not yet made predictions.\")\n", "            return\n", "        \n", "        calculate_error(self.predictions)\n", "        fig = show_metrics(self.predictions)\n", "        return fig"]}, {"block": 32, "type": "markdown", "linesLength": 1, "startIndex": 406, "lines": ["To use the model, we just fit and predict like any Scikit-Learn model. This time though, the predictions are a dataframe with intervals."]}, {"block": 33, "type": "code", "linesLength": 5, "startIndex": 407, "lines": ["model = GradientBoostingPredictionIntervals(lower_alpha=0.1, upper_alpha=0.9, n_estimators=50, max_depth=3)\n", "\n", "# Fit and make predictions\n", "_ = model.fit(X_train, y_train)\n", "predictions = model.predict(X_test, y_test)"]}, {"block": 34, "type": "markdown", "linesLength": 1, "startIndex": 412, "lines": ["We can show the interval and metric plot to assess the predictions."]}, {"block": 35, "type": "code", "linesLength": 3, "startIndex": 413, "lines": ["fig = model.plot_intervals(mid=True, start='2017-05-01', \n", "                           stop='2017-06-01')\n", "iplot(fig)"]}, {"block": 36, "type": "code", "linesLength": 2, "startIndex": 416, "lines": ["metric_fig = model.calculate_and_show_errors()\n", "iplot(metric_fig)"]}, {"block": 37, "type": "markdown", "linesLength": 17, "startIndex": 418, "lines": ["# Quantile Loss Explained\n", "\n", "The quantile loss for a predicted is expressed as:\n", "\n", "\n", "$$ \\text{loss} = \\alpha * (\\text{actual} - \\text{predicted}) \\quad \\text{if} \\quad (\\text{actual} - \\text{predicted}) > 0$$\n", "$$ \\text{loss} = (\\alpha - 1) *(\\text{actual} - \\text{predicted}) \\quad \\text{if} \\quad (\\text{actual} - \\text{predicted}) < 0$$\n", "\n", "I find this much easier to parse in Python code. \n", "\n", "```python\n", "def calculate_quantile_loss(quantile, actual, predicted):\n", "    \"\"\"\n", "    Quantile loss for a given quantile and prediction\n", "    \"\"\"\n", "    return np.maximum(quantile * (actual - predicted), (quantile - 1) * (actual - predicted))\n", "```"]}, {"block": 38, "type": "code", "linesLength": 5, "startIndex": 435, "lines": ["def calculate_quantile_loss(quantile, actual, predicted):\n", "    \"\"\"\n", "    Quantile loss for a given quantile and prediction\n", "    \"\"\"\n", "    return np.maximum(quantile * (actual - predicted), (quantile - 1) * (actual - predicted))"]}, {"block": 39, "type": "markdown", "linesLength": 16, "startIndex": 440, "lines": ["## Walkthrough\n", "\n", "When we graph the quantile loss versus the error, the weighting of the errors appears as the slope. \n", "\n", "Let's walk through an example using lower quantile = 0.1, upper quantile = 0.9, and actual value = 10. There are four possibilities for the predictions:\n", "\n", "1. Prediction = 15 with Quantile = 0.1. Actual < Predicted; Loss = (0.1 - 1) * (10 - 15) = 4.5\n", "2. Prediction = 5 with Quantile = 0.1. Actual > Predicted; Loss = 0.1 * (10 - 5) = 1\n", "3. Predicted = 15 with Quantile = 0.9. Actual < Predicted; Loss = (0.9 - 1) * (10 - 15) = 1\n", "4. Predicted = 5 with Quantile = 0.9. Actual < Predicted; Loss = 0.9 * (10 - 5) = 4.5\n", "\n", "__For cases where the quantile > 0.5 we penalize low predictions more heavily. For cases where the quantile < 0.5 we penalize high predictions more heavily.__ \n", "\n", "If the quantile == 0.5, then the weighting is the same for both low and high predictions. For quantile == 0.5, we are predicting the median.\n", "\n", "__The model is fit by minimizing the loss function. Through changing the quantile, we can produce predictions corresponding to prediction intervals.__"]}, {"block": 40, "type": "code", "linesLength": 46, "startIndex": 456, "lines": ["def plot_quantile_loss(actual, prediction_list, quantile_list):\n", "    \"\"\"\n", "    Shows the quantile loss associated with predictions at different quantiles.\n", "    Figure shows the loss versus the error\n", "    \n", "    :param actual: array-like of actual values\n", "    :param prediction_list: list of array-like predictions\n", "    :param quantile_list: list of float quantiles corresponding to the predictions\n", "    \n", "    :return fig: plotly figure\n", "    \"\"\"\n", "    data = []\n", "\n", "    # Iterate through each combination of prediction and quantile\n", "    for predictions, quantile in zip(prediction_list, quantile_list):\n", "        # Calculate the loss\n", "        quantile_loss = calculate_quantile_loss(quantile, actual, predictions)\n", "        \n", "        errors = actual - predictions\n", "        # Sort errors and loss by error\n", "        idx = np.argsort(errors)\n", "        errors = errors[idx]; quantile_loss = quantile_loss[idx]\n", "    \n", "        # Add data to plot\n", "        data.append(go.Scatter(mode=\"lines\", x=errors, y=quantile_loss, line=dict(width=4), name=f\"{quantile} Quantile\"))\n", "\n", "    # Simple plot layout\n", "    layout = go.Layout(\n", "        title=\"Quantile Loss vs Error at Different Quantiles\",\n", "        yaxis=dict(title=\"Quantile Loss\"),\n", "        xaxis=dict(title=\"Error\"),\n", "        width=1000, height=600,\n", "    )\n", "\n", "    fig = go.Figure(data=data, layout=layout)\n", "    fig['layout']['font'] = dict(size=18)\n", "    return fig\n", "\n", "\n", "# Make dummy predictions and actual values\n", "predictions = np.arange(-10, 10, step=0.5)\n", "actual = np.zeros(len(predictions))\n", "\n", "# Create a plot showing the same predictions at different quantiles\n", "fig = plot_quantile_loss(actual, [predictions, predictions, predictions], [0.1, 0.5, 0.9])\n", "iplot(fig)"]}, {"block": 41, "type": "markdown", "linesLength": 1, "startIndex": 502, "lines": ["We can see how the quantile loss is asymmetric with a weighting (slope) equal to the quantile value or to (quantile - 1) depending on if the error is positive or negative. With an error defined as (actual - predicted), for a quantile greater than 0.5, we penalize positive errors more and for a quantile less than 0.5, we penalize negative errors more. This drives the predictions with a higher quantile higher than the actual value, and predictions with a lower quantile lower than the actual value. The quantile loss is always positive. "]}, {"block": 42, "type": "markdown", "linesLength": 3, "startIndex": 503, "lines": ["## Quantile Loss on Predictions\n", "\n", "Let's look at the quantile loss associated with our model's predictions. We make sure to use the alpha associated with the lower and upper predictions from the model."]}, {"block": 43, "type": "code", "linesLength": 8, "startIndex": 506, "lines": ["predictions = model.predictions.copy()\n", "\n", "fig = plot_quantile_loss(\n", "    predictions[\"energy\"],\n", "    [predictions[\"lower\"], predictions[\"mid\"], predictions[\"upper\"]],\n", "    [model.lower_alpha, 0.5, model.upper_alpha],\n", ")\n", "iplot(fig)"]}, {"block": 44, "type": "markdown", "linesLength": 3, "startIndex": 514, "lines": ["We can see the same weighting applied to the model's predictions. When the error is negative - meaning the actual value was less than the predicted value - _and_ the quantile is __less__ than 0.5, we weight the error by (quantile - 1) to penalize the high prediction. When the error is positive - meaning the actual value was greater than the predicted value - _and_ the quantile is __greater__ than 0.5, we weight the error by the quantile to penalize the low prediction. \n", "\n", "__This is only a high level explanation, but it's enough to allow us to use the model.__ For further information, start with [this article](https://towardsdatascience.com/quantile-regression-from-linear-models-to-trees-to-deep-learning-af3738b527c3) or the [Wikipedia page on quantile loss](https://en.wikipedia.org/wiki/Quantile_regression) and dig into the sources."]}, {"block": 45, "type": "markdown", "linesLength": 10, "startIndex": 517, "lines": ["# Conclusions\n", "\n", "Predicting intervals instead of a single point value is useful to emphasize that a prediction from a model is always an estimate and hence has uncertainty. One easy way to generate prediction intervals with Scikit-Learn is through the Gradient Boosting Regressor as implemented in this notebook. There are other methods, so don't take away that there is only one way to accomplish this task. As with many aspects of machine learning / data science, you have to try different techniques to find the right one. \n", "\n", "Hopefully you found this walkthrough useful, and I encourage you to build on / improve it. The best way to learn anything is to use it solve a problem, so apply this to one of yours and let me know how you are using prediction intervals.\n", "\n", "Will Koehrsen\n", "\n", "* [@koehrsen_will](https://twitter.com/@koehrsen_will)\n", "* [Medium articles](https://medium.com/@williamkoehrsen)"]}, {"block": 46, "type": "code", "linesLength": 1, "startIndex": 527, "lines": ["iplot(model.plot_intervals(mid=True, start='2017-06-05', stop='2017-06-12'))"]}, {"block": 47, "type": "code", "linesLength": 0, "startIndex": 528, "lines": []}]