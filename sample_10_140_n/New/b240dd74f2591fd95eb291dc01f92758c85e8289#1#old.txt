[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["##### Copyright 2019 The TensorFlow Authors."]}, {"block": 1, "type": "code", "linesLength": 11, "startIndex": 1, "lines": ["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n", "# you may not use this file except in compliance with the License.\n", "# You may obtain a copy of the License at\n", "#\n", "# https://www.apache.org/licenses/LICENSE-2.0\n", "#\n", "# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n", "# limitations under the License."]}, {"block": 2, "type": "markdown", "linesLength": 1, "startIndex": 12, "lines": ["# Text classification with TensorFlow Lite Model Maker with TensorFlow 2.0"]}, {"block": 3, "type": "markdown", "linesLength": 12, "startIndex": 13, "lines": ["<table class=\"tfo-notebook-buttons\" align=\"left\">\n", "  <td>\n", "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/tensorflow_examples/lite/model_maker/demo/text_classification.ipynb\">\n", "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n", "    Run in Google Colab</a>\n", "  </td>\n", "  <td>\n", "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/tensorflow_examples/lite/model_maker/demo/text_classification.ipynb\">\n", "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n", "    View source on GitHub</a>\n", "  </td>\n", "</table>"]}, {"block": 4, "type": "markdown", "linesLength": 3, "startIndex": 25, "lines": ["The TensorFlow Lite Model Maker library simplifies the process of adapting and converting a TensorFlow neural-network model to particular input data when deploying this model for on-device ML applications.\n", "\n", "This notebook shows an end-to-end example that utilizes this Model Maker library to illustrate the adaption and conversion of a commonly-used text classification model to classify movie reviews on a mobile device."]}, {"block": 5, "type": "markdown", "linesLength": 3, "startIndex": 28, "lines": ["## Prerequisites\n", "\n", "To run this example, we first need to install several required packages, including Model Maker package that in github [repo](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker)."]}, {"block": 6, "type": "code", "linesLength": 1, "startIndex": 31, "lines": ["!pip install git+git://github.com/tensorflow/examples.git#egg=tensorflow-examples[model_maker]"]}, {"block": 7, "type": "markdown", "linesLength": 1, "startIndex": 32, "lines": ["Import the required packages."]}, {"block": 8, "type": "code", "linesLength": 10, "startIndex": 33, "lines": ["import numpy as np\n", "import os\n", "\n", "import tensorflow as tf\n", "assert tf.__version__.startswith('2')\n", "\n", "from tensorflow_examples.lite.model_maker.core.data_util.text_dataloader import TextClassifierDataLoader\n", "from tensorflow_examples.lite.model_maker.core.task.model_spec import AverageWordVecModelSpec\n", "from tensorflow_examples.lite.model_maker.core.task.model_spec import BertClassifierModelSpec\n", "from tensorflow_examples.lite.model_maker.core.task import text_classifier"]}, {"block": 9, "type": "markdown", "linesLength": 1, "startIndex": 43, "lines": ["## Simple End-to-End Example"]}, {"block": 10, "type": "markdown", "linesLength": 2, "startIndex": 44, "lines": ["### Get the data path\n", "Let's get some texts to play with this simple end-to-end example."]}, {"block": 11, "type": "code", "linesLength": 4, "startIndex": 46, "lines": ["data_path = tf.keras.utils.get_file(\n", "      fname='aclImdb',\n", "      origin='http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',\n", "      untar=True)"]}, {"block": 12, "type": "markdown", "linesLength": 3, "startIndex": 50, "lines": [" You could replace it with your own text folders. As for uploading data to colab, you could find the upload button in the left sidebar shown in the image below with the red rectangle. Just have a try to upload a zip file and unzip it. The root file path is the current path.\n", "\n", "<img src=\"https://storage.googleapis.com/download.tensorflow.org/models/tflite/screenshots/model_maker_text_classification.png\" alt=\"Upload File\" width=\"800\" hspace=\"100\">\n"]}, {"block": 13, "type": "markdown", "linesLength": 1, "startIndex": 53, "lines": ["If you prefer not to upload your images to the cloud, you could try to run the library locally following the [guide](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker) in github."]}, {"block": 14, "type": "markdown", "linesLength": 3, "startIndex": 54, "lines": ["### Run the example\n", "\n", "The example just consists of 6 lines of code as shown below, representing 5 steps of the overall process."]}, {"block": 15, "type": "markdown", "linesLength": 1, "startIndex": 57, "lines": ["Step 0. Choose a `model_spec` that represents a model for text classifier."]}, {"block": 16, "type": "code", "linesLength": 1, "startIndex": 58, "lines": ["model_spec = AverageWordVecModelSpec()"]}, {"block": 17, "type": "markdown", "linesLength": 1, "startIndex": 59, "lines": ["Step 1.   Load train and test data specific to an on-device ML app and preprocess the data according to specific `model_spec`."]}, {"block": 18, "type": "code", "linesLength": 2, "startIndex": 60, "lines": ["train_data = TextClassifierDataLoader.from_folder(os.path.join(data_path, 'train'), model_spec=model_spec, class_labels=['pos', 'neg'])\n", "test_data = TextClassifierDataLoader.from_folder(os.path.join(data_path, 'test'), model_spec=model_spec, is_training=False, shuffle=False)"]}, {"block": 19, "type": "markdown", "linesLength": 1, "startIndex": 62, "lines": ["Step 2. Customize the TensorFlow model."]}, {"block": 20, "type": "code", "linesLength": 1, "startIndex": 63, "lines": ["model = text_classifier.create(train_data, model_spec=model_spec)"]}, {"block": 21, "type": "markdown", "linesLength": 1, "startIndex": 64, "lines": ["Step 3. Evaluate the model."]}, {"block": 22, "type": "code", "linesLength": 1, "startIndex": 65, "lines": ["loss, acc = model.evaluate(test_data)"]}, {"block": 23, "type": "markdown", "linesLength": 2, "startIndex": 66, "lines": ["Step 4.  Export to TensorFlow Lite  model.\n", "You could download it in the left sidebar same as the uploading part for your own use."]}, {"block": 24, "type": "code", "linesLength": 1, "startIndex": 68, "lines": ["model.export(export_dir='.')"]}, {"block": 25, "type": "markdown", "linesLength": 1, "startIndex": 69, "lines": ["After this simple 5 steps, we could further use TensorFlow Lite model file and label file in on-device applications like in [text classification](https://github.com/tensorflow/examples/tree/master/lite/examples/text_classification) reference app."]}, {"block": 26, "type": "markdown", "linesLength": 3, "startIndex": 70, "lines": ["## Detailed Process\n", "\n", "In the above, we tried the simple end-to-end example. The following walks through the example step by step to show more detail."]}, {"block": 27, "type": "markdown", "linesLength": 3, "startIndex": 73, "lines": ["### Step 0: Choose a model_spec that represents a model for text classifier.\n", "\n", "each `model_spec` object represents a specific model for the text classifier. Currently, we support averging word embedding model and BERT-base model."]}, {"block": 28, "type": "code", "linesLength": 1, "startIndex": 76, "lines": ["model_spec = AverageWordVecModelSpec()"]}, {"block": 29, "type": "markdown", "linesLength": 21, "startIndex": 77, "lines": ["### Step 1: Load Input Data Specific to an On-device ML App\n", "\n", "The IMDB dataset contains 25000 movie reviews for training and 25000 movie reviews for testing from the [Internet Movie Database](https://www.imdb.com/). The dataset has two classes: positive and negative movie reviews.\n", "\n", "Download the archive version of the dataset and untar it.\n", "\n", "The IMDB dataset has the following directory structure:\n", "\n", "<pre>\n", "<b>aclImdb</b>\n", "|__ <b>train</b>\n", "    |______ <b>pos</b>: [1962_10.txt, 2499_10.txt, ...]\n", "    |______ <b>neg</b>: [104_3.txt, 109_2.txt, ...]\n", "    |______ unsup: [12099_0.txt, 1424_0.txt, ...]\n", "|__ <b>test</b>\n", "    |______ <b>pos</b>: [1384_9.txt, 191_9.txt, ...]\n", "    |______ <b>neg</b>: [1629_1.txt, 21_1.txt]\n", "\n", "</pre>\n", "\n", "Note that the text data under `train/unsup` folder are unlabeled documents for unsupervised learning and such data should be ignored in this tutorial.\n"]}, {"block": 30, "type": "code", "linesLength": 4, "startIndex": 98, "lines": ["data_path = tf.keras.utils.get_file(\n", "      fname='aclImdb',\n", "      origin='http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',\n", "      untar=True)"]}, {"block": 31, "type": "markdown", "linesLength": 5, "startIndex": 102, "lines": ["Use `TextClassifierDataLoader` to load data.\n", "\n", "As for `from_folder()` method, it could load data from the folder. It assumes that the text data of the same class are in the same subdirectory and the subfolder name is the class name. Each text file contains one movie review sample.\n", "\n", "Parameter `class_labels` is used to specify which subfolder should be considered. As for `train` folder, this parameter is used to skip `unsup` subfolder.\n"]}, {"block": 32, "type": "code", "linesLength": 3, "startIndex": 107, "lines": ["train_data = TextClassifierDataLoader.from_folder(os.path.join(data_path, 'train'), model_spec=model_spec, class_labels=['pos', 'neg'])\n", "test_data = TextClassifierDataLoader.from_folder(os.path.join(data_path, 'test'), model_spec=model_spec, is_training=False, shuffle=False)\n", "train_data, validation_data = train_data.split(0.9)"]}, {"block": 33, "type": "markdown", "linesLength": 3, "startIndex": 110, "lines": ["### Step 2: Customize the TensorFlow Model\n", "\n", "Create a custom text classifier model based on the loaded data. Currently, we support averaging word embedding and BERT-base model."]}, {"block": 34, "type": "code", "linesLength": 1, "startIndex": 113, "lines": ["model = text_classifier.create(train_data, model_spec=model_spec, validation_data=validation_data)"]}, {"block": 35, "type": "markdown", "linesLength": 1, "startIndex": 114, "lines": ["Have a look at the detailed model structure."]}, {"block": 36, "type": "code", "linesLength": 1, "startIndex": 115, "lines": ["model.summary()"]}, {"block": 37, "type": "markdown", "linesLength": 5, "startIndex": 116, "lines": ["### Step 3: Evaluate the Customized Model\n", "\n", "Evaluate the result of the model, get the loss and accuracy of the model.\n", "\n", "Evaluate the loss and accuracy in `test_data`. If no data is given the results are evaluated on the data that's splitted in the `create` method."]}, {"block": 38, "type": "code", "linesLength": 1, "startIndex": 121, "lines": ["loss, acc = model.evaluate(test_data)"]}, {"block": 39, "type": "markdown", "linesLength": 3, "startIndex": 122, "lines": ["### Step 4: Export to TensorFlow Lite Model\n", "\n", "Convert the existing model to TensorFlow Lite model format that could be later used in on-device ML application. Meanwhile, save the text labels in label file and vocabulary in vocab file. The default TFLite filename is `model.tflite`, the default label filename is `label.txt`, the default vocab filename is `vocab`."]}, {"block": 40, "type": "code", "linesLength": 1, "startIndex": 125, "lines": ["model.export(export_dir='.')"]}, {"block": 41, "type": "markdown", "linesLength": 3, "startIndex": 126, "lines": ["The TensorFlow Lite model file and label file could be used in the [text classification](https://github.com/tensorflow/examples/tree/master/lite/examples/text_classification) reference app.\n", "\n", "In detail, we could add `movie_review_classifier.tflite`, `text_label.txt` and `vocab.txt` to the [assets directory](https://github.com/tensorflow/examples/tree/master/lite/examples/text_classification/android/app/src/main/assets) folder. Meanwhile, change the filenames in [code](https://github.com/tensorflow/examples/blob/master/lite/examples/text_classification/android/app/src/main/java/org/tensorflow/lite/examples/textclassification/TextClassificationClient.java#L43). "]}, {"block": 42, "type": "markdown", "linesLength": 1, "startIndex": 129, "lines": ["Here, we also demonstrate how to use the above files to run and evaluate the TensorFlow Lite model."]}, {"block": 43, "type": "code", "linesLength": 35, "startIndex": 130, "lines": ["# Read TensorFlow Lite model from TensorFlow Lite file.\n", "with tf.io.gfile.GFile('model.tflite', 'rb') as f:\n", "  model_content = f.read()\n", "\n", "# Read label names from label file.\n", "with tf.io.gfile.GFile('labels.txt', 'r') as f:\n", "  label_names = f.read().split('\\n')\n", "\n", "# Initialze TensorFlow Lite inpterpreter.\n", "interpreter = tf.lite.Interpreter(model_content=model_content)\n", "interpreter.allocate_tensors()\n", "input_index = interpreter.get_input_details()[0]['index']\n", "output = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])\n", "\n", "# Run predictions on each test data and calculate accuracy.\n", "accurate_count = 0\n", "for text, label in test_data.dataset:\n", "    # Add batch dimension and convert to float32 to match with the model's input\n", "    # data format.\n", "    text = tf.expand_dims(text, 0)\n", "    text = tf.cast(text, tf.float32)\n", "\n", "    # Run inference.\n", "    interpreter.set_tensor(input_index, text)\n", "    interpreter.invoke()\n", "\n", "    # Post-processing: remove batch dimension and find the label with highest\n", "    # probability.\n", "    predict_label = np.argmax(output()[0])\n", "    # Get label name with label index.\n", "    predict_label_name = label_names[predict_label]\n", "    accurate_count += (predict_label == label.numpy())\n", "\n", "accuracy = accurate_count * 1.0 / test_data.size\n", "print('TensorFlow Lite model accuracy = %.4f' % accuracy)"]}, {"block": 44, "type": "markdown", "linesLength": 1, "startIndex": 165, "lines": ["Note that preprocessing for inference should be the same as training. Currently, preprocessing contains split the text to tokens by '\\W', encode the tokens to ids, the pad the text with `pad_id` to have the length of `seq_length`."]}, {"block": 45, "type": "markdown", "linesLength": 10, "startIndex": 166, "lines": ["# Advanced Usage\n", "\n", "The `create` function is the critical part of this library in which parameter `model_spec` defines the specification of the model, currently `AverageWordVecModelSpec` and `BertModelSpec` is supported. The `create` function contains the following steps for `AverageWordVecModelSpec`:\n", "\n", "1.   Tokenize the text and select the top `num_words` most frequent words to generate the vocubulary. The default value of `num_words` in `AverageWordVecModelSpec` object is `10000`.\n", "2.   Encode the text string tokens to int ids.\n", "3.   Create the text classifier model. Currently, this library supports one model: average the word embedding of the text with RELU activation, then leverage softmax dense layer for classification. As for [Embedding layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding), the input dimension is the size of the vocabulary, the output dimension is `AverageWordVecModelSpec` object's variable `wordvec_dim` which default value is `16`, the input length is `AverageWordVecModelSpec` object's variable `seq_len` which default value is `256`.\n", "4.   Train the classifier model. The default epoch is `2` and the default batch size is `32`.\n", "\n", "In this section, we describe several advanced topics, including adjusting the model, changing the training hyperparameters etc.\n"]}, {"block": 46, "type": "markdown", "linesLength": 3, "startIndex": 176, "lines": ["# Adjust the model\n", "\n", "We could adjust the model infrastructure like variables `wordvec_dim`, `seq_len` in `AverageWordVecModelSpec` class.\n"]}, {"block": 47, "type": "markdown", "linesLength": 4, "startIndex": 179, "lines": ["*   `wordvec_dim`: Dimension of word embedding.\n", "*   `seq_len`: length of sequence.\n", "\n", "For example, we could train with larger `wordvec_dim`. If we change the model, we need to construct the new `model_spec` firstly."]}, {"block": 48, "type": "code", "linesLength": 1, "startIndex": 183, "lines": ["new_model_spec = AverageWordVecModelSpec(wordvec_dim=32)"]}, {"block": 49, "type": "markdown", "linesLength": 1, "startIndex": 184, "lines": ["Secondly, we should get the preprocessed data accordingly."]}, {"block": 50, "type": "code", "linesLength": 2, "startIndex": 185, "lines": ["new_train_data = TextClassifierDataLoader.from_folder(os.path.join(data_path, 'train'), model_spec=new_model_spec, class_labels=['pos', 'neg'])\n", "new_train_data, new_validation_data = new_train_data.split(0.9)"]}, {"block": 51, "type": "markdown", "linesLength": 1, "startIndex": 187, "lines": ["Finally, we could train the new model."]}, {"block": 52, "type": "code", "linesLength": 1, "startIndex": 188, "lines": ["model = text_classifier.create(new_train_data, model_spec=new_model_spec, validation_data=new_validation_data)"]}, {"block": 53, "type": "markdown", "linesLength": 7, "startIndex": 189, "lines": ["## Change the training hyperparameters\n", "We could also change the training hyperparameters like `epochs` and `batch_size` that could affect the model accuracy. For instance,\n", "\n", "*   `epochs`: more epochs could achieve better accuracy, but may lead to overfitting.\n", "*   `batch_size`: number of samples to use in one training step.\n", "\n", "For example, we could train with more epochs."]}, {"block": 54, "type": "code", "linesLength": 1, "startIndex": 196, "lines": ["model = text_classifier.create(train_data, model_spec=model_spec, validation_data=validation_data, epochs=5)"]}, {"block": 55, "type": "markdown", "linesLength": 1, "startIndex": 197, "lines": ["Evaluate the newly retrained model with 5 training epochs."]}, {"block": 56, "type": "code", "linesLength": 1, "startIndex": 198, "lines": ["loss, accuracy = model.evaluate(test_data)"]}, {"block": 57, "type": "markdown", "linesLength": 5, "startIndex": 199, "lines": ["## Change the Model\n", "\n", "We could change the model by changing the `model_spec`. The following shows how we change to BERT-base model.\n", "\n", "First, we could change `model_spec` to `BertModelSpec`."]}, {"block": 58, "type": "code", "linesLength": 1, "startIndex": 204, "lines": ["model_spec = BertClassifierModelSpec()"]}, {"block": 59, "type": "markdown", "linesLength": 3, "startIndex": 205, "lines": ["The remaining steps remains the same.\n", "\n", "Load data and preprocess the data according to `model_spec`."]}, {"block": 60, "type": "code", "linesLength": 2, "startIndex": 208, "lines": ["train_data = TextClassifierDataLoader.from_folder(os.path.join(data_path, 'train'), model_spec=model_spec, class_labels=['pos', 'neg'])\n", "test_data = TextClassifierDataLoader.from_folder(os.path.join(data_path, 'test'), model_spec=model_spec, is_training=False, shuffle=False)"]}, {"block": 61, "type": "markdown", "linesLength": 1, "startIndex": 210, "lines": ["Then retrain the model. Note that it could take a long time to retrain the BERT model. we just set `epochs` equals 1 to demonstrate it."]}, {"block": 62, "type": "code", "linesLength": 1, "startIndex": 211, "lines": ["model = text_classifier.create(train_data, model_spec=model_spec, epochs=1)"]}]