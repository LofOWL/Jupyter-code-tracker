[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["# All the Linear Algebra You Need for AI"]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 1, "lines": ["The purpose of this notebook is to serve as an explanation of two crucial linear algebra operations used when coding neural networks: matrix multiplication and broadcasting."]}, {"block": 2, "type": "markdown", "linesLength": 1, "startIndex": 2, "lines": ["## Introduction"]}, {"block": 3, "type": "markdown", "linesLength": 1, "startIndex": 3, "lines": ["**Matrix multiplication** is a way of combining two matrices (involving multiplying and summing their entries in a particular way).  **Broadcasting** refers to how libraries such as Numpy and PyTorch can perform operations on matrices/vectors with mismatched dimensions (in particular cases, with set rules).  We will use broadcasting to show an alternative way of thinking about matrix multiplication from, different from the way it is standardly taught."]}, {"block": 4, "type": "markdown", "linesLength": 12, "startIndex": 4, "lines": ["In keeping with the [fast.ai teaching philosophy](http://www.fast.ai/2016/10/08/teaching-philosophy/) of [\"the whole game\"](https://www.amazon.com/Making-Learning-Whole-Principles-Transform/dp/0470633719/ref=sr_1_1?ie=UTF8&qid=1505094653), we will:\n", "\n", "- first use a pre-defined class for our neural network\n", "- then define the net ourselves to see where it uses matrix multiplication & broadcasting\n", "- and finally dig into the details of how those operations work\n", "\n", "This is different from how most math courses are taught, where you have to learn all the individual elements before you can combine them (Harvard professor David Perkins call this *elementitis*), but it is similar to how topics like *driving* and *baseball* are taught.  That is, you can start driving without [knowing how an internal combustion engine works](https://medium.com/towards-data-science/thoughts-after-taking-the-deeplearning-ai-courses-8568f132153), and children begin playing baseball before they learn all the formal rules.\n", "\n", "<img src=\"images/demba_combustion_engine.png\" alt=\"\" style=\"width: 50%\"/>\n", "<center>\n", "(source: [Demba Ba](https://github.com/zalandoresearch/fashion-mnist) and [Arvind Nagaraj](https://medium.com/towards-data-science/thoughts-after-taking-the-deeplearning-ai-courses-8568f132153))\n", "</center>"]}, {"block": 5, "type": "markdown", "linesLength": 1, "startIndex": 16, "lines": ["### More linear algebra resources"]}, {"block": 6, "type": "markdown", "linesLength": 7, "startIndex": 17, "lines": ["This notebook was originally created for a 40 minute talk I gave at the [O'Reilly AI conference in San Francisco](https://conferences.oreilly.com/artificial-intelligence/ai-ca).  If you want further resources for linear algebra, here are a few recommendations:\n", "\n", "- [3Blue1Brown Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) videos about *geometric intuition* (fantastic! gorgeous!)\n", "- [Khan Academy Linear Algebra](https://www.khanacademy.org/math/linear-algebra)\n", "- [Immersive linear algebra](http://immersivemath.com/ila/) free online textbook with interactive graphics\n", "- [Chapter 2](http://www.deeplearningbook.org/contents/linear_algebra.html) of Ian Goodfellow's Deep Learning Book\n", "- [Computational Linear Algebra](http://www.fast.ai/2017/07/17/num-lin-alg/): a free, online fast.ai course, originally taught in the University of San Francisco's Masters in Analytics program. It includes a free [online textbook](https://github.com/fastai/numerical-linear-algebra/blob/master/README.md) and [series of videos](https://www.youtube.com/playlist?list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY). This course is very different from standard linear algebra (which often focuses on how **humans** do matrix calculations), because it is about how to get **computers** to do matrix computations with speed and accuracy, and incorporates modern tools and algorithms.  All the material is taught in Python and centered around solving practical problems such as removing the background from a surveillance video or implementing Google's PageRank search algorithm on Wikipedia pages."]}, {"block": 7, "type": "markdown", "linesLength": 1, "startIndex": 24, "lines": ["## Our Tools"]}, {"block": 8, "type": "markdown", "linesLength": 1, "startIndex": 25, "lines": ["We will be using the open source fast.ai [deep learning library, fastai](https://github.com/fastai/fastai), which provides high level abstractions and best practices on top of PyTorch."]}, {"block": 9, "type": "code", "linesLength": 2, "startIndex": 26, "lines": ["import sys\n", "sys.path.insert(0, '../')"]}, {"block": 10, "type": "code", "linesLength": 2, "startIndex": 28, "lines": ["%load_ext autoreload\n", "%autoreload 2"]}, {"block": 11, "type": "code", "linesLength": 3, "startIndex": 30, "lines": ["from fastai.imports import *\n", "from fastai.torch_imports import *\n", "from fastai.io import *"]}, {"block": 12, "type": "markdown", "linesLength": 1, "startIndex": 33, "lines": ["### PyTorch"]}, {"block": 13, "type": "markdown", "linesLength": 11, "startIndex": 34, "lines": ["The fastai deep learning library uses [PyTorch](http://pytorch.org/), a Python framework for tensors and dynamic neural networks with GPU acceleration, which was released by Facebook's AI team.  \n", "\n", "From the [PyTorch documentation](http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html):\n", "\n", "<img src=\"images/what_is_pytorch.png\" alt=\"pytorch\" style=\"width: 80%\"/>\n", "\n", "**Further learning**: If you are curious to learn what *dynamic* neural networks are, you may want to watch [this talk](https://www.youtube.com/watch?v=Z15cBAuY7Sc) by Soumith Chintala, Facebook AI researcher and core PyTorch contributor.\n", "\n", "If you want to learn more PyTorch, you can try this [tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) or this [learning by examples](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html).\n", "\n", "**Note about GPUs**: If you are not using a GPU, you will need to remove the `.cuda()` from the methods below. GPU usage is not required for this tutorial, but I thought it would be of interest to some of you.  To learn how to create an AWS instance with a GPU, you can watch the [fast.ai setup lesson](http://course.fast.ai/lessons/aws.html).]"]}, {"block": 14, "type": "markdown", "linesLength": 1, "startIndex": 45, "lines": ["## Data"]}, {"block": 15, "type": "markdown", "linesLength": 5, "startIndex": 46, "lines": ["A matrix can represent an image, by creating a grid where each entry corresponds to a different pixel.\n", "\n", "<img src=\"images/digit.gif\" alt=\"digit\" style=\"width: 55%\"/>\n", "  (Source: [Adam Geitgey\n", "](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721))\n"]}, {"block": 16, "type": "markdown", "linesLength": 1, "startIndex": 51, "lines": ["Today we will be working with MNIST, a classic data set of hand-written digits.  Solutions to this problem are used by banks to automatically recognize the amounts on checks, and by the postal service to automatically recognize zip codes on mail."]}, {"block": 17, "type": "markdown", "linesLength": 1, "startIndex": 52, "lines": ["<img src=\"images/mnist.png\" alt=\"\" style=\"width: 60%\"/>"]}, {"block": 18, "type": "markdown", "linesLength": 6, "startIndex": 53, "lines": ["Next, we will look at **CIFAR 10**, a dataset that consists of 32x32 *color* images in 10 different categories.  Color images have an extra dimension, containing RGB values, compared to black & white images.\n", "\n", "<img src=\"images/cifar10.png\" alt=\"\" style=\"width: 70%\"/>\n", "<center>\n", "(source: [Cifar 10](https://www.cs.toronto.edu/~kriz/cifar.html))\n", "</center>"]}, {"block": 19, "type": "markdown", "linesLength": 1, "startIndex": 59, "lines": ["### Download"]}, {"block": 20, "type": "markdown", "linesLength": 1, "startIndex": 60, "lines": ["Let's download, unzip, and format the data."]}, {"block": 21, "type": "code", "linesLength": 1, "startIndex": 61, "lines": ["path = '../data/'"]}, {"block": 22, "type": "code", "linesLength": 2, "startIndex": 62, "lines": ["import os\n", "os.makedirs(path, exist_ok=True)"]}, {"block": 23, "type": "code", "linesLength": 5, "startIndex": 64, "lines": ["URL='http://deeplearning.net/data/mnist/'\n", "FILENAME='mnist.pkl.gz'\n", "\n", "def load_mnist(filename):\n", "    return pickle.load(gzip.open(filename, 'rb'), encoding='latin-1')"]}, {"block": 24, "type": "code", "linesLength": 2, "startIndex": 69, "lines": ["get_data(URL+FILENAME, path+FILENAME)\n", "((x, y), (x_valid, y_valid), (x_test, y_test)) = load_mnist(path+FILENAME)"]}, {"block": 25, "type": "markdown", "linesLength": 1, "startIndex": 71, "lines": ["### Normalize"]}, {"block": 26, "type": "markdown", "linesLength": 1, "startIndex": 72, "lines": ["One of the challenges in training neural networks is keeping your numbers from exploding (going to infinity) or vanishing (going to zero).  There are several different ways to add normalization to address this.  We will subtract off the mean and standard deviation from our training set:"]}, {"block": 27, "type": "code", "linesLength": 2, "startIndex": 73, "lines": ["mean = x.mean()\n", "std = x.std()"]}, {"block": 28, "type": "code", "linesLength": 2, "startIndex": 75, "lines": ["x=(x-mean)/std\n", "x.mean(), x.std()"]}, {"block": 29, "type": "markdown", "linesLength": 1, "startIndex": 77, "lines": ["Note that for consistency (with the parameters we learn when training), we subtract the mean and standard deviation of our training set from our validation set. "]}, {"block": 30, "type": "code", "linesLength": 2, "startIndex": 78, "lines": ["x_valid = (x_valid-mean)/std\n", "x_valid.mean(), x_valid.std()"]}, {"block": 31, "type": "markdown", "linesLength": 1, "startIndex": 80, "lines": ["### Look at the data"]}, {"block": 32, "type": "markdown", "linesLength": 1, "startIndex": 81, "lines": ["#### Helper methods"]}, {"block": 33, "type": "code", "linesLength": 7, "startIndex": 82, "lines": ["%matplotlib inline\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "def show(img, title=None):\n", "    plt.imshow(img, interpolation='none', cmap=\"gray\")\n", "    if title is not None: plt.title(title)"]}, {"block": 34, "type": "code", "linesLength": 8, "startIndex": 89, "lines": ["def plots(ims, figsize=(12,6), rows=2, titles=None):\n", "    f = plt.figure(figsize=figsize)\n", "    cols = len(ims)//rows\n", "    for i in range(len(ims)):\n", "        sp = f.add_subplot(rows, cols, i+1)\n", "        sp.axis('Off')\n", "        if titles is not None: sp.set_title(titles[i], fontsize=16)\n", "        plt.imshow(ims[i], interpolation='none', cmap='gray')"]}, {"block": 35, "type": "markdown", "linesLength": 1, "startIndex": 97, "lines": ["In any sort of data science work, it's important to look at your data, to make sure you understand the format, how it's stored, what type of values it holds, etc. To make it easier to work with, let's reshape it into 2d images from the flattened 1d format."]}, {"block": 36, "type": "markdown", "linesLength": 1, "startIndex": 98, "lines": ["#### Helper methods"]}, {"block": 37, "type": "code", "linesLength": 7, "startIndex": 99, "lines": ["%matplotlib inline\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "def show(img, title=None):\n", "    plt.imshow(img, interpolation='none', cmap=\"gray\")\n", "    if title is not None: plt.title(title)"]}, {"block": 38, "type": "code", "linesLength": 8, "startIndex": 106, "lines": ["def plots(ims, figsize=(12,6), rows=2, titles=None):\n", "    f = plt.figure(figsize=figsize)\n", "    cols = len(ims)//rows\n", "    for i in range(len(ims)):\n", "        sp = f.add_subplot(rows, cols, i+1)\n", "        sp.axis('Off')\n", "        if titles is not None: sp.set_title(titles[i], fontsize=16)\n", "        plt.imshow(ims[i], interpolation='none', cmap='gray')"]}, {"block": 39, "type": "markdown", "linesLength": 1, "startIndex": 114, "lines": ["#### Plots "]}, {"block": 40, "type": "code", "linesLength": 1, "startIndex": 115, "lines": ["x_imgs = np.reshape(x_valid, (-1,28,28)); x_imgs.shape"]}, {"block": 41, "type": "markdown", "linesLength": 1, "startIndex": 116, "lines": ["We can look at part of an image:"]}, {"block": 42, "type": "code", "linesLength": 1, "startIndex": 117, "lines": ["x_imgs[0,10:15,10:15]"]}, {"block": 43, "type": "code", "linesLength": 1, "startIndex": 118, "lines": ["show(x_imgs[0], y[0])"]}, {"block": 44, "type": "markdown", "linesLength": 1, "startIndex": 119, "lines": ["It's the digit 5!  And that's stored in the y value:"]}, {"block": 45, "type": "code", "linesLength": 1, "startIndex": 120, "lines": ["y[0]"]}, {"block": 46, "type": "code", "linesLength": 1, "startIndex": 121, "lines": ["show(x_imgs[0,10:15,10:15])"]}, {"block": 47, "type": "code", "linesLength": 1, "startIndex": 122, "lines": ["plots(x_imgs[:8], titles=y[:8])"]}, {"block": 48, "type": "markdown", "linesLength": 1, "startIndex": 123, "lines": ["## Neural Net (with nn.torch)"]}, {"block": 49, "type": "markdown", "linesLength": 3, "startIndex": 124, "lines": ["We will begin with the highest level abstraction: using a neural net defined by PyTorch's Sequential class (Keras has a similar Sequential class).  \n", "\n", "We will use fastai's ImageClassifierData, which holds our training and validation sets and will provide batches of that data in a form ready for use by a PyTorch model."]}, {"block": 50, "type": "code", "linesLength": 6, "startIndex": 127, "lines": ["from fastai.metrics import *\n", "from fastai.model import *\n", "from fastai.dataset import *\n", "from fastai.core import *\n", "\n", "import torch.nn as nn"]}, {"block": 51, "type": "code", "linesLength": 1, "startIndex": 133, "lines": ["md = ImageClassifierData.from_arrays(path, (x,y), (x_valid, y_valid))"]}, {"block": 52, "type": "markdown", "linesLength": 1, "startIndex": 134, "lines": ["Neural networks consist of **linear layers alternating with non-linear layers**.  This creates functions which are incredibly flexible."]}, {"block": 53, "type": "code", "linesLength": 5, "startIndex": 135, "lines": ["net = nn.Sequential(\n", "    nn.Linear(28*28, 256),\n", "    nn.ReLU(),\n", "    nn.Linear(256, 10)\n", ").cuda()"]}, {"block": 54, "type": "markdown", "linesLength": 3, "startIndex": 140, "lines": ["Each input is a vector of size $28\\times 28$ pixels and our output is of size $10$ (since there are 10 digits: 0, 1, ..., 9).\n", "\n", "I just chose $256$ as the number of hidden states, you could change this to something else."]}, {"block": 55, "type": "markdown", "linesLength": 4, "startIndex": 143, "lines": ["Next we will set a few inputs for our *fit* method:\n", "- **Loss**: what function is the optimizer trying to minimize?  We need to say how we're defining the error.\n", "- **Optimizer**: algorithm for finding the minimum. typically these are variations on *stochastic gradient descent*, involve taking a step that appears to be the right direction based on the gradient\n", "- **Metrics**: other calculations you want printed out as you train"]}, {"block": 56, "type": "code", "linesLength": 3, "startIndex": 147, "lines": ["loss=F.cross_entropy\n", "metrics=[accuracy]\n", "opt=optim.Adam(net.parameters())"]}, {"block": 57, "type": "markdown", "linesLength": 1, "startIndex": 150, "lines": ["*Fitting* is the process by which the neural net learns the best parameters for the dataset."]}, {"block": 58, "type": "code", "linesLength": 1, "startIndex": 151, "lines": ["fit(net, md, epochs=1, crit=loss, opt=opt, metrics=metrics)"]}, {"block": 59, "type": "markdown", "linesLength": 1, "startIndex": 152, "lines": ["Now that we have the parameters for our model, we can make predictions on our validation set."]}, {"block": 60, "type": "code", "linesLength": 1, "startIndex": 153, "lines": ["preds = predict(net, md.val_dl)"]}, {"block": 61, "type": "code", "linesLength": 1, "startIndex": 154, "lines": ["preds = preds.max(1)[1]"]}, {"block": 62, "type": "markdown", "linesLength": 1, "startIndex": 155, "lines": ["Let's see how some of our preditions look!"]}, {"block": 63, "type": "code", "linesLength": 1, "startIndex": 156, "lines": ["plots(x_imgs[:8], titles=preds[:8])"]}, {"block": 64, "type": "markdown", "linesLength": 1, "startIndex": 157, "lines": ["These predictions are pretty good! Note that the image in the 2nd column of the 2nd row appears to be a 4, yet is labeled 9.  Although incorrect, the way the 4 is drawn does look quite similar to a 9."]}, {"block": 65, "type": "markdown", "linesLength": 1, "startIndex": 158, "lines": ["## Coding the Neural Net ourselves"]}, {"block": 66, "type": "markdown", "linesLength": 1, "startIndex": 159, "lines": ["Now, instead of using PyTorch's `Sequential`, we will define the neural network ourselves.  This will allow us to see exactly where matrix multiplication is used.  Just as Numpy has `np.matmul` for matrix multiplication (in Python 3, this is equivalent to the `@` operator), PyTorch has `torch.matmul`.  The method `forward` describes how the neural net converts inputs to outputs."]}, {"block": 67, "type": "code", "linesLength": 14, "startIndex": 160, "lines": ["class SimpleMnist(nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.l1_w = get_weights(28*28, 256)  # Layer 1 weights\n", "        self.l1_b = get_weights(256)         # Layer 1 bias\n", "        self.l2_w = get_weights(256, 10)     # Layer 2 weights\n", "        self.l2_b = get_weights(10)          # Layer 2 bias\n", "\n", "    def forward(self, x):\n", "        x = x.view(x.size(0), -1)\n", "        x = torch.matmul(x, self.l1_w) + self.l1_b\n", "        x = x * (x > 0).float()\n", "        x = torch.matmul(x, self.l2_w) + self.l2_b\n", "        return x"]}, {"block": 68, "type": "code", "linesLength": 2, "startIndex": 174, "lines": ["from torch.autograd import Variable\n", "def get_weights(*dims): return nn.Parameter(torch.randn(*dims)/dims[0])"]}, {"block": 69, "type": "markdown", "linesLength": 1, "startIndex": 176, "lines": ["We create our neural net and the optimizer.  (We will use the same loss and metrics from above)."]}, {"block": 70, "type": "code", "linesLength": 2, "startIndex": 177, "lines": ["net2 = SimpleMnist().cuda()\n", "opt=optim.Adam(net2.parameters())"]}, {"block": 71, "type": "code", "linesLength": 1, "startIndex": 179, "lines": ["fit(net2, md, epochs=1, crit=loss, opt=opt, metrics=metrics)"]}, {"block": 72, "type": "markdown", "linesLength": 1, "startIndex": 180, "lines": ["Now we can check our predictions:"]}, {"block": 73, "type": "code", "linesLength": 2, "startIndex": 181, "lines": ["preds = predict(net2, md.val_dl).max(1)[1]\n", "plots(x_imgs[:8], titles=preds[:8])"]}, {"block": 74, "type": "markdown", "linesLength": 1, "startIndex": 183, "lines": ["## what torch.matmul (matrix multiplication) is doing"]}, {"block": 75, "type": "markdown", "linesLength": 1, "startIndex": 184, "lines": ["Now let's dig in to what we were doing with `torch.matmul`: matrix multiplication.  First, let's start with a simpler building block: **broadcasting**."]}, {"block": 76, "type": "markdown", "linesLength": 1, "startIndex": 185, "lines": ["### Element-wise operations "]}, {"block": 77, "type": "markdown", "linesLength": 5, "startIndex": 186, "lines": ["Broadcasting and element-wise operations are supported in the same way by both numpy and pytorch.\n", "\n", "Operators (+,-,\\*,/,>,<,==) are usually element-wise.\n", "\n", "Examples of element-wise operations:"]}, {"block": 78, "type": "code", "linesLength": 2, "startIndex": 191, "lines": ["a = np.array([10, 6, -4])\n", "b = np.array([2, 8, 7])"]}, {"block": 79, "type": "code", "linesLength": 1, "startIndex": 193, "lines": ["a + b"]}, {"block": 80, "type": "code", "linesLength": 1, "startIndex": 194, "lines": ["a < b"]}, {"block": 81, "type": "markdown", "linesLength": 1, "startIndex": 195, "lines": ["### Broadcasting"]}, {"block": 82, "type": "markdown", "linesLength": 8, "startIndex": 196, "lines": ["The term **broadcasting** describes how arrays with different shapes are treated during arithmetic operations.  The term broadcasting was first used by Numpy, although is now used in other libraries such as [Tensorflow](https://www.tensorflow.org/performance/xla/broadcasting) and Matlab; the rules can vary by library.\n", "\n", "From the [Numpy Documentation](https://docs.scipy.org/doc/numpy-1.10.0/user/basics.broadcasting.html):\n", "\n", "    Broadcasting provides a means of vectorizing array operations so \n", "    that looping occurs in C instead of Python. It does this without \n", "    making needless copies of data and usually leads to efficient \n", "    algorithm implementations."]}, {"block": 83, "type": "markdown", "linesLength": 1, "startIndex": 204, "lines": ["This section was adapted from [Chapter 4](http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/4.%20Compressed%20Sensing%20of%20CT%20Scans%20with%20Robust%20Regression.ipynb#4.-Compressed-Sensing-of-CT-Scans-with-Robust-Regression) of my [Computational Linear Algebra](https://github.com/fastai/numerical-linear-algebra) course."]}, {"block": 84, "type": "markdown", "linesLength": 1, "startIndex": 205, "lines": ["#### Broadcasting with a scalar"]}, {"block": 85, "type": "code", "linesLength": 1, "startIndex": 206, "lines": ["a > 0"]}, {"block": 86, "type": "markdown", "linesLength": 3, "startIndex": 207, "lines": ["How are we able to do a > 0?  0 is being **broadcast** to have the same dimensions as a.\n", "\n", "Other examples of broadcasting with a scalar (as we did when we normalized our data):"]}, {"block": 87, "type": "code", "linesLength": 1, "startIndex": 210, "lines": ["a + 1"]}, {"block": 88, "type": "code", "linesLength": 1, "startIndex": 211, "lines": ["m = np.array([[1, 2, 3], [4,5,6], [7,8,9]]); m"]}, {"block": 89, "type": "code", "linesLength": 1, "startIndex": 212, "lines": ["m * 2"]}, {"block": 90, "type": "markdown", "linesLength": 1, "startIndex": 213, "lines": ["#### Broadcasting a vector to a matrix"]}, {"block": 91, "type": "markdown", "linesLength": 1, "startIndex": 214, "lines": ["We can also broadcast a vector to a matrix:"]}, {"block": 92, "type": "code", "linesLength": 1, "startIndex": 215, "lines": ["c = np.array([10,20,30]); c"]}, {"block": 93, "type": "code", "linesLength": 1, "startIndex": 216, "lines": ["m + c"]}, {"block": 94, "type": "markdown", "linesLength": 1, "startIndex": 217, "lines": ["Although numpy does this automatically, you can also use the `broadcast_to` method:"]}, {"block": 95, "type": "code", "linesLength": 1, "startIndex": 218, "lines": ["np.broadcast_to(c, (3,3))"]}, {"block": 96, "type": "code", "linesLength": 1, "startIndex": 219, "lines": ["c.shape"]}, {"block": 97, "type": "markdown", "linesLength": 1, "startIndex": 220, "lines": ["The numpy `expand_dims` method lets us convert the 1-dimensional array `c` into a 2-dimensional array (although one of those dimensions has value 1)."]}, {"block": 98, "type": "code", "linesLength": 1, "startIndex": 221, "lines": ["np.expand_dims(c,0).shape"]}, {"block": 99, "type": "code", "linesLength": 1, "startIndex": 222, "lines": ["m + np.expand_dims(c,0)"]}, {"block": 100, "type": "code", "linesLength": 1, "startIndex": 223, "lines": ["np.expand_dims(c,1).shape"]}, {"block": 101, "type": "code", "linesLength": 1, "startIndex": 224, "lines": ["m + np.expand_dims(c,1)"]}, {"block": 102, "type": "code", "linesLength": 1, "startIndex": 225, "lines": ["np.broadcast_to(np.expand_dims(c,1), (3,3))"]}, {"block": 103, "type": "markdown", "linesLength": 1, "startIndex": 226, "lines": ["#### Broadcasting Rules"]}, {"block": 104, "type": "markdown", "linesLength": 4, "startIndex": 227, "lines": ["When operating on two arrays, Numpy/PyTorch compares their shapes element-wise. It starts with the **trailing dimensions**, and works its way forward. Two dimensions are **compatible** when\n", "\n", "- they are equal, or\n", "- one of them is 1"]}, {"block": 105, "type": "markdown", "linesLength": 5, "startIndex": 231, "lines": ["Arrays do not need to have the same number of dimensions. For example, if you have a $256 \\times 256 \\times 3$ array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:\n", "\n", "    Image  (3d array): 256 x 256 x 3\n", "    Scale  (1d array):             3\n", "    Result (3d array): 256 x 256 x 3"]}, {"block": 106, "type": "markdown", "linesLength": 1, "startIndex": 236, "lines": ["### Matrix Multiplication"]}, {"block": 107, "type": "markdown", "linesLength": 1, "startIndex": 237, "lines": ["We are going to use broadcasting to define matrix multiplication."]}, {"block": 108, "type": "markdown", "linesLength": 1, "startIndex": 238, "lines": ["#### Matrix-Vector Multiplication"]}, {"block": 109, "type": "code", "linesLength": 1, "startIndex": 239, "lines": ["m, c"]}, {"block": 110, "type": "code", "linesLength": 1, "startIndex": 240, "lines": ["m @ c  # np.matmul(m, c)"]}, {"block": 111, "type": "markdown", "linesLength": 1, "startIndex": 241, "lines": ["We get the same answer using `torch.matmul`:"]}, {"block": 112, "type": "code", "linesLength": 1, "startIndex": 242, "lines": ["torch.matmul(torch.from_numpy(m), torch.from_numpy(c))"]}, {"block": 113, "type": "markdown", "linesLength": 1, "startIndex": 243, "lines": ["The following is **NOT** matrix multiplication.  What is it?"]}, {"block": 114, "type": "code", "linesLength": 1, "startIndex": 244, "lines": ["m * c"]}, {"block": 115, "type": "code", "linesLength": 1, "startIndex": 245, "lines": ["(m * c).sum(axis=1)"]}, {"block": 116, "type": "code", "linesLength": 1, "startIndex": 246, "lines": ["c"]}, {"block": 117, "type": "code", "linesLength": 1, "startIndex": 247, "lines": ["np.broadcast_to(c, (3,3))"]}, {"block": 118, "type": "markdown", "linesLength": 1, "startIndex": 248, "lines": ["From a machine learning perspective, matrix multiplication is a way of creating features by saying how much we want to weight each input column.  **Different features are different weighted averages of the input columns**. "]}, {"block": 119, "type": "markdown", "linesLength": 1, "startIndex": 249, "lines": ["The website [matrixmultiplication.xyz](http://matrixmultiplication.xyz/) provides a nice visualization of matrix multiplcation"]}, {"block": 120, "type": "markdown", "linesLength": 1, "startIndex": 250, "lines": ["Draw a picture"]}, {"block": 121, "type": "code", "linesLength": 1, "startIndex": 251, "lines": ["d = np.array([30,20,10])"]}, {"block": 122, "type": "code", "linesLength": 1, "startIndex": 252, "lines": ["nn = np.stack([c, d], axis=1); nn"]}, {"block": 123, "type": "code", "linesLength": 1, "startIndex": 253, "lines": ["n = np.array([[10,30],[20,20],[30,10]])"]}, {"block": 124, "type": "code", "linesLength": 1, "startIndex": 254, "lines": ["m @ n"]}, {"block": 125, "type": "code", "linesLength": 1, "startIndex": 255, "lines": ["(m * c).sum(axis=1)"]}, {"block": 126, "type": "code", "linesLength": 1, "startIndex": 256, "lines": ["(m * d).sum(axis=1)"]}, {"block": 127, "type": "markdown", "linesLength": 1, "startIndex": 257, "lines": ["## Other applications of Matrix and Tensor Products"]}, {"block": 128, "type": "markdown", "linesLength": 1, "startIndex": 258, "lines": ["Here are some other examples of where matrix multiplication arises.  This material is taken from [Chapter 1](http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb) of my [Computational Linear Algebra](https://github.com/fastai/numerical-linear-algebra) course. "]}, {"block": 129, "type": "markdown", "linesLength": 1, "startIndex": 259, "lines": ["#### Matrix-Vector Products:"]}, {"block": 130, "type": "markdown", "linesLength": 7, "startIndex": 260, "lines": ["The matrix below gives the probabilities of moving from 1 health state to another in 1 year.  If the current health states for a group are:\n", "- 85% asymptomatic\n", "- 10% symptomatic\n", "- 5% AIDS\n", "- 0% death\n", "\n", "what will be the % in each health state in 1 year?"]}, {"block": 131, "type": "markdown", "linesLength": 1, "startIndex": 267, "lines": ["<img src=\"images/markov_health.jpg\" alt=\"floating point\" style=\"width: 80%\"/>(Source: [Concepts of Markov Chains](https://www.youtube.com/watch?v=0Il-y_WLTo4))"]}, {"block": 132, "type": "markdown", "linesLength": 1, "startIndex": 268, "lines": ["#### Answer"]}, {"block": 133, "type": "code", "linesLength": 1, "startIndex": 269, "lines": ["import numpy as np"]}, {"block": 134, "type": "code", "linesLength": 1, "startIndex": 270, "lines": ["#Exercise: Use Numpy to compute the answer to the above\n"]}, {"block": 135, "type": "markdown", "linesLength": 1, "startIndex": 271, "lines": ["#### Matrix-Matrix Products"]}, {"block": 136, "type": "markdown", "linesLength": 1, "startIndex": 272, "lines": ["<img src=\"images/shop.png\" alt=\"floating point\" style=\"width: 100%\"/>(Source: [Several Simple Real-world Applications of Linear Algebra Tools](https://www.mff.cuni.cz/veda/konference/wds/proc/pdf06/WDS06_106_m8_Ulrychova.pdf))"]}, {"block": 137, "type": "markdown", "linesLength": 1, "startIndex": 273, "lines": ["#### Answer"]}, {"block": 138, "type": "code", "linesLength": 1, "startIndex": 274, "lines": ["#Exercise: Use Numpy to compute the answer to the above\n"]}, {"block": 139, "type": "markdown", "linesLength": 1, "startIndex": 275, "lines": ["## End"]}, {"block": 140, "type": "code", "linesLength": 0, "startIndex": 276, "lines": []}, {"block": 141, "type": "markdown", "linesLength": 1, "startIndex": 276, "lines": ["A Tensor is a *multi-dimensional matrix containing elements of a single data type*: a group of data, all with the same type (e.g. A Tensor could store a 4 x 4 x 6 matrix of 32-bit signed integers)."]}]