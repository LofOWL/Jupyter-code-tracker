[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["# Image classification with Convolutional Neural Networks"]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 1, "lines": ["Welcome to the first week of the second deep learning certificate! We're going to use convolutional neural networks (CNNs) to allow our computer to see - something that is only possible thanks to deep learning."]}, {"block": 2, "type": "code", "linesLength": 4, "startIndex": 2, "lines": ["# Put these at the top of every notebook, to get automatic reloading and inline plotting\n", "%reload_ext autoreload\n", "%autoreload 2\n", "%matplotlib inline"]}, {"block": 3, "type": "code", "linesLength": 2, "startIndex": 6, "lines": ["# This file contains all the main external libs we'll use\n", "from fastai.imports import *"]}, {"block": 4, "type": "markdown", "linesLength": 1, "startIndex": 8, "lines": ["## Single versus multi label classification"]}, {"block": 5, "type": "code", "linesLength": 7, "startIndex": 9, "lines": ["def plot_imgs(img_list, title_list, title):\n", "    plt.figure(1)\n", "    plt.suptitle(title, size=16)\n", "    for i in range(len(img_list)):\n", "        plt.subplot(1,2,i+1)\n", "        plt.imshow(img_list[i])\n", "        plt.title(title_list[i])"]}, {"block": 6, "type": "code", "linesLength": 6, "startIndex": 16, "lines": ["path = \"/data/yinterian/dogscats/\"\n", "files = !ls {path}valid/cats | head\n", "img_cat = plt.imread(f'{path}valid/cats/{files[0]}')\n", "files = !ls {path}valid/dogs | head\n", "img_dog = plt.imread(f'{path}valid/dogs/{files[0]}')\n", "plot_imgs([img_cat, img_dog], [\"cat\", \"dog\"], \"This is single label classification\")"]}, {"block": 7, "type": "markdown", "linesLength": 1, "startIndex": 22, "lines": ["In <b>single label classification</b> each sample has a single target value. In this example is either \"cat\" or \"dog.\""]}, {"block": 8, "type": "code", "linesLength": 5, "startIndex": 23, "lines": ["# two examples from planet competition\n", "path = \"/data/jhoward/fast/planet/\"\n", "train_csv_list = list(open(f'{path}train_v2.csv'))\n", "train_0, label_0 = train_csv_list[1].strip().split(\",\")\n", "train_1, label_1 = train_csv_list[2].strip().split(\",\")"]}, {"block": 9, "type": "code", "linesLength": 3, "startIndex": 28, "lines": ["img_0 = plt.imread(f'{path}train-jpg/{train_0}.jpg')\n", "img_1 = plt.imread(f'{path}train-jpg/{train_1}.jpg')\n", "plot_imgs([img_0, img_1], [label_0, label_1], \"This is multi label classification\")"]}, {"block": 10, "type": "markdown", "linesLength": 1, "startIndex": 31, "lines": ["In <b>multi label classification</b> each sample can have a multiple target values. In this example the first image has 2 labels: \"haze\" and \"primary\", the second image has 4 labels: \"agriculture\", \"clear\", \"primary\" and \"water.\""]}, {"block": 11, "type": "markdown", "linesLength": 1, "startIndex": 32, "lines": ["## Introduction to our first task: 'Dogs vs Cats'"]}, {"block": 12, "type": "markdown", "linesLength": 3, "startIndex": 33, "lines": ["We're going to try to create a model to enter the Dogs vs Cats competition at Kaggle. There are 25,000 labelled dog and cat photos available for training, and 12,500 in the test set that we have to try to label for this competition. According to the Kaggle web-site, when this competition was launched (end of 2013): \"State of the art: The current literature suggests machine classifiers can score above 80% accuracy on this task\". So if we can beat 80%, then we will be at the cutting edge as of 2013!\n", "\n", "Here is the dataset http://files.fast.ai/data/dogscats.zip. You can download it directly on your server by running the following line in your terminal. \"wget http://files.fast.ai/data/dogscats.zip\""]}, {"block": 13, "type": "code", "linesLength": 2, "startIndex": 36, "lines": ["PATH = \"data/dogscats/\"\n", "#PATH = \"/data/yinterian/dogscats/\""]}, {"block": 14, "type": "markdown", "linesLength": 1, "startIndex": 38, "lines": ["### First look at cat pictures"]}, {"block": 15, "type": "markdown", "linesLength": 1, "startIndex": 39, "lines": ["Our library will assume that you have *train* and *valid* directories. It also assumes that each dir will have subdirs for each class you wish to recognize (in this case, 'cats' and 'dogs')."]}, {"block": 16, "type": "code", "linesLength": 1, "startIndex": 40, "lines": ["!ls {PATH}"]}, {"block": 17, "type": "code", "linesLength": 1, "startIndex": 41, "lines": ["!ls {PATH}valid"]}, {"block": 18, "type": "code", "linesLength": 2, "startIndex": 42, "lines": ["files = !ls {PATH}valid/cats | head\n", "files"]}, {"block": 19, "type": "code", "linesLength": 2, "startIndex": 44, "lines": ["img = plt.imread(f'{PATH}valid/cats/{files[0]}')\n", "plt.imshow(img);"]}, {"block": 20, "type": "markdown", "linesLength": 1, "startIndex": 46, "lines": ["Here is how the raw data looks like"]}, {"block": 21, "type": "code", "linesLength": 1, "startIndex": 47, "lines": ["img.shape"]}, {"block": 22, "type": "code", "linesLength": 1, "startIndex": 48, "lines": ["img[:4,:4]"]}, {"block": 23, "type": "markdown", "linesLength": 1, "startIndex": 49, "lines": ["### Our first model"]}, {"block": 24, "type": "markdown", "linesLength": 1, "startIndex": 50, "lines": ["#### Quick start"]}, {"block": 25, "type": "markdown", "linesLength": 1, "startIndex": 51, "lines": ["Here we import the libraries we need. We'll learn about what each does during the course."]}, {"block": 26, "type": "code", "linesLength": 5, "startIndex": 52, "lines": ["from fastai.transforms import *\n", "from fastai.conv_learner import *\n", "from fastai.model import *\n", "from fastai.dataset import *\n", "from fastai.sgdr import *"]}, {"block": 27, "type": "markdown", "linesLength": 5, "startIndex": 57, "lines": ["We're going to use a <b>pre-trained</b> model, that is, a model created by some one else to solve a different problem. Instead of building a model from scratch to solve a similar problem, we'll use a model trained on ImageNet (1.2 million images and 1000 classes) as a starting point. The model is a Convolutional Neural Network (CNN), a type of Neural Network that builds state-of-the-art models for computer vision. We'll be learning all about CNNs during this course.\n", "\n", "We will be using the <b>resnet34</b> model. resnet34 is a version of the model that won the 2015 ImageNet competition. Here is more info on [resnet models](https://github.com/KaimingHe/deep-residual-networks). We'll be studying them in depth later, but for now we'll focus on using them effectively.\n", "\n", "Here's how to train and evalulate a *dogs vs cats* model in 3 lines of code, and under 20 seconds of compute time:"]}, {"block": 28, "type": "code", "linesLength": 3, "startIndex": 62, "lines": ["data = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(resnet34, 299))\n", "learn = ConvLearner.pretrained(resnet34, data, precompute=True)\n", "learn.fit(0.01, 2)"]}, {"block": 29, "type": "markdown", "linesLength": 1, "startIndex": 65, "lines": ["How good is this model? Well, as we mentioned, prior to this competition, the state of the art was 80% accuracy. But the competition resulted in a huge jump to 98.9% accuracy, with the author of a popular deep learning library winning the competition. Extraordinarily, less than 4 years later, we can now beat that result in seconds! Even last year in this same course, our initial model had 98.3% accuracy, which is nearly double the error we're getting just a year later."]}, {"block": 30, "type": "markdown", "linesLength": 4, "startIndex": 66, "lines": ["**Key concepts:**\n", "* **Epoch:** During iterative training of a neural network, an Epoch is a single pass through the entire training set.\n", "* **Accuracy:** The ratio of correctly predicted observation to the total observations. \n", "* **Loss:**  A (computationally feasible) function representing the price paid for inaccuracy of predictions."]}, {"block": 31, "type": "markdown", "linesLength": 1, "startIndex": 70, "lines": ["#### Understanding the code for our first model"]}, {"block": 32, "type": "markdown", "linesLength": 3, "startIndex": 71, "lines": ["Let's look at this code line by line.\n", "\n", "**tfms** stands for *transformations*. `tfms_from_model` takes care of resizing (we are going to use size 299x299 images), image cropping, initial normalization (creating data with (mean,stdev) of (0,1)), and more."]}, {"block": 33, "type": "code", "linesLength": 1, "startIndex": 74, "lines": ["tfms = tfms_from_model(resnet34, 299)"]}, {"block": 34, "type": "markdown", "linesLength": 1, "startIndex": 75, "lines": ["We need a <b>path</b> that points to the dataset. In this path we will also store temporary data and final results. `ImageClassifierData.from_paths` reads data from a provided path and creates a dataset ready for training."]}, {"block": 35, "type": "code", "linesLength": 1, "startIndex": 76, "lines": ["data = ImageClassifierData.from_paths(PATH, tfms=tfms)"]}, {"block": 36, "type": "markdown", "linesLength": 5, "startIndex": 77, "lines": ["`ConvLearner.pretrained` builds *learner* that contains a pretrained model. The last layer of the model needs to be replaced with the layer of the right dimensions. The pretained model was trained for 1000 classes but we need a model with two classes (cats and dogs). The diagram below shows in an example how this was done in one of the earliest successful CNNs. The layer \"FC8\" here would get replaced with a new layer with 2 outputs. \n", "\n", "\n", "<img src=\"images/pretrained.png\" width=\"500\">\n", "[original image](https://image.slidesharecdn.com/practicaldeeplearning-160329181459/95/practical-deep-learning-16-638.jpg)"]}, {"block": 37, "type": "code", "linesLength": 1, "startIndex": 82, "lines": ["learn = ConvLearner.pretrained(resnet34, data, precompute=True)"]}, {"block": 38, "type": "markdown", "linesLength": 1, "startIndex": 83, "lines": ["`precompute=True` triggers a faster way of training this model. Here is how we do it. We use the all layers but the last one (up to layer FC7 in the previous diagram) to precompute activations from the training set. You can think about this as a feature extractor, for every image we get a vector of the size of the input to the last layer (4096 in the diagram). With the precomputed features our training of the last layer is much faster."]}, {"block": 39, "type": "markdown", "linesLength": 7, "startIndex": 84, "lines": ["**What are model parameters?** A machine learning model is the definition of a mathematical formula with a number of parameters (or weights) that need to be learned from the data. For an example, let's consider logistic regression which is the simplest neural network. Given an input vector $x$ and a set of parameters (weights) $w$ and $b$. A logistic regression has a form:\n", "\n", "$$\\hat{y}=\\sigma(wx +b) $$\n", "\n", "Where $\\sigma$ is the sigmoid function. \n", "\n", "For the logistic regression $w, b$ are the set of parameters. Given a set of parameters we can make predictions for every input $x$."]}, {"block": 40, "type": "markdown", "linesLength": 1, "startIndex": 91, "lines": ["**Model training (fitting)**: We can now *fit* the model. This is done through a process known as model training (using gradient descent, which we will be studying later). By training a model with existing data, we are able to fit the model parameters."]}, {"block": 41, "type": "markdown", "linesLength": 2, "startIndex": 92, "lines": ["**What are model hyparameters?**\n", "A hyperparameter is another kind of parameter whose values are set before to the start of the training process. These parameters express \u201chigher-level\u201d properties of the model such as its complexity or how fast it should learn."]}, {"block": 42, "type": "markdown", "linesLength": 1, "startIndex": 94, "lines": ["We need to pass two hyperparameters to the `learn.fit` function: the *learning rate* (generally 1e-2 or 1e-3 is a good starting point, we'll look more at this next) and the *number of epochs* (you can pass in a higher number and just stop training when you see it's no longer improving, then re-run it with the number of epochs you found works well.)"]}, {"block": 43, "type": "code", "linesLength": 1, "startIndex": 95, "lines": ["learn.fit(1e-2, 2)"]}, {"block": 44, "type": "markdown", "linesLength": 1, "startIndex": 96, "lines": ["#### Choosing a learning rate"]}, {"block": 45, "type": "markdown", "linesLength": 4, "startIndex": 97, "lines": ["The **learning rate** determines how quickly or how slowly you want to update the weight parameters in gradient descent. Too small a learning rate will make a training algorithm converge slowly while too large a learning rate will make the training algorithm\n", "diverge. The learning rate is the most important hyper-parameter to tune for training deep neural networks.\n", "\n", "`learn.lr_find()` helps you find an optimal learning rate. It uses the technique developed in the 2015 paper [Cyclical Learning Rates for Training Neural Networks](http://arxiv.org/abs/1506.01186), where we simply keep increasing the learning rate from a very small value, until the loss stops decreasing. We can plot the learning rate across batches to see what this looks like. Our `learn` object contains an attribute `sched` that contains our learning rate scheduler, and has some convenient plotting functionality including this one:"]}, {"block": 46, "type": "code", "linesLength": 4, "startIndex": 101, "lines": ["lrf=learn.lr_find()\n", "# should we make this a scaterplot instead? Should we skip this plot for now?\n", "# maybe label the axis\n", "learn.sched.plot_lr()"]}, {"block": 47, "type": "markdown", "linesLength": 1, "startIndex": 105, "lines": ["A **batch** is a set of obseravations from the training set. The bash size (bs) is the size of the batch which is ussually a small constant. Here is the bash size, number and training examples and the ratio between the two. "]}, {"block": 48, "type": "code", "linesLength": 1, "startIndex": 106, "lines": ["print(data.bs, len(data.trn_ds.y), len(data.trn_ds.y)/data.bs )"]}, {"block": 49, "type": "markdown", "linesLength": 1, "startIndex": 107, "lines": ["In the computation of `learn.lr_find()` a batch of training data is used at each iteration. We can see the plot of loss versus learning rate to see where our loss stops decreasing:"]}, {"block": 50, "type": "code", "linesLength": 1, "startIndex": 108, "lines": ["learn.sched.plot()"]}, {"block": 51, "type": "markdown", "linesLength": 1, "startIndex": 109, "lines": ["The loss is still clearly improving at lr=1e-2 (0.01), so that's what we use (ignore the first few batches, since there wasn't enough history to have a meaningful average). Note that the optimal learning rate can change as we training the model, so you may want to re-run this function from time to time."]}, {"block": 52, "type": "markdown", "linesLength": 1, "startIndex": 110, "lines": ["### Improving our model"]}, {"block": 53, "type": "markdown", "linesLength": 1, "startIndex": 111, "lines": ["#### Data augmentation"]}, {"block": 54, "type": "markdown", "linesLength": 3, "startIndex": 112, "lines": ["If you try training for more epochs, you'll notice that we start to *overfit*, which means that our model is learning to recognize the specific images in the training set, rather than generalizaing such that we also get good results on the validation set. One way to fix this is to effectively create more data, through *data augmentation*. This refers to randomly changing the images in ways that shouldn't impact their interpretation, such as horizontal flipping, zooming, and rotating.\n", "\n", "We can do this by passing `aug_tfms` (*augmentation transforms*) to `tfms_from_model`, with a list of functions to apply that randomly change the image however we wish. For photos that are largely taken from the side (e.g. most photos of dogs and cats, as opposed to photos taken from the top down, such as satellite imagery) we can use the pre-defined list of functions `transforms_side_on`. We can also specify random zooming of images up to specified scale by adding the `max_zoom` parameter."]}, {"block": 55, "type": "code", "linesLength": 1, "startIndex": 115, "lines": ["from fastai.plots import *"]}, {"block": 56, "type": "code", "linesLength": 1, "startIndex": 116, "lines": ["tfms = tfms_from_model(resnet34, 299, aug_tfms=transforms_side_on, max_zoom=1.1)"]}, {"block": 57, "type": "code", "linesLength": 7, "startIndex": 117, "lines": ["def get_augs():\n", "    data = ImageClassifierData.from_paths(PATH, bs=2, tfms=tfms)\n", "    x,_ = next(iter(data.aug_dl))\n", "    return data.trn_ds.denorm(x)[1]\n", "\n", "ims = np.stack([get_augs() for i in range(6)])\n", "plots(ims, rows=2)"]}, {"block": 58, "type": "code", "linesLength": 2, "startIndex": 124, "lines": ["data = ImageClassifierData.from_paths(PATH, tfms=tfms)\n", "learn = ConvLearner.pretrained(resnet34, data, precompute=True)"]}, {"block": 59, "type": "markdown", "linesLength": 1, "startIndex": 126, "lines": ["We fit the final layer from the pre-computed activations as before, but just for one epoch this time to get it started quickly."]}, {"block": 60, "type": "code", "linesLength": 1, "startIndex": 127, "lines": ["learn.fit(1e-2, 1)"]}, {"block": 61, "type": "markdown", "linesLength": 1, "startIndex": 128, "lines": ["To use our augmented data, we can no longer pre-compute the penultimate layer's activations - we never see the exact same picture twice since we have random augmentations, so we don't have the same activations. Therefore we disable `precompute`, the attribute that toggles the use of pre-computed activations."]}, {"block": 62, "type": "code", "linesLength": 1, "startIndex": 129, "lines": ["learn.precompute = False"]}, {"block": 63, "type": "markdown", "linesLength": 1, "startIndex": 130, "lines": ["By default when we create a learner, it sets all but the last layer to *frozen*. That means that even although we're not using the precomputed activations any more, it's still only updating the weights in the last layer when we call `fit`."]}, {"block": 64, "type": "code", "linesLength": 1, "startIndex": 131, "lines": ["learn.fit(1e-2, 3)"]}, {"block": 65, "type": "markdown", "linesLength": 3, "startIndex": 132, "lines": ["Our validation loss isn't improving much, so there's probably no point further training the last layer on its own. But there is something else we can do with data augmentation: use it at *inference* time (also known as *test* time). Not surprisingly, this is known as *test time augmentation*, or just *TTA*.\n", "\n", "TTA simply makes predictions not just on the images in your validation set, but also makes predictions on a number of randomly augmented versions of them too (by default, it uses the original image along with 4 randomly augmented versions). It then takes the average prediction from these images, and uses that. To use TTA on the validation set, we can use the learner's `TTA()` method."]}, {"block": 66, "type": "code", "linesLength": 1, "startIndex": 135, "lines": ["accuracy(*learn.TTA())"]}, {"block": 67, "type": "markdown", "linesLength": 3, "startIndex": 136, "lines": ["I generally see about a 30% reduction in error on this dataset when using TTA at this point, which is an amazing result for such a quick and easy technique!\n", "\n", "Since we've got a pretty good model at this point, we might want to save it so we can load it again later without training it from scratch."]}, {"block": 68, "type": "code", "linesLength": 1, "startIndex": 139, "lines": ["learn.save('299_fc')"]}, {"block": 69, "type": "markdown", "linesLength": 1, "startIndex": 140, "lines": ["#### Fine-tuning and differential learning rate annealing"]}, {"block": 70, "type": "markdown", "linesLength": 1, "startIndex": 141, "lines": ["Now that we have a good final layer trained, we can try fine-tuning the other layers. To tell the learner that we want to unfreeze the remaining layers, just call (surprise surprise!) `unfreeze()`."]}, {"block": 71, "type": "code", "linesLength": 1, "startIndex": 142, "lines": ["learn.unfreeze()"]}, {"block": 72, "type": "markdown", "linesLength": 3, "startIndex": 143, "lines": ["Note that the other layers have *already* been trained to recognize imagenet photos (whereas our final layers where randomly initialized), so we want to be careful of not destroying the carefully tuned weights that are already there.\n", "\n", "Generally speaking, the earlier layers (as we've seen) have more general-purpose features. Therefore we would expect them to need less fine-tuning for new datasets. For this reason we will use different learning rates for different layers: the first few layers will be at 1e-4, the middle layers at 1e-3, and our FC layers we'll leave at 1e-2 as before. We refer to this as *differential learning rates*, although there's no standard name for this techique in the literature that we're aware of."]}, {"block": 73, "type": "code", "linesLength": 1, "startIndex": 146, "lines": ["lr=np.array([1e-4,1e-3,1e-2])"]}, {"block": 74, "type": "code", "linesLength": 1, "startIndex": 147, "lines": ["learn.fit(lr, 2, cycle_len=2)"]}, {"block": 75, "type": "markdown", "linesLength": 6, "startIndex": 148, "lines": ["Why did this complete 4 epochs when we passed an argument of 2? And what is that `cycle_len` parameter? What we've done here is used a technique called *learning rate annealing*, which gradually decreases the learning rate as training progresses. This is helpful because as we get closer to the optimal weights, we want to take smaller steps.\n", "\n", "However, we may find ourselves in a part of the weight space that isn't very resilient - that is, small changes to the weights may result in big changes to the loss. We want to encourage our model to find parts of the weight space that are both accurate and stable. Therefore, from time to time we increase the learning rate, which will force the model to jump to a different part of the weight space if the current area is \"spikey\". Here's a picture of how that might look if we reset the learning rates 3 times (in this paper they call it a \"cyclic LR schedule\"):\n", "\n", "<img src=\"images/sgdr.png\" width=\"80%\">\n", "(From the paper [Snapshot Ensembles](https://arxiv.org/abs/1704.00109))."]}, {"block": 76, "type": "markdown", "linesLength": 1, "startIndex": 154, "lines": ["The number of epochs between resetting the learning rate is set by `cycle_len`, and the number of times this happens is refered to as the *number of cycles*, and is what we're actually passing as the 2nd parameter to `fit()`. So here's what our actual learning rates looked like:"]}, {"block": 77, "type": "code", "linesLength": 1, "startIndex": 155, "lines": ["learn.sched.plot_lr()"]}, {"block": 78, "type": "markdown", "linesLength": 3, "startIndex": 156, "lines": ["Note that's what being plotted above is the learning rate of the *final layers*. The learning rates of the earlier layers are fixed at the same multiples of the final layer rates as we initially requested (i.e. the first layers have 100x smaller, and middle layers 10x smaller learning rates, since we set `lr=np.array([1e-4,1e-3,1e-2])`.\n", "\n", "You should find our TTA accuracy has improved a little more."]}, {"block": 79, "type": "code", "linesLength": 1, "startIndex": 159, "lines": ["accuracy(*learn.TTA())"]}, {"block": 80, "type": "markdown", "linesLength": 1, "startIndex": 160, "lines": ["## Currently unused"]}, {"block": 81, "type": "markdown", "linesLength": 1, "startIndex": 161, "lines": ["### Class Saliency Visualisation"]}, {"block": 82, "type": "markdown", "linesLength": 1, "startIndex": 162, "lines": ["This method allows an object classifier to be used for object localization, or to better understand misclassifications."]}, {"block": 83, "type": "code", "linesLength": 1, "startIndex": 163, "lines": ["from IPython.display import display, Math, Latex"]}, {"block": 84, "type": "markdown", "linesLength": 2, "startIndex": 164, "lines": ["Our model visualization is a version of this.\n", "[script](https://github.com/leelabcnbc/cnnvis-pytorch/blob/master/test.ipynb). The details of this approach are descriped in this [paper](https://arxiv.org/pdf/1312.6034.pdf)."]}, {"block": 85, "type": "markdown", "linesLength": 1, "startIndex": 166, "lines": ["Given an image $I_0$, a class $c$, and a classification ConvNet with the class score function $Sc(I)$, we would like to rank the pixels of $I_0$ based on their influence on the score $Sc(I_0)$."]}, {"block": 86, "type": "code", "linesLength": 8, "startIndex": 167, "lines": ["bs=1\n", "f_model = resnet34\n", "path = \"/data/yinterian/dogscats/\"\n", "sz=299\n", "tfms = tfms_from_model(f_model, sz, aug_tfms=transforms_side_on, max_zoom=1.1)\n", "data = ImageClassifierData.from_paths(path, bs, tfms, test_name='test1')\n", "learn = ConvLearner.pretrained(f_model, data)\n", "learn.load('299_no')"]}, {"block": 87, "type": "code", "linesLength": 6, "startIndex": 175, "lines": ["# to get the first element of the validation data\n", "x, y = data.val_dl.dataset[5]\n", "model = learn.model\n", "denorm = tfms[0].denorm\n", "x2 = np.rollaxis(x, 0, 3)\n", "x_org = denorm(x2)"]}, {"block": 88, "type": "code", "linesLength": 1, "startIndex": 181, "lines": ["print(x.shape, x2.shape, x_org.shape)"]}, {"block": 89, "type": "code", "linesLength": 1, "startIndex": 182, "lines": ["plt.imshow(x2)"]}, {"block": 90, "type": "code", "linesLength": 1, "startIndex": 183, "lines": ["plt.imshow(x_org)"]}, {"block": 91, "type": "code", "linesLength": 1, "startIndex": 184, "lines": ["_ = model.eval()"]}, {"block": 92, "type": "code", "linesLength": 3, "startIndex": 185, "lines": ["from torch.autograd import Variable\n", "from torch.nn import Parameter\n", "from torch import Tensor"]}, {"block": 93, "type": "code", "linesLength": 1, "startIndex": 188, "lines": ["x.shape"]}, {"block": 94, "type": "code", "linesLength": 3, "startIndex": 189, "lines": ["xx = T(x).unsqueeze(0)\n", "xx.size()\n", "xx = xx.contiguous()"]}, {"block": 95, "type": "code", "linesLength": 1, "startIndex": 192, "lines": ["input_img = Parameter(xx.cuda(async=True), requires_grad=True)"]}, {"block": 96, "type": "code", "linesLength": 2, "startIndex": 193, "lines": ["if input_img.grad is not None:\n", "    input_img.grad.data.zero_()"]}, {"block": 97, "type": "code", "linesLength": 1, "startIndex": 195, "lines": ["model.zero_grad()"]}, {"block": 98, "type": "code", "linesLength": 1, "startIndex": 196, "lines": ["raw_score = model(input_img)"]}, {"block": 99, "type": "code", "linesLength": 2, "startIndex": 197, "lines": ["raw_score_numpy = raw_score.data.cpu().numpy()\n", "print(np.around(np.exp(raw_score_numpy), decimals=4))"]}, {"block": 100, "type": "code", "linesLength": 21, "startIndex": 199, "lines": ["def show_images(img_original, saliency, title):\n", "    # convert from c01 to 01c\n", "    print(saliency.min(), saliency.max(), saliency.mean(), saliency.std())\n", "    saliency = saliency[::-1]  # to BGR\n", "    saliency = saliency.transpose(1, 2, 0)\n", "    # plot the original image and the three saliency map variants\n", "    plt.figure(figsize=(10, 10), facecolor='w')\n", "    plt.subplot(2, 2, 1)\n", "    plt.title('input')\n", "    plt.imshow(np.asarray(img_original))\n", "    plt.subplot(2, 2, 2)\n", "    plt.title('abs. saliency')\n", "    plt.imshow(np.abs(saliency).max(axis=-1), cmap='gray')\n", "    plt.subplot(2, 2, 3)\n", "    plt.title('pos. saliency')\n", "    plt.imshow((np.maximum(0, saliency) / saliency.max()))\n", "    plt.subplot(2, 2, 4)\n", "    plt.title('neg. saliency')\n", "    plt.imshow((np.maximum(0, -saliency) / -saliency.min()))\n", "    plt.suptitle(title)\n", "    plt.show()"]}, {"block": 101, "type": "code", "linesLength": 1, "startIndex": 220, "lines": ["loss = raw_score.sum()"]}, {"block": 102, "type": "code", "linesLength": 1, "startIndex": 221, "lines": ["loss.backward(retain_variables=True)"]}, {"block": 103, "type": "code", "linesLength": 1, "startIndex": 222, "lines": ["show_images(x_org, input_img.grad.data.cpu().numpy()[0], 'naive')\n"]}, {"block": 104, "type": "markdown", "linesLength": 1, "startIndex": 223, "lines": ["### Visualizing kernels"]}, {"block": 105, "type": "code", "linesLength": 2, "startIndex": 224, "lines": ["# to look at the model\n", "#print(model)"]}, {"block": 106, "type": "code", "linesLength": 14, "startIndex": 226, "lines": ["def plot_first_layer_kernels(tensor, num_cols=6):\n", "    num_kernels = tensor.shape[0]\n", "    num_rows = 1+ num_kernels // num_cols\n", "    fig = plt.figure(figsize=(num_cols,num_rows))\n", "    for i in range(tensor.shape[0]):\n", "        kernel = tensor[i]\n", "        kernel = np.rollaxis(kernel, 0, 3)\n", "        ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n", "        ax1.imshow(kernel)\n", "        ax1.axis('off')\n", "        ax1.set_xticklabels([])\n", "        ax1.set_yticklabels([])\n", "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n", "    plt.show()"]}, {"block": 107, "type": "code", "linesLength": 1, "startIndex": 240, "lines": ["mm = model.double()"]}, {"block": 108, "type": "code", "linesLength": 3, "startIndex": 241, "lines": ["filters = mm.modules\n", "layer1 = [i for i in mm.children()][0]\n", "tensor = layer1.weight.data.cpu().numpy()"]}, {"block": 109, "type": "code", "linesLength": 1, "startIndex": 244, "lines": ["tensor.shape"]}, {"block": 110, "type": "code", "linesLength": 1, "startIndex": 245, "lines": ["layer1"]}, {"block": 111, "type": "code", "linesLength": 2, "startIndex": 246, "lines": ["# 18 children\n", "[i for i in mm.children()][4][0]"]}, {"block": 112, "type": "code", "linesLength": 2, "startIndex": 248, "lines": ["# here are the 64 7x7x3 top kernels\n", "plot_first_layer_kernels(tensor, num_cols=8)"]}, {"block": 113, "type": "code", "linesLength": 2, "startIndex": 250, "lines": ["layer2 = mm[4][0].conv1\n", "tensor2 = layer2.weight.data.cpu().numpy()"]}, {"block": 114, "type": "code", "linesLength": 1, "startIndex": 252, "lines": ["tensor2.shape "]}, {"block": 115, "type": "code", "linesLength": 15, "startIndex": 253, "lines": ["def plot_kernels_v2(tensor):\n", "    num_kernels = tensor.shape[0]*tensor.shape[1]\n", "    num_cols = tensor.shape[0]\n", "    num_rows = tensor.shape[1]\n", "    fig = plt.figure(figsize=(num_cols,num_rows))\n", "    for i in range(tensor.shape[0]):\n", "        for j in range(tensor.shape[1]):\n", "            kernel = tensor[i][j]\n", "            ax1 = fig.add_subplot(num_rows,num_cols,i+1 + j*i)\n", "            ax1.imshow(kernel, cmap='gray')\n", "            ax1.axis('off')\n", "            ax1.set_xticklabels([])\n", "            ax1.set_yticklabels([])\n", "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n", "    plt.show()"]}, {"block": 116, "type": "code", "linesLength": 2, "startIndex": 268, "lines": ["#This is not the right way of visualizing this\n", "plot_kernels_v2(tensor2)"]}, {"block": 117, "type": "code", "linesLength": 1, "startIndex": 270, "lines": ["tensor2[0][0].shape"]}]