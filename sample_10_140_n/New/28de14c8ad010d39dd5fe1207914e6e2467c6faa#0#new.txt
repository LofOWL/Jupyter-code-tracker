[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["# Spark Collaborative Filtering (ALS) Deep Dive"]}, {"block": 1, "type": "markdown", "linesLength": 3, "startIndex": 1, "lines": ["Spark MLlib provides a collaborative filtering algorithm that can be used for training a matrix factorization model, which predicts explicit or implicit ratings of users on items, for recommendations.\n", "\n", "This notebook presents a deep dive into the Spark collaborative filtering algorithm."]}, {"block": 2, "type": "markdown", "linesLength": 1, "startIndex": 4, "lines": ["## 0 Global settings\n"]}, {"block": 3, "type": "code", "linesLength": 23, "startIndex": 5, "lines": ["# set the environment path to find Recommenders\n", "import sys\n", "sys.path.append(\"../../\")\n", "import pandas as pd\n", "from matplotlib import pyplot as plt\n", "import numpy as np\n", "import seaborn as sns\n", "import sys\n", "import pandas as pd\n", "import pyspark\n", "from pyspark.ml.recommendation import ALS\n", "from pyspark.sql.functions import col\n", "from pyspark.ml.tuning import CrossValidator\n", "\n", "from reco_utils.common.spark_utils import start_or_get_spark\n", "from reco_utils.evaluation.spark_evaluation import SparkRankingEvaluation, SparkRatingEvaluation\n", "from reco_utils.evaluation.parameter_sweep import generate_param_grid\n", "from reco_utils.dataset.url_utils import maybe_download\n", "from reco_utils.dataset.spark_splitters import spark_random_split\n", "\n", "print(\"System version: {}\".format(sys.version))\n", "print(\"Pandas version: {}\".format(pd.__version__))\n", "print(\"PySpark version: {}\".format(pyspark.__version__))"]}, {"block": 4, "type": "markdown", "linesLength": 1, "startIndex": 28, "lines": ["Data URL and path"]}, {"block": 5, "type": "code", "linesLength": 1, "startIndex": 29, "lines": ["DATA_URL = \"http://files.grouplens.org/datasets/movielens/ml-100k/u.data\"\n"]}, {"block": 6, "type": "markdown", "linesLength": 1, "startIndex": 30, "lines": ["Data column names"]}, {"block": 7, "type": "code", "linesLength": 12, "startIndex": 31, "lines": ["COL_USER = \"userID\"\n", "COL_ITEM = \"itemID\"\n", "COL_RATING = \"rating\"\n", "COL_PREDICTION = \"prediction\"\n", "COL_TIMESTAMP = \"timestamp\"\n", "\n", "HEADER = {\n", "    \"col_user\": COL_USER,\n", "    \"col_item\": COL_ITEM,\n", "    \"col_rating\": COL_RATING,\n", "    \"col_prediction\": COL_PREDICTION,\n", "}"]}, {"block": 8, "type": "markdown", "linesLength": 1, "startIndex": 43, "lines": ["Model hyper parameters"]}, {"block": 9, "type": "code", "linesLength": 3, "startIndex": 44, "lines": ["RANK = 10\n", "MAX_ITER = 5\n", "REG_PARAM = 0.01"]}, {"block": 10, "type": "markdown", "linesLength": 1, "startIndex": 47, "lines": ["Number of recommended items"]}, {"block": 11, "type": "code", "linesLength": 1, "startIndex": 48, "lines": ["K = 10"]}, {"block": 12, "type": "markdown", "linesLength": 25, "startIndex": 49, "lines": ["## 1 Matrix factorization algorithm\n", "\n", "### 1.1 Matrix factorization for collaborative filtering problem\n", "\n", "Matrix factorization is a common technique used in recommendation tasks. Basically, a matrix factorization algorithm tries to find latent factors that represent intrinsic user and item attributes in a lower dimension. That is,\n", "\n", "$$\\hat r_{u,i} = q_{i}^{T}p_{u}$$\n", "\n", "where $\\hat r_{u,i}$ is the predicted ratings for user $u$ and item $i$, and $q_{i}^{T}$ and $p_{u}$ are latent factors for item and user, respectively. The challenge to the matrix factorization problem is to find $q_{i}^{T}$ and $p_{u}$. This is achieved by methods such as matrix decomposition. A learning approach is therefore developed to converge the decomposition results close to the observed ratings as much as possible. Furthermore, to avoid overfitting issue, the learning process is regularized. For example, a basic form of such matrix factorization algorithm is represented as below.\n", "\n", "$$\\min\\sum(r_{u,i} - q_{i}^{T}p_{u})^2 + \\lambda(||q_{i}||^2 + ||p_{u}||^2)$$\n", "\n", "where $\\lambda$ is a the regularization parameter. \n", "\n", "In case explict ratings are not available, implicit ratings which are usually derived from users' historical interactions with the items (e.g., clicks, views, purchases, etc.). To account for such implicit ratings, the original matrix factorization algorithm can be formulated as \n", "\n", "$$\\min\\sum c_{u,i}(p_{u,i} - q_{i}^{T}p_{u})^2 + \\lambda(||q_{i}||^2 + ||p_{u}||^2)$$\n", "\n", "where $c_{u,i}=1+\\alpha r_{u,i}$ and $p_{u,i}=1$ if $r_{u,i}>0$ and $p_{u,i}=0$ if $r_{u,i}=0$. $r_{u,i}$ is a numerical representation of users' preferences (e.g., number of clicks, etc.). \n", "\n", "### 1.2 Alternating Least Square (ALS)\n", "\n", "Owing to the term of $q_{i}^{T}p_{u}$ the loss function is non-convex. Gradient descent method can be applied but this will incur expensive computations. An Alternating Least Square (ALS) algorithm was therefore developed to overcome this issue. \n", "\n", "The basic idea of ALS is to learn one of $q$ and $p$ at a time for optimization while keeping the other as constant. This makes the objective at each iteration convex and solvable. The alternating between $q$ and $p$ stops when there is convergence to the optimal. It is worth noting that this iterative computation can be parallelised and/or distributed, which makes the algorithm desirable for use cases where the dataset is large and thus the user-item rating matrix is super sparse (as is typical in recommendation scenarios). A comprehensive discussion of ALS and its distributed computation can be found [here](http://stanford.edu/~rezab/classes/cme323/S15/notes/lec14.pdf)."]}, {"block": 13, "type": "markdown", "linesLength": 7, "startIndex": 74, "lines": ["## 2 Spark Mllib implementation\n", "\n", "The matrix factorization algorithm is available as `ALS` module in [Spark `ml`](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html) for DataFrame or [Spark `mllib`](https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html) for RDD. \n", "\n", "* The uniqueness of ALS implementation is that it distributes the matrix factorization model training by using \"Alternating Least Square\" method. \n", "* In the training method, there are parameters that can be selected to control the model performance.\n", "* Both explicit and implicit ratings are supported by Spark ALS model."]}, {"block": 14, "type": "markdown", "linesLength": 5, "startIndex": 81, "lines": ["## 3 Spark ALS based Movielens recommender\n", "\n", "In the following code, the Movielens-100K dataset is used to illustrate the ALS algorithm in Spark.\n", "\n", "First, a Spark session is initialized."]}, {"block": 15, "type": "code", "linesLength": 1, "startIndex": 86, "lines": ["spark = start_or_get_spark(\"Spark ALS\", \"local\")"]}, {"block": 16, "type": "markdown", "linesLength": 1, "startIndex": 87, "lines": ["### 3.1 Load and prepare data"]}, {"block": 17, "type": "markdown", "linesLength": 1, "startIndex": 88, "lines": ["Data is read from csv into a Spark DataFrame."]}, {"block": 18, "type": "code", "linesLength": 5, "startIndex": 89, "lines": ["# read small file over HTTP\n", "dfs = spark.createDataFrame(pd.read_csv(DATA_URL, delimiter = \"\\t\", header = None))\\\n", ".withColumnRenamed(\"0\", COL_USER).withColumnRenamed(\"1\", COL_ITEM) \\\n", ".withColumnRenamed(\"2\", COL_RATING) \\\n", ".withColumnRenamed(\"3\", COL_TIMESTAMP)"]}, {"block": 19, "type": "code", "linesLength": 1, "startIndex": 94, "lines": ["dfs.show(5)"]}, {"block": 20, "type": "markdown", "linesLength": 1, "startIndex": 95, "lines": ["Data is then randomly split by 80-20 ratio for training and testing."]}, {"block": 21, "type": "code", "linesLength": 1, "startIndex": 96, "lines": ["dfs_train, dfs_test = spark_random_split(dfs, ratio=0.8)"]}, {"block": 22, "type": "code", "linesLength": 2, "startIndex": 97, "lines": ["dfs_train = dfs_train.select(COL_USER, COL_ITEM, COL_RATING)\n", "dfs_test = dfs_test.select(COL_USER, COL_ITEM, COL_RATING).withColumn(COL_RATING, col(COL_RATING).cast(\"float\"))"]}, {"block": 23, "type": "markdown", "linesLength": 1, "startIndex": 99, "lines": ["### 3.2 Train a movielens model "]}, {"block": 24, "type": "markdown", "linesLength": 1, "startIndex": 100, "lines": ["It is worth noting that Spark ALS model allows dropping cold users to favor a robust evaluation with the testing data. In case there are cold users, Spark ALS implementation allows users to drop cold users in order to make sure evaluations on the prediction results are sound."]}, {"block": 25, "type": "code", "linesLength": 11, "startIndex": 101, "lines": ["als = ALS(\n", "    maxIter=MAX_ITER, \n", "    rank=RANK,\n", "    regParam=REG_PARAM, \n", "    userCol=COL_USER, \n", "    itemCol=COL_ITEM, \n", "    ratingCol=COL_RATING, \n", "    coldStartStrategy=\"drop\"\n", ")\n", "\n", "model = als.fit(dfs_train)"]}, {"block": 26, "type": "markdown", "linesLength": 3, "startIndex": 112, "lines": ["### 3.3 Prediction with the model\n", "\n", "The trained model can be used to predict ratings with a given test data."]}, {"block": 27, "type": "code", "linesLength": 1, "startIndex": 115, "lines": ["dfs_pred = model.transform(dfs_test).drop(COL_RATING)"]}, {"block": 28, "type": "markdown", "linesLength": 1, "startIndex": 116, "lines": ["With the prediction results, the model performance can be evaluated."]}, {"block": 29, "type": "code", "linesLength": 16, "startIndex": 117, "lines": ["evaluations = SparkRatingEvaluation(\n", "    dfs_test, \n", "    dfs_pred,\n", "    col_user=COL_USER,\n", "    col_item=COL_ITEM,\n", "    col_rating=COL_RATING,\n", "    col_prediction=COL_PREDICTION\n", ")\n", "\n", "print(\n", "    \"RMSE score = {}\".format(evaluations.rmse()),\n", "    \"MAE score = {}\".format(evaluations.mae()),\n", "    \"R2 score = {}\".format(evaluations.rsquared()),\n", "    \"Explained variance score = {}\".format(evaluations.exp_var()),\n", "    sep=\"\\n\"\n", ")"]}, {"block": 30, "type": "markdown", "linesLength": 1, "startIndex": 133, "lines": ["Oftentimes ranking metrics are also of interest to data scientists."]}, {"block": 31, "type": "code", "linesLength": 17, "startIndex": 134, "lines": ["evaluations = SparkRankingEvaluation(\n", "    dfs_test, \n", "    dfs_pred,\n", "    col_user=COL_USER,\n", "    col_item=COL_ITEM,\n", "    col_rating=COL_RATING,\n", "    col_prediction=COL_PREDICTION,\n", "    k=K\n", ")\n", "\n", "print(\n", "    \"Precision@k = {}\".format(evaluations.precision_at_k()),\n", "    \"Recall@k = {}\".format(evaluations.recall_at_k()),\n", "    \"NDCG@k = {}\".format(evaluations.ndcg_at_k()),\n", "    \"Mean average precision = {}\".format(evaluations.map_at_k()),\n", "    sep=\"\\n\"\n", ")"]}, {"block": 32, "type": "markdown", "linesLength": 11, "startIndex": 151, "lines": ["### 3.4 Fine tune the model\n", "\n", "Prediction performance of a Spark ALS model is often affected by the parameters\n", "\n", "|Parameter|Description|Default value|Notes|\n", "|-------------|-----------------|------------------|-----------------|\n", "|`rank`|Number of latent factors|10|The larger the more intrinsic factors considered in the factorization modeling.|\n", "|`regParam`|Regularization parameter|1.0|The value needs to be selected empirically to avoid overfitting.|\n", "|`maxIters`|Maximum number of iterations|10|The more iterations the better the model converges to the optimal point.|\n", "\n", "It is always a good practice to start model building with default parameter values and then sweep the parameter in a range to find the optimal combination of parameters. The following parameter set is used for training ALS models for comparison study purposes."]}, {"block": 33, "type": "code", "linesLength": 4, "startIndex": 162, "lines": ["param_dict = {\n", "    \"rank\": [10, 15, 20, 25],\n", "    \"regParam\": [0.001, 0.1, 1.0]\n", "}"]}, {"block": 34, "type": "markdown", "linesLength": 1, "startIndex": 166, "lines": ["Generate a dictionary for each parameter combination which can then be fed into model training."]}, {"block": 35, "type": "code", "linesLength": 1, "startIndex": 167, "lines": ["param_grid = generate_param_grid(param_dict)"]}, {"block": 36, "type": "markdown", "linesLength": 1, "startIndex": 168, "lines": ["Train models with parameters specified in the parameter grid. Evaluate the model with, for example, the RMSE metric, and then record the metrics for visualization."]}, {"block": 37, "type": "code", "linesLength": 28, "startIndex": 169, "lines": ["rmse_score = []\n", "\n", "for g in param_grid:\n", "    als = ALS(        \n", "        userCol=COL_USER, \n", "        itemCol=COL_ITEM, \n", "        ratingCol=COL_RATING, \n", "        coldStartStrategy=\"drop\",\n", "        **g\n", "    )\n", "    \n", "    model = als.fit(dfs_train)\n", "    \n", "    dfs_pred = model.transform(dfs_test).drop(COL_RATING)\n", "    \n", "    evaluations = SparkRatingEvaluation(\n", "        dfs_test, \n", "        dfs_pred,\n", "        col_user=COL_USER,\n", "        col_item=COL_ITEM,\n", "        col_rating=COL_RATING,\n", "        col_prediction=COL_PREDICTION\n", "    )\n", "\n", "    rmse_score.append(evaluations.rmse())\n", "\n", "rmse_score = [float('%.4f' % x) for x in rmse_score]\n", "rmse_score_array = np.reshape(rmse_score, (len(param_dict[\"rank\"]), len(param_dict[\"regParam\"]))) "]}, {"block": 38, "type": "code", "linesLength": 2, "startIndex": 197, "lines": ["rmse_df = pd.DataFrame(data=rmse_score_array, index=pd.Index(param_dict[\"rank\"], name=\"rank\"), \n", "                       columns=pd.Index(param_dict[\"regParam\"], name=\"reg. parameter\"))"]}, {"block": 39, "type": "code", "linesLength": 4, "startIndex": 199, "lines": ["from matplotlib import pyplot as plt\n", "fig, ax = plt.subplots()\n", "sns.heatmap(rmse_df, cbar=False, annot=True, fmt=\".4g\")\n", "display(fig)"]}, {"block": 40, "type": "markdown", "linesLength": 1, "startIndex": 203, "lines": ["The calculated RMSE scores can be visualized to comparatively study how model performance is affected by different parameters."]}, {"block": 41, "type": "markdown", "linesLength": 1, "startIndex": 204, "lines": ["It can be seen from this visualization that RMSE first decreases and then increases as rank increases, due to overfitting. When the rank equals 20 and the regularization parameter equals 0.1, the model achieves the lowest RMSE score."]}, {"block": 42, "type": "markdown", "linesLength": 1, "startIndex": 205, "lines": ["It is noted from the visualization that the RMSE does not decrease together with increase of `rank`, which may be owing to the reason of overfitting. When `rank` is 10 and `regParam` is 0.1, the lowest RMSE score is achieved, which indicates that the model is optimal."]}, {"block": 43, "type": "markdown", "linesLength": 1, "startIndex": 206, "lines": ["### 3.5 Top K recommendation"]}, {"block": 44, "type": "markdown", "linesLength": 1, "startIndex": 207, "lines": ["#### 3.5.1 Top k for all users (items)"]}, {"block": 45, "type": "code", "linesLength": 1, "startIndex": 208, "lines": ["dfs_rec = model.recommendForAllUsers(10)"]}, {"block": 46, "type": "code", "linesLength": 1, "startIndex": 209, "lines": ["dfs_rec.show(10)"]}, {"block": 47, "type": "markdown", "linesLength": 1, "startIndex": 210, "lines": ["#### 3.5.2 Top k for a selected set of users (items)"]}, {"block": 48, "type": "code", "linesLength": 3, "startIndex": 211, "lines": ["users = dfs_train.select(als.getUserCol()).distinct().limit(3)\n", "\n", "dfs_rec_subset = model.recommendForUserSubset(users, 10)"]}, {"block": 49, "type": "code", "linesLength": 1, "startIndex": 214, "lines": ["dfs_rec_subset.show(10)"]}, {"block": 50, "type": "markdown", "linesLength": 7, "startIndex": 215, "lines": ["#### 3.5.3 Run-time considerations for top-k recommendations\n", "\n", "It is worth noting that usually computing the top-k recommendations for all users is the bottleneck of the whole pipeline (model training and scoring) of an ALS based recommendation system. This is because\n", "* Getting the top k from all user-item pairs requires a cross join which is usually very computationally expensive. \n", "* Inner products of user-item pairs are calculated individually instead of leveraging matrix block multiplication features which are available in certain contemporary computing acceleration libraries (e.g., BLAS).\n", "\n", "More details about possible optimizations of the top k recommendations in Spark can be found [here](https://engineeringblog.yelp.com/2018/05/scaling-collaborative-filtering-with-pyspark.html)."]}, {"block": 51, "type": "markdown", "linesLength": 1, "startIndex": 222, "lines": ["## References"]}, {"block": 52, "type": "markdown", "linesLength": 8, "startIndex": 223, "lines": ["1. Yehuda Koren, Robert Bell, and Chris Volinsky, \"Matrix Factorization Techniques for Recommender Systems\n", "\", ACM Computer, Vol. 42, Issue 8, pp 30-37, Aug., 2009.\n", "2. Yifan Hu, Yehuda Koren, and Chris Volinsky, \"Collaborative Filtering for Implicit Feedback Datasets\n", "\", Proc. IEEE ICDM, 2008, Dec, Pisa, Italy.\n", "3. Apache Spark. url: https://spark.apache.org/docs/latest/ml-collaborative-filtering.html\n", "4. Seaborn. url: https://seaborn.pydata.org/\n", "5. Scaling collaborative filtering with PySpark. url: https://engineeringblog.yelp.com/2018/05/scaling-collaborative-filtering-with-pyspark.html\n", "6. Matrix Completion via Alternating Least Square (ALS). url: http://stanford.edu/~rezab/classes/cme323/S15/notes/lec14.pdf"]}]