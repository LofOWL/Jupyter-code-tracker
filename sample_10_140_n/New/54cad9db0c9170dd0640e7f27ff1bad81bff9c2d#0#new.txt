[{"block": 0, "type": "markdown", "linesLength": 9, "startIndex": 0, "lines": ["# H2O Tutorial\n", "\n", "Author: Spencer Aiello\n", "\n", "Contact: spencer@h2oai.com\n", "\n", "This tutorial steps through a quick introduction to H2O's Python API. The goal of this tutorial is to introduce through a complete example H2O's capabilities from Python. Also, to help those that are accustomed to Scikit Learn and Pandas, the demo will be specific call outs for differences between H2O and those packages; this is intended to help anyone that needs to do machine learning on really Big Data make the transition. It is not meant to be a tutorial on machine learning or algorithms.\n", "\n", "Detailed documentation about H2O's and the Python API is available at http://docs.h2o.ai."]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 9, "lines": ["## Setting up your system for this demo"]}, {"block": 2, "type": "markdown", "linesLength": 1, "startIndex": 10, "lines": ["The following code creates two csv files using data from the [Boston Housing dataset](https://archive.ics.uci.edu/ml/datasets/Housing) which is built into scikit-learn and adds them to the local directory"]}, {"block": 3, "type": "code", "linesLength": 9, "startIndex": 11, "lines": ["import pandas as pd\n", "import numpy\n", "from numpy.random import choice\n", "from sklearn.datasets import load_boston\n", "from h2o.estimators.random_forest import H2ORandomForestEstimator\n", "\n", "\n", "import h2o\n", "h2o.init()"]}, {"block": 4, "type": "code", "linesLength": 5, "startIndex": 20, "lines": ["# transfer the boston data from pandas to H2O\n", "boston_data = load_boston()\n", "X = pd.DataFrame(data=boston_data.data, columns=boston_data.feature_names)\n", "X[\"Median_value\"] = boston_data.target\n", "X = h2o.H2OFrame.from_python(X.to_dict(\"list\"))"]}, {"block": 5, "type": "code", "linesLength": 7, "startIndex": 25, "lines": ["# select 10% for valdation\n", "r = X.runif(seed=123456789)\n", "train = X[r < 0.9,:]\n", "valid = X[r >= 0.9,:]\n", "\n", "h2o.export_file(train, \"Boston_housing_train.csv\", force=True)\n", "h2o.export_file(valid, \"Boston_housing_test.csv\", force=True)"]}, {"block": 6, "type": "markdown", "linesLength": 1, "startIndex": 32, "lines": ["Enable inline plotting in the Jupyter Notebook"]}, {"block": 7, "type": "code", "linesLength": 2, "startIndex": 33, "lines": ["%matplotlib inline\n", "import matplotlib.pyplot as plt"]}, {"block": 8, "type": "markdown", "linesLength": 1, "startIndex": 35, "lines": ["## Intro to H2O Data Munging"]}, {"block": 9, "type": "markdown", "linesLength": 1, "startIndex": 36, "lines": ["Read csv data into H2O. This loads the data into the H2O column compressed, in-memory, key-value store."]}, {"block": 10, "type": "code", "linesLength": 1, "startIndex": 37, "lines": ["fr = h2o.import_file(\"Boston_housing_train.csv\")"]}, {"block": 11, "type": "markdown", "linesLength": 1, "startIndex": 38, "lines": ["View the top of the H2O frame."]}, {"block": 12, "type": "code", "linesLength": 1, "startIndex": 39, "lines": ["fr.head()"]}, {"block": 13, "type": "markdown", "linesLength": 1, "startIndex": 40, "lines": ["View the bottom of the H2O Frame"]}, {"block": 14, "type": "code", "linesLength": 1, "startIndex": 41, "lines": ["fr.tail()"]}, {"block": 15, "type": "markdown", "linesLength": 3, "startIndex": 42, "lines": ["Select a column\n", "\n", "fr[\"VAR_NAME\"]"]}, {"block": 16, "type": "code", "linesLength": 1, "startIndex": 45, "lines": ["fr[\"CRIM\"].head() # Tab completes"]}, {"block": 17, "type": "markdown", "linesLength": 1, "startIndex": 46, "lines": ["Select a few columns"]}, {"block": 18, "type": "code", "linesLength": 2, "startIndex": 47, "lines": ["columns = [\"CRIM\", \"RM\", \"RAD\"]\n", "fr[columns].head()"]}, {"block": 19, "type": "markdown", "linesLength": 3, "startIndex": 49, "lines": ["Select a subset of rows\n", "\n", "Unlike in Pandas, columns may be identified by index or column name. **Therefore, when subsetting by rows, you must also pass the column selection.**"]}, {"block": 20, "type": "code", "linesLength": 1, "startIndex": 52, "lines": ["fr[2:7,:]  # explicitly select all columns with :"]}, {"block": 21, "type": "markdown", "linesLength": 12, "startIndex": 53, "lines": ["Key attributes:\n", "      * columns, names, col_names\n", "      * len, shape, dim, nrow, ncol\n", "      * types\n", "      \n", "Note: \n", "\n", "Since the data is _not_ in local python memory\n", "there is no \"values\" attribute. If you want to \n", "pull all of the data into the local python memory\n", "then do so explicitly with h2o.export_file and\n", "reading the data into python memory from disk."]}, {"block": 22, "type": "code", "linesLength": 14, "startIndex": 65, "lines": ["# The columns attribute is exactly like Pandas\n", "print(\"Columns:\", fr.columns, \"\\n\")\n", "print(\"Columns:\", fr.names, \"\\n\")\n", "print(\"Columns:\", fr.col_names, \"\\n\")\n", "\n", "# There are a number of attributes to get at the shape\n", "print(\"length:\", str( len(fr) ), \"\\n\")\n", "print(\"shape:\", fr.shape, \"\\n\")\n", "print(\"dim:\", fr.dim, \"\\n\")\n", "print(\"nrow:\", fr.nrow, \"\\n\")\n", "print(\"ncol:\", fr.ncol, \"\\n\")\n", "\n", "# Use the \"types\" attribute to list the column types\n", "print(\"types:\", fr.types, \"\\n\")"]}, {"block": 23, "type": "markdown", "linesLength": 1, "startIndex": 79, "lines": ["Select rows based on value"]}, {"block": 24, "type": "code", "linesLength": 1, "startIndex": 80, "lines": ["fr.shape"]}, {"block": 25, "type": "markdown", "linesLength": 1, "startIndex": 81, "lines": ["Boolean masks can be used to subselect rows based on a criteria."]}, {"block": 26, "type": "code", "linesLength": 2, "startIndex": 82, "lines": ["mask = fr[\"CRIM\"]>1\n", "fr[mask,:].shape"]}, {"block": 27, "type": "markdown", "linesLength": 1, "startIndex": 84, "lines": ["Get summary statistics of the data and additional data distribution information."]}, {"block": 28, "type": "code", "linesLength": 1, "startIndex": 85, "lines": ["fr.describe()"]}, {"block": 29, "type": "markdown", "linesLength": 4, "startIndex": 86, "lines": ["Set up the predictor and response column names\n", "\n", "Using H2O algorithms, it's easier to reference predictor and response columns\n", "by name in a single frame (i.e., don't split up X and y)"]}, {"block": 30, "type": "code", "linesLength": 3, "startIndex": 90, "lines": ["x = fr.names[:]\n", "y=\"Median_value\"\n", "x.remove(y)"]}, {"block": 31, "type": "markdown", "linesLength": 1, "startIndex": 93, "lines": ["## Machine Learning With H2O"]}, {"block": 32, "type": "markdown", "linesLength": 7, "startIndex": 94, "lines": ["H2O is a machine learning library built in Java with interfaces in Python, R, Scala, and Javascript. It is [open source](http://github.com/h2oai) and [well-documented](http://docs.h2o.ai).\n", "\n", "Unlike Scikit-learn, H2O allows for categorical and missing data.\n", "\n", "The basic work flow is as follows:\n", "* Fit the training data with a machine learning algorithm\n", "* Predict on the testing data"]}, {"block": 33, "type": "markdown", "linesLength": 1, "startIndex": 101, "lines": ["### Simple model"]}, {"block": 34, "type": "code", "linesLength": 3, "startIndex": 102, "lines": ["# Define and fit first 400 points\n", "model = H2ORandomForestEstimator(seed=42)\n", "model.train(x=x, y=y, training_frame=fr[:400,:])"]}, {"block": 35, "type": "code", "linesLength": 1, "startIndex": 105, "lines": ["model.predict(fr[400:fr.nrow,:])        # Predict the rest"]}, {"block": 36, "type": "markdown", "linesLength": 1, "startIndex": 106, "lines": ["The performance of the model can be checked using the holdout dataset"]}, {"block": 37, "type": "code", "linesLength": 4, "startIndex": 107, "lines": ["perf = model.model_performance(fr[400:fr.nrow,:])\n", "perf.r2()      # get the r2 on the holdout data\n", "perf.mse()     # get the mse on the holdout data\n", "perf           # display the performance object"]}, {"block": 38, "type": "markdown", "linesLength": 1, "startIndex": 111, "lines": ["### Train-Test Split"]}, {"block": 39, "type": "markdown", "linesLength": 1, "startIndex": 112, "lines": ["Instead of taking the first 400 observations for training, we can use H2O to create a random test train split of the data."]}, {"block": 40, "type": "code", "linesLength": 9, "startIndex": 113, "lines": ["r = fr.runif(seed=12345)   # build random uniform column over [0,1]\n", "train= fr[r<0.75,:]     # perform a 75-25 split\n", "test = fr[r>=0.75,:]\n", "\n", "model = H2ORandomForestEstimator(seed=42)\n", "model.train(x=x, y=y, training_frame=train, validation_frame=test)\n", "\n", "perf = model.model_performance(test)\n", "perf.r2()"]}, {"block": 41, "type": "markdown", "linesLength": 1, "startIndex": 122, "lines": ["There was a massive jump in the R^2 value. This is because the original data is not shuffled."]}, {"block": 42, "type": "markdown", "linesLength": 1, "startIndex": 123, "lines": ["### Cross validation"]}, {"block": 43, "type": "markdown", "linesLength": 1, "startIndex": 124, "lines": ["H2O's machine learning algorithms take an optional parameter **nfolds** to specify the number of cross-validation folds to build. H2O's cross-validation uses an internal weight vector to build the folds in an efficient manner (instead of physically building the splits)."]}, {"block": 44, "type": "markdown", "linesLength": 4, "startIndex": 125, "lines": ["In conjunction with the **nfolds** parameter, a user may specify the way in which observations are assigned to each fold with the **fold_assignment** parameter, which can be set to either:\n", "        * AUTO:  Perform random assignment\n", "        * Random: Each row has a equal (1/nfolds) chance of being in any fold.\n", "        * Modulo: Observations are in/out of the fold based by modding on nfolds"]}, {"block": 45, "type": "code", "linesLength": 2, "startIndex": 129, "lines": ["model = H2ORandomForestEstimator(nfolds=10) # build a 10-fold cross-validated model\n", "model.train(x=x, y=y, training_frame=fr)"]}, {"block": 46, "type": "code", "linesLength": 3, "startIndex": 131, "lines": ["scores = numpy.array([m.r2() for m in model.xvals]) # iterate over the xval models using the xvals attribute\n", "print(\"Expected R^2: %.2f +/- %.2f \\n\" % (scores.mean(), scores.std()*1.96))\n", "print(\"Scores:\", scores.round(2))"]}, {"block": 47, "type": "markdown", "linesLength": 1, "startIndex": 134, "lines": ["However, you can still make use of the cross_val_score from Scikit-Learn"]}, {"block": 48, "type": "markdown", "linesLength": 1, "startIndex": 135, "lines": ["### Cross validation: H2O and Scikit-Learn"]}, {"block": 49, "type": "code", "linesLength": 4, "startIndex": 136, "lines": ["from sklearn.cross_validation import cross_val_score\n", "from h2o.cross_validation import H2OKFold\n", "from h2o.model.regression import h2o_r2_score\n", "from sklearn.metrics.scorer import make_scorer"]}, {"block": 50, "type": "markdown", "linesLength": 1, "startIndex": 140, "lines": ["You still must use H2O to make the folds. Currently, there is no H2OStratifiedKFold. Additionally, the H2ORandomForestEstimator is similar to the scikit-learn RandomForestRegressor object with its own ``train`` method."]}, {"block": 51, "type": "code", "linesLength": 1, "startIndex": 141, "lines": ["model = H2ORandomForestEstimator(seed=42)"]}, {"block": 52, "type": "code", "linesLength": 6, "startIndex": 142, "lines": ["scorer = make_scorer(h2o_r2_score)   # make h2o_r2_score into a scikit_learn scorer\n", "custom_cv = H2OKFold(fr, n_folds=10, seed=42) # make a cv \n", "scores = cross_val_score(model, fr[x], fr[y], scoring=scorer, cv=custom_cv)\n", "\n", "print(\"Expected R^2: %.2f +/- %.2f \\n\" % (scores.mean(), scores.std()*1.96))\n", "print(\"Scores:\", scores.round(2))"]}, {"block": 53, "type": "markdown", "linesLength": 1, "startIndex": 148, "lines": ["There isn't much difference in the R^2 value since the fold strategy is exactly the same. However, there was a major difference in terms of computation time and memory usage."]}, {"block": 54, "type": "markdown", "linesLength": 1, "startIndex": 149, "lines": ["Since the progress bar print out gets annoying let's disable that"]}, {"block": 55, "type": "code", "linesLength": 2, "startIndex": 150, "lines": ["h2o.__PROGRESS_BAR__=False\n", "h2o.no_progress()"]}, {"block": 56, "type": "markdown", "linesLength": 1, "startIndex": 152, "lines": ["### Grid Search"]}, {"block": 57, "type": "markdown", "linesLength": 1, "startIndex": 153, "lines": ["Grid search in H2O is still under active development and it will be available very soon. However, it is possible to make use of Scikit's grid search infrastructure (with some performance penalties)"]}, {"block": 58, "type": "markdown", "linesLength": 1, "startIndex": 154, "lines": ["### Randomized grid search: H2O and Scikit-Learn"]}, {"block": 59, "type": "code", "linesLength": 3, "startIndex": 155, "lines": ["from sklearn import __version__\n", "sklearn_version = __version__\n", "print(sklearn_version)"]}, {"block": 60, "type": "markdown", "linesLength": 13, "startIndex": 158, "lines": ["If you have 0.16.1, then your system can't handle complex randomized grid searches (it works in every other version of sklearn, including the soon to be released 0.16.2 and the older versions).\n", "\n", "The steps to perform a randomized grid search:\n", "1. Import model and RandomizedSearchCV\n", "2. Define model\n", "3. Specify parameters to test\n", "4. Define grid search object\n", "5. Fit data to grid search object\n", "6. Collect scores\n", "\n", "All the steps will be repeated from above.\n", "\n", "Because 0.16.1 is installed, we use scipy to define specific distributions"]}, {"block": 61, "type": "markdown", "linesLength": 11, "startIndex": 171, "lines": ["ADVANCED TIP:\n", "\n", "Turn off reference counting for spawning jobs in parallel (n_jobs=-1, or n_jobs > 1).\n", "We'll turn it back on again in the aftermath of a Parallel job.\n", "\n", "If you don't want to run jobs in parallel, don't turn off the reference counting.\n", "\n", "Pattern is:\n", "         >>> h2o.turn_off_ref_cnts()\n", "         >>> .... parallel job ....\n", "         >>> h2o.turn_on_ref_cnts()"]}, {"block": 62, "type": "code", "linesLength": 24, "startIndex": 182, "lines": ["%%time\n", "from sklearn.grid_search import RandomizedSearchCV  # Import grid search\n", "from scipy.stats import randint, uniform\n", "\n", "model = H2ORandomForestEstimator(seed=42)        # Define model\n", "\n", "params = {\"ntrees\": randint(20,50),\n", "          \"max_depth\": randint(1,10),\n", "          \"min_rows\": randint(1,10),    # scikit's  min_samples_leaf\n", "          \"mtries\": randint(2,fr[x].shape[1]),} # Specify parameters to test\n", "\n", "scorer = make_scorer(h2o_r2_score)   # make h2o_r2_score into a scikit_learn scorer\n", "custom_cv = H2OKFold(fr, n_folds=10, seed=42) # make a cv \n", "random_search = RandomizedSearchCV(model, params, \n", "                                   n_iter=30, \n", "                                   scoring=scorer, \n", "                                   cv=custom_cv, \n", "                                   random_state=42,\n", "                                   n_jobs=1)       # Define grid search object\n", "\n", "random_search.fit(fr[x], fr[y])\n", "\n", "print(\"Best R^2:\", random_search.best_score_, \"\\n\")\n", "print(\"Best params:\", random_search.best_params_)"]}, {"block": 63, "type": "markdown", "linesLength": 1, "startIndex": 206, "lines": ["We might be tempted to think that we just had a large improvement; however we must be cautious. The function below creates a more detailed report."]}, {"block": 64, "type": "code", "linesLength": 30, "startIndex": 207, "lines": ["def report_grid_score_detail(random_search, charts=True):\n", "    \"\"\"Input fit grid search estimator. Returns df of scores with details\"\"\"\n", "    df_list = []\n", "\n", "    for line in random_search.grid_scores_:\n", "        results_dict = dict(line.parameters)\n", "        results_dict[\"score\"] = line.mean_validation_score\n", "        results_dict[\"std\"] = line.cv_validation_scores.std()*1.96\n", "        df_list.append(results_dict)\n", "\n", "    result_df = pd.DataFrame(df_list)\n", "    result_df = result_df.sort(\"score\", ascending=False)\n", "    \n", "    if charts:\n", "        for col in get_numeric(result_df):\n", "            if col not in [\"score\", \"std\"]:\n", "                plt.scatter(result_df[col], result_df.score)\n", "                plt.title(col)\n", "                plt.show()\n", "\n", "        for col in list(result_df.columns[result_df.dtypes == \"object\"]):\n", "            cat_plot = result_df.score.groupby(result_df[col]).mean()[0]\n", "            cat_plot.sort()\n", "            cat_plot.plot(kind=\"barh\", xlim=(.5, None), figsize=(7, cat_plot.shape[0]/2))\n", "            plt.show()\n", "    return result_df\n", "\n", "def get_numeric(X):\n", "    \"\"\"Return list of numeric dtypes variables\"\"\"\n", "    return X.dtypes[X.dtypes.apply(lambda x: str(x).startswith((\"float\", \"int\", \"bool\")))].index.tolist()"]}, {"block": 65, "type": "code", "linesLength": 1, "startIndex": 237, "lines": ["report_grid_score_detail(random_search).head()"]}, {"block": 66, "type": "markdown", "linesLength": 1, "startIndex": 238, "lines": ["Based on the grid search report, we can narrow the parameters to search and rerun the analysis. The parameters below were chosen after a few runs:"]}, {"block": 67, "type": "code", "linesLength": 21, "startIndex": 239, "lines": ["%%time\n", "\n", "params = {\"ntrees\": randint(30,40),\n", "          \"max_depth\": randint(4,10),\n", "          \"mtries\": randint(4,10),}\n", "\n", "custom_cv = H2OKFold(fr, n_folds=5, seed=42)           # In small datasets, the fold size can have a big\n", "                                                       # impact on the std of the resulting scores. More\n", "random_search = RandomizedSearchCV(model, params,      # folds --> Less examples per fold --> higher \n", "                                   n_iter=10,          # variation per sample\n", "                                   scoring=scorer, \n", "                                   cv=custom_cv, \n", "                                   random_state=43, \n", "                                   n_jobs=1)       \n", "\n", "random_search.fit(fr[x], fr[y])\n", "\n", "print(\"Best R^2:\", random_search.best_score_, \"\\n\")\n", "print(\"Best params:\", random_search.best_params_)\n", "\n", "report_grid_score_detail(random_search)"]}, {"block": 68, "type": "markdown", "linesLength": 1, "startIndex": 260, "lines": ["### Transformations"]}, {"block": 69, "type": "markdown", "linesLength": 1, "startIndex": 261, "lines": ["Rule of machine learning: Don't use your testing data to inform your training data. Unfortunately, this happens all the time when preparing a dataset for the final model. But on smaller datasets, you must be especially careful."]}, {"block": 70, "type": "markdown", "linesLength": 10, "startIndex": 262, "lines": ["At the moment, there are no classes for managing data transformations. On the one hand, this requires the user to tote around some extra state, but on the other, it allows the user to be more explicit about transforming H2OFrames.\n", "\n", "Basic steps:\n", "\n", "0. Remove the response variable from transformations.\n", "1. Import transformer\n", "2. Define transformer\n", "3. Fit train data to transformer\n", "4. Transform test and train data\n", "5. Re-attach the response variable."]}, {"block": 71, "type": "markdown", "linesLength": 3, "startIndex": 272, "lines": ["First let's normalize the data using the means and standard deviations of the training data.\n", "Then let's perform a principal component analysis on the training data and select the top 5 components.\n", "Using these components, let's use them to reduce the train and test design matrices."]}, {"block": 72, "type": "code", "linesLength": 2, "startIndex": 275, "lines": ["from h2o.transforms.preprocessing import H2OScaler\n", "from h2o.transforms.decomposition import H2OPCA"]}, {"block": 73, "type": "markdown", "linesLength": 1, "startIndex": 277, "lines": ["#### Normalize Data: Use the means and standard deviations from the training data."]}, {"block": 74, "type": "code", "linesLength": 2, "startIndex": 278, "lines": ["y_train = train.pop(\"Median_value\")\n", "y_test  = test.pop(\"Median_value\")"]}, {"block": 75, "type": "code", "linesLength": 4, "startIndex": 280, "lines": ["norm = H2OScaler()\n", "norm.fit(train)\n", "X_train_norm = norm.transform(train)\n", "X_test_norm  = norm.transform(test)"]}, {"block": 76, "type": "code", "linesLength": 2, "startIndex": 284, "lines": ["print(X_test_norm.shape)\n", "X_test_norm"]}, {"block": 77, "type": "markdown", "linesLength": 1, "startIndex": 286, "lines": ["Then, we can apply PCA and keep the top 5 components. A user warning is expected here."]}, {"block": 78, "type": "code", "linesLength": 4, "startIndex": 287, "lines": ["pca = H2OPCA(k=5)\n", "pca.fit(X_train_norm)\n", "X_train_norm_pca = pca.transform(X_train_norm)\n", "X_test_norm_pca  = pca.transform(X_test_norm)"]}, {"block": 79, "type": "code", "linesLength": 1, "startIndex": 291, "lines": ["# prop of variance explained by top 5 components?"]}, {"block": 80, "type": "code", "linesLength": 2, "startIndex": 292, "lines": ["print(X_test_norm_pca.shape)\n", "X_test_norm_pca[:5]"]}, {"block": 81, "type": "code", "linesLength": 3, "startIndex": 294, "lines": ["model = H2ORandomForestEstimator(seed=42)\n", "model.train(x=X_train_norm_pca.names, y=y_train.names, training_frame=X_train_norm_pca.cbind(y_train))\n", "y_hat  = model.predict(X_test_norm_pca)"]}, {"block": 82, "type": "code", "linesLength": 1, "startIndex": 297, "lines": ["h2o_r2_score(y_test,y_hat)"]}, {"block": 83, "type": "markdown", "linesLength": 1, "startIndex": 298, "lines": ["Although this is MUCH simpler than keeping track of all of these transformations manually, it gets to be somewhat of a burden when you want to chain together multiple transformers."]}, {"block": 84, "type": "markdown", "linesLength": 1, "startIndex": 299, "lines": ["### Pipelines"]}, {"block": 85, "type": "markdown", "linesLength": 10, "startIndex": 300, "lines": ["\"Tranformers unite!\"\n", "\n", "If your raw data is a mess and you have to perform several transformations before using it, use a pipeline to keep things simple.\n", "\n", "Steps:\n", "\n", "1. Import Pipeline, transformers, and model\n", "2. Define pipeline. The first and only argument is a *list* of *tuples* where the first element of each tuple is a name you give the step and the second element is a defined transformer. The last step is optionally an estimator class (like a RandomForest).\n", "3. Fit the training data to pipeline\n", "4. Either transform or predict the testing data"]}, {"block": 86, "type": "code", "linesLength": 2, "startIndex": 310, "lines": ["from h2o.transforms.preprocessing import H2OScaler\n", "from h2o.transforms.decomposition import H2OPCA"]}, {"block": 87, "type": "code", "linesLength": 9, "startIndex": 312, "lines": ["from sklearn.pipeline import Pipeline                # Import Pipeline <other imports not shown>\n", "model = H2ORandomForestEstimator(seed=42)\n", "pipe = Pipeline([(\"standardize\", H2OScaler()),       # Define pipeline as a series of steps\n", "                 (\"pca\", H2OPCA(k=5)),\n", "                 (\"rf\", model)])                     # Notice the last step is an estimator\n", "\n", "pipe.fit(train, y_train)                             # Fit training data\n", "y_hat = pipe.predict(test)                           # Predict testing data (due to last step being an estimator)\n", "h2o_r2_score(y_test, y_hat)                          # Notice the final score is identical to before"]}, {"block": 88, "type": "markdown", "linesLength": 1, "startIndex": 321, "lines": ["This is so much easier!!!"]}, {"block": 89, "type": "markdown", "linesLength": 1, "startIndex": 322, "lines": ["But, wait a second, we did worse after applying these transformations! We might wonder how different hyperparameters for the transformations impact the final score."]}, {"block": 90, "type": "markdown", "linesLength": 10, "startIndex": 323, "lines": ["### Combining randomized grid search and pipelines\n", "\"Yo dawg, I heard you like models, so I put models in your models to model models.\"\n", "\n", "Steps:\n", "\n", "1. Import Pipeline, grid search, transformers, and estimators <Not shown below>\n", "2. Define pipeline\n", "3. Define parameters to test in the form: \"(Step name)__(argument name)\" A double underscore separates the two words.\n", "4. Define grid search\n", "5. Fit to grid search"]}, {"block": 91, "type": "code", "linesLength": 31, "startIndex": 333, "lines": ["pipe = Pipeline([(\"standardize\", H2OScaler()),\n", "                 (\"pca\", H2OPCA()),\n", "                 (\"rf\", H2ORandomForestEstimator(seed=42))])\n", "\n", "params = {\"standardize__center\":    [True, False],           # Parameters to test\n", "          \"standardize__scale\":     [True, False],\n", "          \"pca__k\":                 randint(2, 6),\n", "          \"rf__ntrees\":             randint(50,80),\n", "          \"rf__max_depth\":          randint(4,10),\n", "          \"rf__min_rows\":           randint(5,10), }\n", "#           \"rf__mtries\":             randint(1,4),}           # gridding over mtries is \n", "                                                               # problematic with pca grid over \n", "                                                               # k above \n", "\n", "from sklearn.grid_search import RandomizedSearchCV\n", "from h2o.cross_validation import H2OKFold\n", "from h2o.model.regression import h2o_r2_score\n", "from sklearn.metrics.scorer import make_scorer\n", "\n", "custom_cv = H2OKFold(fr, n_folds=5, seed=42)\n", "random_search = RandomizedSearchCV(pipe, params,\n", "                                   n_iter=30,\n", "                                   scoring=make_scorer(h2o_r2_score),\n", "                                   cv=custom_cv,\n", "                                   random_state=42,\n", "                                   n_jobs=1)\n", "\n", "\n", "random_search.fit(fr[x],fr[y])\n", "results = report_grid_score_detail(random_search)\n", "results.head()"]}, {"block": 92, "type": "markdown", "linesLength": 4, "startIndex": 364, "lines": ["Currently Under Development (drop-in scikit-learn pieces):\n", "    * Richer set of transforms (only PCA and Scale are implemented)\n", "    * Richer set of estimators (only RandomForest is available)\n", "    * Full H2O Grid Search"]}, {"block": 93, "type": "markdown", "linesLength": 1, "startIndex": 368, "lines": ["### Other Tips: Model Save/Load"]}, {"block": 94, "type": "markdown", "linesLength": 1, "startIndex": 369, "lines": ["It is useful to save constructed models to disk and reload them between H2O sessions. Here's how:"]}, {"block": 95, "type": "code", "linesLength": 2, "startIndex": 370, "lines": ["best_estimator = random_search.best_estimator_                        # fetch the pipeline from the grid search\n", "h2o_model      = h2o.get_model(best_estimator._final_estimator._id)    # fetch the model from the pipeline"]}, {"block": 96, "type": "code", "linesLength": 2, "startIndex": 372, "lines": ["save_path = h2o.save_model(h2o_model, path=\".\", force=True)\n", "print(save_path)"]}, {"block": 97, "type": "code", "linesLength": 2, "startIndex": 374, "lines": ["# assumes new session\n", "my_model = h2o.load_model(path=save_path)"]}, {"block": 98, "type": "code", "linesLength": 1, "startIndex": 376, "lines": ["my_model.predict(X_test_norm_pca)"]}, {"block": 99, "type": "code", "linesLength": 0, "startIndex": 377, "lines": []}]