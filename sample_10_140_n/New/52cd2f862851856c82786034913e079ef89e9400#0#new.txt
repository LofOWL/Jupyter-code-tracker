[{"block": 0, "type": "markdown", "linesLength": 3, "startIndex": 0, "lines": ["<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n", "\n", "<i>Licensed under the MIT License.</i>"]}, {"block": 1, "type": "markdown", "linesLength": 27, "startIndex": 3, "lines": ["# Building a Real-time Recommendation API\n", "\n", "This reference architecture shows the full lifecycle of building a recommendation system. It walks through the creation of appropriate azure resources, training a recommendation model using a Virtual Machine or Databricks, and deploying it as an API. It uses Azure Cosmos DB, Azure Machine Learning, and Azure Kubernetes Service. \n", "\n", "This architecture can be generalized for many recommendation engine scenarios, including recommendations for products, movies, and news. \n", "### Architecture\n", "![architecture](https://camo.githubusercontent.com/35ffc0c1bb61f8cc928f02589941376d3005c82d/68747470733a2f2f7265636f64617461736574732e626c6f622e636f72652e77696e646f77732e6e65742f696d616765732f7265636f2d617263682e706e67 \"Architecture\")\n", "\n", "**Scenario**: A media organization wants to provide movie or video recommendations to its users. By providing personalized recommendations, the organization meets several business goals, including increased click-through rates, increased engagement on site, and higher user satisfaction.\n", "\n", "In this reference, we train and deploy a real-time recommender service API that can provide the top 10 movie recommendations for a given user. \n", "\n", "### Components\n", "This architecture consists of the following key components:\n", "* [Azure Databricks](https://docs.microsoft.com/en-us/azure/azure-databricks/what-is-azure-databricks)<sup>1)</sup> is used as a development environment to prepare input data and train the recommender model on a Spark cluster. Azure Databricks also provides an interactive workspace to run and collaborate on notebooks for any data processing or machine learning tasks.\n", "* [Azure Kubernetes Service](https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes)(AKS) is used to deploy and operationalize a machine learning model service API on a Kubernetes cluster. AKS hosts the containerized model, providing scalability that meets throughput requirements, identity and access management, and logging and health monitoring. \n", "* [Azure Cosmos DB](https://docs.microsoft.com/en-us/azure/cosmos-db/introduction) is a globally distributed database service used to store the top 10 recommended movies for each user. Azure Cosmos DB is ideal for this scenario as it provides low latency (10 ms at 99th percentile) to read the top recommended items for a given user. \n", "* [Azure Machine Learning Service](https://docs.microsoft.com/en-us/azure/machine-learning/service/) is a service used to track and manage machine learning models, and then package and deploy these models to a scalable Azure Kubernetes Service environment.\n", "\n", "<sup>1) Here, we are just giving an example of using Azure Databricks. Any platforms listed in [SETUP](https://github.com/microsoft/recommenders/blob/master/SETUP.md) can be used as well.</sup>\n", "\n", "\n", "### Table of Contents.\n", "0. [File Imports](#0-File-Imports)\n", "1. [Service Creation](#1-Service-Creation)\n", "2. [Training and evaluation](#2-Training)\n", "3. [Operationalization](#3.-Operationalize-the-Recommender-Service)"]}, {"block": 2, "type": "markdown", "linesLength": 4, "startIndex": 30, "lines": ["## Setup\n", "To run this notebook on Azure Databricks, you should setup Azure Databricks by following the appropriate sections in the repository [SETUP instructions](https://github.com/microsoft/recommenders/blob/master/SETUP.md) and import this notebook into your Azure Databricks Workspace (see instructions [here](https://docs.azuredatabricks.net/user-guide/notebooks/notebook-manage.html#import-a-notebook)).\n", "\n", "Please note: This notebook **REQUIRES** that you add the dependencies to support **operationalization**. See [SETUP](https://github.com/microsoft/recommenders/blob/master/SETUP.md) for details.\n"]}, {"block": 3, "type": "markdown", "linesLength": 1, "startIndex": 34, "lines": ["## 0 File Imports"]}, {"block": 4, "type": "code", "linesLength": 34, "startIndex": 35, "lines": ["import os\n", "import sys\n", "sys.path.append(\"../../\")\n", "import time\n", "import urllib\n", "\n", "from azure.common.client_factory import get_client_from_cli_profile\n", "import azure.mgmt.cosmosdb\n", "import azureml.core\n", "from azureml.core import Workspace\n", "from azureml.core.model import Model\n", "from azureml.core.compute import AksCompute, ComputeTarget\n", "from azureml.core.compute_target import ComputeTargetException\n", "from azureml.core.webservice import Webservice, AksWebservice\n", "from azureml.exceptions import WebserviceException\n", "from azureml.core import Environment\n", "from azureml.core.environment import CondaDependencies\n", "from azureml.core.model import InferenceConfig\n", "from azureml.core.environment import SparkPackage\n", "import pydocumentdb.document_client as document_client\n", "from pyspark.ml.recommendation import ALS\n", "from pyspark.sql.types import StructType, StructField\n", "from pyspark.sql.types import FloatType, IntegerType, LongType\n", "\n", "from reco_utils.common.timer import Timer\n", "from reco_utils.common.spark_utils import start_or_get_spark\n", "from reco_utils.dataset import movielens\n", "from reco_utils.dataset.cosmos_cli import find_collection, read_collection, read_database, find_database\n", "from reco_utils.dataset.download_utils import maybe_download\n", "from reco_utils.dataset.spark_splitters import spark_random_split\n", "from reco_utils.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation\n", "from reco_utils.common.notebook_utils import is_databricks\n", "\n", "print(\"Azure SDK version:\", azureml.core.VERSION)"]}, {"block": 5, "type": "markdown", "linesLength": 1, "startIndex": 69, "lines": ["In JupyterHub, environment variables defined in `.../etc/conda/activate.d` may not be activated. If so, run the following cell to set PySpark environment variables. Make sure your conda environment path is `/anaconda/envs/reco_pyspark` or change the paths in the script."]}, {"block": 6, "type": "code", "linesLength": 2, "startIndex": 70, "lines": ["os.environ[\"PYSPARK_PYTHON\"]=\"/anaconda/envs/reco_pyspark/bin/python\"\n", "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/anaconda/envs/reco_pyspark/bin/python\""]}, {"block": 7, "type": "code", "linesLength": 10, "startIndex": 72, "lines": ["# Start spark session if needed\n", "if not is_databricks():\n", "    cosmos_connector = (\n", "        \"https://search.maven.org/remotecontent?filepath=com/microsoft/azure/\"\n", "        \"azure-cosmosdb-spark_2.3.0_2.11/1.3.3/azure-cosmosdb-spark_2.3.0_2.11-1.3.3-uber.jar\"\n", "    )\n", "    jar_filepath = maybe_download(url=cosmos_connector, filename=\"cosmos.jar\")\n", "    spark = start_or_get_spark(\"ALS\", memory=\"10g\", jars=[jar_filepath])\n", "    sc = spark.sparkContext\n", "display(sc)"]}, {"block": 8, "type": "markdown", "linesLength": 12, "startIndex": 82, "lines": ["## 1 Service Creation\n", "Modify the **Subscription ID** to the subscription you would like to deploy to and set the resource name variables.\n", "\n", "#### Services created by this notebook:\n", "1. [Azure ML Service](https://azure.microsoft.com/en-us/services/machine-learning-service/)\n", "1. [Azure Cosmos DB](https://azure.microsoft.com/en-us/services/cosmos-db/)\n", "1. [Azure Container Registery](https://docs.microsoft.com/en-us/azure/container-registry/)\n", "1. [Azure Container Instances](https://docs.microsoft.com/en-us/azure/container-instances/)\n", "1. [Azure Application Insights](https://azure.microsoft.com/en-us/services/monitor/)\n", "1. [Azure Storage](https://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview)\n", "1. [Azure Key Vault](https://azure.microsoft.com/en-us/services/key-vault/)\n", "1. [Azure Kubernetes Service (AKS)](https://azure.microsoft.com/en-us/services/kubernetes-service/)"]}, {"block": 9, "type": "markdown", "linesLength": 1, "startIndex": 94, "lines": ["**Add your Azure subscription ID**"]}, {"block": 10, "type": "code", "linesLength": 12, "startIndex": 95, "lines": ["# Add your subscription ID\n", "subscription_id = \"\"\n", "\n", "# Set your workspace name\n", "workspace_name = \"o16n-test\"\n", "resource_group = \"{}-rg\".format(workspace_name)\n", "\n", "# Set your region to deploy Azure ML workspace\n", "location = \"eastus\"\n", "\n", "# AzureML service and Azure Kubernetes Service prefix\n", "service_name = \"mvl-als\""]}, {"block": 11, "type": "code", "linesLength": 2, "startIndex": 107, "lines": ["# Login for Azure CLI so that AzureML can use Azure CLI login credentials\n", "!az login"]}, {"block": 12, "type": "code", "linesLength": 2, "startIndex": 109, "lines": ["# Change subscription if needed\n", "!az account set --subscription {subscription_id}"]}, {"block": 13, "type": "code", "linesLength": 2, "startIndex": 111, "lines": ["# Check account\n", "!az account show"]}, {"block": 14, "type": "code", "linesLength": 11, "startIndex": 113, "lines": ["# CosmosDB\n", "account_name = \"{}-ds-sql\".format(workspace_name)\n", "# account_name for CosmosDB cannot have \"_\" and needs to be less than 31 chars\n", "account_name = account_name.replace(\"_\", \"-\")[:31]\n", "cosmos_database = \"recommendations\"\n", "cosmos_collection = \"user_recommendations_als\"\n", "\n", "# AzureML resource names\n", "model_name = \"{}-reco.mml\".format(service_name)\n", "aks_name = \"{}-aks\".format(service_name)\n", "container_image_name = \"{}-img\".format(service_name)"]}, {"block": 15, "type": "code", "linesLength": 5, "startIndex": 124, "lines": ["# top k items to recommend\n", "TOP_K = 10\n", "\n", "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n", "MOVIELENS_DATA_SIZE = '100k'"]}, {"block": 16, "type": "code", "linesLength": 6, "startIndex": 129, "lines": ["userCol = \"UserId\"\n", "itemCol = \"MovieId\"\n", "ratingCol = \"Rating\"\n", "\n", "train_data_path = \"train\"\n", "test_data_path = \"test\""]}, {"block": 17, "type": "markdown", "linesLength": 2, "startIndex": 135, "lines": ["### 1.1 Import or create the AzureML Workspace. \n", "This command will check if the AzureML Workspace exists or not, and will create the workspace if it doesn't exist."]}, {"block": 18, "type": "code", "linesLength": 7, "startIndex": 137, "lines": ["ws = Workspace.create(\n", "    name=workspace_name,\n", "    subscription_id=subscription_id,\n", "    resource_group=resource_group, \n", "    location=location,\n", "    exist_ok=True\n", ")"]}, {"block": 19, "type": "markdown", "linesLength": 3, "startIndex": 144, "lines": ["### 1.2 Create a Cosmos DB to store recommendation results\n", "\n", "This step will take some time to create CosmosDB resources."]}, {"block": 20, "type": "code", "linesLength": 58, "startIndex": 147, "lines": ["# explicitly pass subscription_id in case user has multiple subscriptions\n", "client = get_client_from_cli_profile(\n", "    azure.mgmt.cosmosdb.CosmosDB,\n", "    subscription_id=subscription_id\n", ")\n", "\n", "async_cosmosdb_create = client.database_accounts.create_or_update(\n", "    resource_group,\n", "    account_name,\n", "    {\n", "        'location': location,\n", "        'locations': [{\n", "            'location_name': location\n", "        }]\n", "    }\n", ")\n", "account = async_cosmosdb_create.result()\n", "\n", "my_keys = client.database_accounts.list_keys(resource_group, account_name)\n", "master_key = my_keys.primary_master_key\n", "endpoint = \"https://\" + account_name + \".documents.azure.com:443/\"\n", "\n", "# DB client\n", "client = document_client.DocumentClient(endpoint, {'masterKey': master_key})\n", "\n", "if not find_database(client, cosmos_database):\n", "    db = client.CreateDatabase({'id': cosmos_database })\n", "    print(\"Database created\")\n", "else:\n", "    db = read_database(client, cosmos_database)\n", "    print(\"Database found\")\n", "\n", "# Create collection options\n", "options = dict(offerThroughput=11000)\n", "\n", "# Create a collection\n", "collection_definition = {\n", "    'id': cosmos_collection,\n", "    'partitionKey': {'paths': ['/id'],'kind': 'Hash'}\n", "}\n", "if not find_collection(client, cosmos_database, cosmos_collection):\n", "    collection = client.CreateCollection(\n", "        db['_self'], \n", "        collection_definition,\n", "        options\n", "    )\n", "    print(\"Collection created\")\n", "else:\n", "    collection = read_collection(client, cosmos_database, cosmos_collection)\n", "    print(\"Collection found\")\n", "    \n", "dbsecrets = dict(\n", "    Endpoint=endpoint, \n", "    Masterkey=master_key, \n", "    Database=cosmos_database, \n", "    Collection=cosmos_collection, \n", "    Upsert=True\n", ")"]}, {"block": 21, "type": "markdown", "linesLength": 5, "startIndex": 205, "lines": ["## 2 Training\n", "\n", "Next, we train an [Alternating Least Squares model](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html) on [MovieLens](https://grouplens.org/datasets/movielens/) dataset.\n", "\n", "### 2.1 Download the MovieLens dataset"]}, {"block": 22, "type": "code", "linesLength": 11, "startIndex": 210, "lines": ["# Note: The DataFrame-based API for ALS currently only supports integers for user and item ids.\n", "schema = StructType(\n", "    (\n", "        StructField(userCol, IntegerType()),\n", "        StructField(itemCol, IntegerType()),\n", "        StructField(ratingCol, FloatType()),\n", "    )\n", ")\n", "\n", "data = movielens.load_spark_df(spark, size=MOVIELENS_DATA_SIZE, schema=schema)\n", "data.show()"]}, {"block": 23, "type": "markdown", "linesLength": 2, "startIndex": 221, "lines": ["### 2.2 Split the data into train, test\n", "There are several ways of splitting the data: random, chronological, stratified, etc., each of which favors a different real-world evaluation use case. We will split randomly in this example \u2013 for more details on which splitter to choose, consult [this guide](https://github.com/Microsoft/Recommenders/blob/master/notebooks/01_data/data_split.ipynb)."]}, {"block": 24, "type": "code", "linesLength": 3, "startIndex": 223, "lines": ["train, test = spark_random_split(data, ratio=0.75, seed=42)\n", "print(\"N train\", train.cache().count())\n", "print(\"N test\", test.cache().count())"]}, {"block": 25, "type": "markdown", "linesLength": 5, "startIndex": 226, "lines": ["### 2.3 Train the ALS model on the training data\n", "\n", "To predict movie ratings, we use the rating data in the training set as users' explicit feedback. The hyperparameters used to estimate the model are set based on [this page](http://mymedialite.net/examples/datasets.html).\n", "\n", "Under most circumstances, you would explore the hyperparameters and choose an optimal set based on some criteria. For additional details on this process, please see additional information in the deep dives [here](https://github.com/microsoft/recommenders/blob/master/notebooks/04_model_select_and_optimize/tuning_spark_als.ipynb)."]}, {"block": 26, "type": "code", "linesLength": 12, "startIndex": 231, "lines": ["als = ALS(\n", "    rank=10,\n", "    maxIter=15,\n", "    implicitPrefs=False,\n", "    alpha=0.1,\n", "    regParam=0.05,\n", "    coldStartStrategy='drop',\n", "    nonnegative=True,\n", "    userCol=userCol,\n", "    itemCol=itemCol,\n", "    ratingCol=ratingCol,\n", ")"]}, {"block": 27, "type": "code", "linesLength": 1, "startIndex": 243, "lines": ["model = als.fit(train)"]}, {"block": 28, "type": "markdown", "linesLength": 5, "startIndex": 244, "lines": ["### 2.4 Get top-k recommendations for our testing data\n", "\n", "In the movie recommendation use case, recommending movies that have been rated by the users do not make sense. Therefore, the rated movies are removed from the recommended items.\n", "\n", "In order to achieve this, we recommend all movies to all users, and then remove the user-movie pairs that exist in the training dataset."]}, {"block": 29, "type": "code", "linesLength": 6, "startIndex": 249, "lines": ["# Get the cross join of all user-item pairs and score them.\n", "users = train.select(userCol).distinct()\n", "items = train.select(itemCol).distinct()\n", "user_item = users.crossJoin(items)\n", "dfs_pred = model.transform(user_item)\n", "dfs_pred.show()"]}, {"block": 30, "type": "code", "linesLength": 10, "startIndex": 255, "lines": ["# Remove seen items.\n", "dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n", "    train.alias(\"train\"),\n", "    (dfs_pred[userCol]==train[userCol]) & (dfs_pred[itemCol]==train[itemCol]),\n", "    how='outer'\n", ")\n", "top_all = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[\"train.\"+ratingCol].isNull()) \\\n", "    .select(\"pred.\"+userCol, \"pred.\"+itemCol, \"pred.prediction\")\n", "\n", "top_all.show()"]}, {"block": 31, "type": "markdown", "linesLength": 3, "startIndex": 265, "lines": ["### 2.5 Evaluate how well ALS performs\n", "\n", "Evaluate model performance using metrics such as Precision@K, Recall@K, [MAP@K](https://en.wikipedia.org/wiki/Evaluation_measures_\\(information_retrieval\\) or [nDCG@K](https://en.wikipedia.org/wiki/Discounted_cumulative_gain). For a full guide on what metrics to evaluate your recommender with, consult [this guide](https://github.com/Microsoft/Recommenders/blob/master/notebooks/03_evaluate/evaluation.ipynb)."]}, {"block": 32, "type": "code", "linesLength": 8, "startIndex": 268, "lines": ["cols = {\n", "    'col_user': userCol,\n", "    'col_item': itemCol,\n", "    'col_rating': ratingCol,\n", "    'col_prediction': \"prediction\",\n", "}\n", "\n", "test.show()"]}, {"block": 33, "type": "code", "linesLength": 16, "startIndex": 276, "lines": ["# Evaluate Ranking Metrics\n", "rank_eval = SparkRankingEvaluation(\n", "    test, \n", "    top_all, \n", "    k=TOP_K,\n", "    **cols\n", ")\n", "\n", "print(\n", "    \"Model:\\tALS\",\n", "    \"Top K:\\t%d\" % rank_eval.k,\n", "    \"MAP:\\t%f\" % rank_eval.map_at_k(),\n", "    \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n", "    \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n", "    \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), sep='\\n'\n", ")"]}, {"block": 34, "type": "code", "linesLength": 15, "startIndex": 292, "lines": ["# Evaluate Rating Metrics\n", "prediction = model.transform(test)\n", "rating_eval = SparkRatingEvaluation(\n", "    test, \n", "    prediction, \n", "    **cols\n", ")\n", "\n", "print(\n", "    \"Model:\\tALS rating prediction\",\n", "    \"RMSE:\\t%.2f\" % rating_eval.rmse(),\n", "    \"MAE:\\t%f\" % rating_eval.mae(),\n", "    \"Explained variance:\\t%f\" % rating_eval.exp_var(),\n", "    \"R squared:\\t%f\" % rating_eval.rsquared(), sep='\\n'\n", ")"]}, {"block": 35, "type": "markdown", "linesLength": 1, "startIndex": 307, "lines": ["### 2.6 Save the model"]}, {"block": 36, "type": "code", "linesLength": 4, "startIndex": 308, "lines": ["(model\n", " .write()\n", " .overwrite()\n", " .save(model_name))"]}, {"block": 37, "type": "markdown", "linesLength": 2, "startIndex": 312, "lines": ["## 3. Operationalize the Recommender Service\n", "Once the model is built with desirable performance, it will be operationalized to run as a REST endpoint to be utilized by a real time service. We will utilize [Azure Cosmos DB](https://azure.microsoft.com/en-us/services/cosmos-db/), [Azure Machine Learning Service](https://azure.microsoft.com/en-us/services/machine-learning-service/), and [Azure Kubernetes Service](https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes) to operationalize the recommender service."]}, {"block": 38, "type": "markdown", "linesLength": 3, "startIndex": 314, "lines": ["### 3.1 Create a look-up for Recommendations in Cosmos DB\n", "\n", "First, the Top-10 recommendations for each user as predicted by the model are stored as a lookup table in Cosmos DB. At runtime, the service will return the Top-10 recommendations as precomputed and stored in Cosmos DB:"]}, {"block": 39, "type": "code", "linesLength": 4, "startIndex": 317, "lines": ["recs = model.recommendForAllUsers(10)\n", "recs_topk = recs.withColumn(\"id\", recs[userCol].cast(\"string\")) \\\n", "    .select(\"id\", \"recommendations.\" + itemCol)\n", "recs_topk.show()"]}, {"block": 40, "type": "code", "linesLength": 7, "startIndex": 321, "lines": ["# Save data to CosmosDB\n", "(recs_topk.coalesce(1)\n", " .write\n", " .format(\"com.microsoft.azure.cosmosdb.spark\")\n", " .mode('overwrite')\n", " .options(**dbsecrets)\n", " .save())"]}, {"block": 41, "type": "markdown", "linesLength": 5, "startIndex": 328, "lines": ["### 3.2 Configure Azure Machine Learning\n", "\n", "Next, Azure Machine Learning Service is used to create a model scoring image and deploy it to Azure Kubernetes Service as a scalable containerized service. To achieve this, a **scoring script** and an **environment config** should be created. The following shows the content of the two files.  \n", "\n", "In the scoring script, we make a call to Cosmos DB to lookup the top 10 movies to recommend given an input User ID:"]}, {"block": 42, "type": "code", "linesLength": 33, "startIndex": 333, "lines": ["score_sparkml = \"\"\"\n", "import json\n", "import pydocumentdb.document_client as document_client\n", "\n", "def init(local=False):\n", "    global client, collection\n", "    try:\n", "        client = document_client.DocumentClient('{endpoint}', dict(masterKey='{key}'))\n", "        collection = client.ReadCollection(collection_link='dbs/{database}/colls/{collection}')\n", "    except Exception as e:\n", "        collection = e\n", "\n", "def run(input_json):\n", "    try:\n", "        # Query them in SQL\n", "        id = str(json.loads(json.loads(input_json)[0])['id'])\n", "        query = dict(query='SELECT * FROM c WHERE c.id = \"' + id +'\"')\n", "        options = dict(partitionKey=str(id))\n", "        document_link = 'dbs/{database}/colls/{collection}/docs/' + id\n", "        result = client.ReadDocument(document_link, options);  \n", "    except Exception as e:\n", "        result = str(e)\n", "    return json.dumps(str(result))\n", "\"\"\".format(key=dbsecrets['Masterkey'], \n", "           endpoint=dbsecrets['Endpoint'], \n", "           database=dbsecrets['Database'], \n", "           collection=dbsecrets['Collection'])\n", "\n", "# test validity of python string\n", "exec(score_sparkml)\n", "\n", "with open(\"score_sparkml.py\", \"w\") as file:\n", "    file.write(score_sparkml)"]}, {"block": 43, "type": "markdown", "linesLength": 1, "startIndex": 366, "lines": ["Register your model:"]}, {"block": 44, "type": "code", "linesLength": 8, "startIndex": 367, "lines": ["mymodel = Model.register(\n", "    model_path=model_name,  # this points to a local file\n", "    model_name=model_name,  # this is the name the model is registered as\n", "    description=\"AML trained model\",\n", "    workspace=ws\n", ")\n", "\n", "print(mymodel.name, mymodel.description, mymodel.version)"]}, {"block": 45, "type": "markdown", "linesLength": 1, "startIndex": 375, "lines": ["### 3.3 Deploy the model as a Service on AKS"]}, {"block": 46, "type": "markdown", "linesLength": 1, "startIndex": 376, "lines": ["#### 3.3.1 Create an Environment for your model:"]}, {"block": 47, "type": "code", "linesLength": 25, "startIndex": 377, "lines": ["env = Environment(name='sparkmlenv')\n", "\n", "# Specify a public image from microsoft/mmlspark as base image\n", "env.docker.base_image=\"microsoft/mmlspark:0.15\"\n", "\n", "pip = [\n", "    'azureml-defaults', \n", "    'numpy==1.14.2', \n", "    'scikit-learn==0.19.1', \n", "    'pandas', \n", "    'pydocumentdb'\n", "]\n", "\n", "# Add dependencies needed for inferencing\n", "env.python.conda_dependencies = CondaDependencies.create(pip_packages=pip)\n", "env.inferencing_stack_version = \"latest\"\n", "\n", "# Add spark packages\n", "env.spark.precache_packages = True\n", "env.spark.repositories = [\"https://mmlspark.azureedge.net/maven\"]\n", "env.spark.packages= [\n", "    SparkPackage(\"com.microsoft.ml.spark\", \"mmlspark_2.11\", \"0.15\"),\n", "    SparkPackage(\"com.microsoft.azure\", artifact=\"azure-storage\", version=\"2.0.0\"),\n", "    SparkPackage(group=\"org.apache.hadoop\", artifact=\"hadoop-azure\", version=\"2.7.0\")\n", "]"]}, {"block": 48, "type": "markdown", "linesLength": 2, "startIndex": 402, "lines": ["#### 3.3.2 Create an AKS Cluster to run your container\n", "This may take 20 to 30 minutes depending on the cluster size."]}, {"block": 49, "type": "code", "linesLength": 15, "startIndex": 404, "lines": ["# Verify that cluster does not exist already\n", "try:\n", "    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n", "    print(\"Found existing cluster, use it.\")\n", "except ComputeTargetException:\n", "    # Create the cluster using the default configuration (can also provide parameters to customize)\n", "    prov_config = AksCompute.provisioning_configuration()\n", "    aks_target = ComputeTarget.create(\n", "        workspace=ws, \n", "        name=aks_name, \n", "        provisioning_configuration=prov_config\n", "    )\n", "    aks_target.wait_for_completion(show_output = True)\n", "    print(aks_target.provisioning_state)\n", "    # To check any error logs, print(aks_target.provisioning_errors)"]}, {"block": 50, "type": "markdown", "linesLength": 1, "startIndex": 419, "lines": ["#### 3.3.3 Deploy the container image to AKS:"]}, {"block": 51, "type": "code", "linesLength": 24, "startIndex": 420, "lines": ["# Create an Inferencing Configuration with your environment and scoring script\n", "inference_config = InferenceConfig(\n", "    environment=env,\n", "    entry_script=\"score_sparkml.py\"\n", ")\n", "\n", "# Set the web service configuration (using default here with app insights)\n", "aks_config = AksWebservice.deploy_configuration(enable_app_insights=True)\n", "\n", "# Webservice creation using single command\n", "try:\n", "    aks_service = Model.deploy(\n", "        workspace=ws,\n", "        models=[mymodel],\n", "        name=service_name,\n", "        inference_config=inference_config,\n", "        deployment_config=aks_config,\n", "        deployment_target=aks_target\n", "    )\n", "    aks_service.wait_for_deployment(show_output=True)\n", "except WebserviceException:\n", "    # Retrieve existing service.\n", "    aks_service = Webservice(ws, name=service_name)\n", "    print(\"Retrieved existing service\")"]}, {"block": 52, "type": "markdown", "linesLength": 3, "startIndex": 444, "lines": ["### 3.4 Call the AKS model service\n", "After the deployment, the service can be called with a user ID \u2013 the service will then look up the top 10 recommendations for that user in Cosmos DB and send back the results.\n", "The following script demonstrates how to call the recommendation service API and view the result for the given user ID:"]}, {"block": 53, "type": "code", "linesLength": 24, "startIndex": 447, "lines": ["import json\n", "\n", "scoring_url = aks_service.scoring_uri\n", "service_key = aks_service.get_keys()[0]\n", "\n", "input_data = '[\"{\\\\\"id\\\\\":\\\\\"496\\\\\"}\"]'.encode()\n", "\n", "req = urllib.request.Request(scoring_url,data=input_data)\n", "req.add_header(\"Authorization\",\"Bearer {}\".format(service_key))\n", "req.add_header(\"Content-Type\",\"application/json\")\n", "\n", "with Timer() as t: \n", "    with urllib.request.urlopen(req) as result:\n", "        res = result.read()\n", "        resj = json.loads(\n", "            # Cleanup to parse into a json object\n", "            res.decode(\"utf-8\")\n", "            .replace(\"\\\\\", \"\")\n", "            .replace('\"', \"\")\n", "            .replace(\"'\", '\"')\n", "        )\n", "        print(json.dumps(resj, indent=4))\n", "    \n", "print(\"Full run took %.2f seconds\" % t.interval)"]}, {"block": 54, "type": "code", "linesLength": 0, "startIndex": 471, "lines": []}]