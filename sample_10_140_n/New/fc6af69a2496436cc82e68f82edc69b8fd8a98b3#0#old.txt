[{"block": 0, "type": "code", "linesLength": 2, "startIndex": 0, "lines": ["import sys\n", "sys.path.insert(0, '../')"]}, {"block": 1, "type": "code", "linesLength": 2, "startIndex": 2, "lines": ["%load_ext autoreload\n", "%autoreload 2"]}, {"block": 2, "type": "code", "linesLength": 2, "startIndex": 4, "lines": ["from fastai.imports import *\n", "from fastai.torch_imports import *"]}, {"block": 3, "type": "markdown", "linesLength": 1, "startIndex": 6, "lines": ["# All the Linear Algebra You Need for AI"]}, {"block": 4, "type": "markdown", "linesLength": 1, "startIndex": 7, "lines": ["## fast.ai teaching philosophy"]}, {"block": 5, "type": "markdown", "linesLength": 10, "startIndex": 8, "lines": ["<img src=\"images/demba_combustion_engine.png\" alt=\"\" style=\"width: 60%\"/>\n", "<center>\n", "(source: [Demba Ba](https://github.com/zalandoresearch/fashion-mnist) and [Arvind Nagaraj](https://medium.com/towards-data-science/thoughts-after-taking-the-deeplearning-ai-courses-8568f132153))\n", "</center>\n", "Top-down, the whole game\n", "\n", "**Plan today**\n", "- neural net with nn.torch\n", "- code a neural net ourselves\n", "- look under the hood at broadcasting & matrix multiplication"]}, {"block": 6, "type": "markdown", "linesLength": 1, "startIndex": 18, "lines": ["## Datasets"]}, {"block": 7, "type": "markdown", "linesLength": 3, "startIndex": 19, "lines": ["**Today**: MNIST\n", "\n", "<img src=\"images/mnist.png\" alt=\"\" style=\"width: 60%\"/>"]}, {"block": 8, "type": "markdown", "linesLength": 1, "startIndex": 22, "lines": ["### Other datasets"]}, {"block": 9, "type": "markdown", "linesLength": 25, "startIndex": 23, "lines": ["**Fashion MNIST**\n", "\n", "<img src=\"images/fashion-mnist.png\" alt=\"\" style=\"width: 70%\"/>\n", "<center>\n", "(source: [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist))\n", "</center>\n", "\n", "**CIFAR 10**\n", "\n", "<img src=\"images/cifar10.png\" alt=\"\" style=\"width: 70%\"/>\n", "<center>\n", "(source: [Cifar 10](https://github.com/zalandoresearch/fashion-mnist))\n", "</center>\n", "\n", "[PyTorch](http://pytorch.org/) is a Python framework for tensors and dynamic neural networks with GPU acceleration.  Many of the core contributors work on Facebook's AI team.  In many ways, it is similar to Numpy, only with the increased parallelization of using a GPU.\n", "\n", "From the [PyTorch documentation](http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html):\n", "\n", "<img src=\"images/what_is_pytorch.png\" alt=\"pytorch\" style=\"width: 80%\"/>\n", "\n", "**Further learning**: If you are curious to learn what *dynamic* neural networks are, you may want to watch [this talk](https://www.youtube.com/watch?v=Z15cBAuY7Sc) by Soumith Chintala, Facebook AI researcher and core PyTorch contributor.\n", "\n", "If you want to learn more PyTorch, you can try this [tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) or this [learning by examples](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html).\n", "\n", "**Note about GPUs**: If you are not using a GPU, you will need to remove the `.cuda()` from the methods below. GPU usage is not required for this course, but I thought it would be of interest to some of you.  To learn how to create an AWS instance with a GPU, you can watch the [fast.ai setup lesson](http://course.fast.ai/lessons/aws.html).]"]}, {"block": 10, "type": "markdown", "linesLength": 1, "startIndex": 48, "lines": ["## Getting Started"]}, {"block": 11, "type": "markdown", "linesLength": 1, "startIndex": 49, "lines": ["### Data and Imports"]}, {"block": 12, "type": "code", "linesLength": 1, "startIndex": 50, "lines": ["from fastai.io import *"]}, {"block": 13, "type": "code", "linesLength": 1, "startIndex": 51, "lines": ["path = '../data/'"]}, {"block": 14, "type": "code", "linesLength": 5, "startIndex": 52, "lines": ["URL='http://deeplearning.net/data/mnist/'\n", "FILENAME='mnist.pkl.gz'\n", "\n", "def load_mnist(filename):\n", "    return pickle.load(gzip.open(filename, 'rb'), encoding='latin-1')"]}, {"block": 15, "type": "code", "linesLength": 2, "startIndex": 57, "lines": ["get_data(URL+FILENAME, path+FILENAME)\n", "((x, y), (x_valid, y_valid), (x_test, y_test)) = load_mnist(path+FILENAME)"]}, {"block": 16, "type": "markdown", "linesLength": 1, "startIndex": 59, "lines": ["One of the challenges in training neural networks is keeping your numbers from exploding (going to infinity) or vanishing (going to zero).  There are several different ways to add normalization to address this."]}, {"block": 17, "type": "markdown", "linesLength": 1, "startIndex": 60, "lines": ["#### Calculate Mean and Standard Deviation in order to normalize"]}, {"block": 18, "type": "code", "linesLength": 2, "startIndex": 61, "lines": ["mean = x.mean()\n", "std = x.std()"]}, {"block": 19, "type": "code", "linesLength": 2, "startIndex": 63, "lines": ["x=(x-mean)/std\n", "x.mean(), x.std()"]}, {"block": 20, "type": "code", "linesLength": 2, "startIndex": 65, "lines": ["x_valid = (x_valid-mean)/std\n", "x_valid.mean(), x_valid.std()"]}, {"block": 21, "type": "markdown", "linesLength": 1, "startIndex": 67, "lines": ["#### Helper methods"]}, {"block": 22, "type": "code", "linesLength": 7, "startIndex": 68, "lines": ["%matplotlib inline\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "def show(img, title=None):\n", "    plt.imshow(img, interpolation='none', cmap=\"gray\")\n", "    if title is not None: plt.title(title)"]}, {"block": 23, "type": "code", "linesLength": 8, "startIndex": 75, "lines": ["def plots(ims, figsize=(12,6), rows=2, titles=None):\n", "    f = plt.figure(figsize=figsize)\n", "    cols = len(ims)//rows\n", "    for i in range(len(ims)):\n", "        sp = f.add_subplot(rows, cols, i+1)\n", "        sp.axis('Off')\n", "        if titles is not None: sp.set_title(titles[i], fontsize=16)\n", "        plt.imshow(ims[i], interpolation='none', cmap='gray')"]}, {"block": 24, "type": "markdown", "linesLength": 1, "startIndex": 83, "lines": ["### Look at the data"]}, {"block": 25, "type": "markdown", "linesLength": 1, "startIndex": 84, "lines": ["In any sort of data science work, it's important to look at your data, to make sure you understand the format, how it's stored, what type of values it holds, etc. To make it easier to work with, let's reshape it into 2d images from the flattened 1d format."]}, {"block": 26, "type": "code", "linesLength": 1, "startIndex": 85, "lines": ["x_imgs = np.reshape(x_valid, (-1,28,28)); x_imgs.shape"]}, {"block": 27, "type": "markdown", "linesLength": 1, "startIndex": 86, "lines": ["We can look at part of an image:"]}, {"block": 28, "type": "code", "linesLength": 1, "startIndex": 87, "lines": ["x_imgs[0,10:15,10:15]"]}, {"block": 29, "type": "code", "linesLength": 1, "startIndex": 88, "lines": ["show(x_imgs[0], y[0])"]}, {"block": 30, "type": "markdown", "linesLength": 1, "startIndex": 89, "lines": ["It's the digit 5!  And that's stored in the y value:"]}, {"block": 31, "type": "code", "linesLength": 1, "startIndex": 90, "lines": ["y[0]"]}, {"block": 32, "type": "code", "linesLength": 1, "startIndex": 91, "lines": ["show(x_imgs[0,10:15,10:15])"]}, {"block": 33, "type": "code", "linesLength": 1, "startIndex": 92, "lines": ["plots(x_imgs[:8], titles=y[:8])"]}, {"block": 34, "type": "markdown", "linesLength": 1, "startIndex": 93, "lines": ["## Neural Net (with nn.torch)"]}, {"block": 35, "type": "code", "linesLength": 6, "startIndex": 94, "lines": ["from fastai.metrics import *\n", "from fastai.model import *\n", "from fastai.dataset import *\n", "from fastai.core import *\n", "\n", "import torch.nn as nn"]}, {"block": 36, "type": "code", "linesLength": 1, "startIndex": 100, "lines": ["md = ImageClassifierData.from_arrays(path, (x,y), (x_valid, y_valid))"]}, {"block": 37, "type": "markdown", "linesLength": 5, "startIndex": 101, "lines": ["We will start with a simple neural network.  Neural networks typically consist of linear layers alternating with non-linear layers.\n", "\n", "Each input is a vector of size $28\\times 28$ pixels and our output is of size $10$ (since there are 10 digits: 0, 1, ..., 9).\n", "\n", "I just chose $256$ as the number of hidden states, you could change this to something else."]}, {"block": 38, "type": "code", "linesLength": 5, "startIndex": 106, "lines": ["net = nn.Sequential(\n", "    nn.Linear(28*28, 256),\n", "    nn.ReLU(),\n", "    nn.Linear(256, 10)\n", ").cuda()"]}, {"block": 39, "type": "markdown", "linesLength": 3, "startIndex": 111, "lines": ["- **Loss**: what function is the optimizer trying to minimize?  We need to say how we're defining the error.\n", "- **Optimizer**: algorithm for finding the minimum. typically these are variations on *stochastic gradient descent*, involve taking a step that appears to be the right direction based on the gradient\n", "- **Metrics**: other calculations you want to see as you train"]}, {"block": 40, "type": "code", "linesLength": 3, "startIndex": 114, "lines": ["loss=F.cross_entropy\n", "metrics=[accuracy]\n", "opt=optim.Adam(net.parameters())"]}, {"block": 41, "type": "code", "linesLength": 1, "startIndex": 117, "lines": ["fit(net, md, epochs=1, crit=loss, opt=opt, metrics=metrics)"]}, {"block": 42, "type": "code", "linesLength": 1, "startIndex": 118, "lines": ["preds = predict(net, md.val_dl)"]}, {"block": 43, "type": "code", "linesLength": 1, "startIndex": 119, "lines": ["preds[0]"]}, {"block": 44, "type": "code", "linesLength": 1, "startIndex": 120, "lines": ["preds = preds.max(1)[1]"]}, {"block": 45, "type": "markdown", "linesLength": 1, "startIndex": 121, "lines": ["Let's see how some of our preditions look!"]}, {"block": 46, "type": "code", "linesLength": 1, "startIndex": 122, "lines": ["plots(x_imgs[:8], titles=preds[:8])"]}, {"block": 47, "type": "markdown", "linesLength": 1, "startIndex": 123, "lines": ["## Coding the Neural Net ourselves"]}, {"block": 48, "type": "markdown", "linesLength": 1, "startIndex": 124, "lines": ["A Tensor is a *multi-dimensional matrix containing elements of a single data type*: a group of data, all with the same type (e.g. A Tensor could store a 4 x 4 x 6 matrix of 32-bit signed integers)."]}, {"block": 49, "type": "code", "linesLength": 2, "startIndex": 125, "lines": ["from torch.autograd import Variable\n", "def get_weights(*dims): return nn.Parameter(torch.randn(*dims)/dims[0])"]}, {"block": 50, "type": "markdown", "linesLength": 1, "startIndex": 127, "lines": ["We will define the neural network ourselves.  Forward describes how the neural net converts inputs --> outputs"]}, {"block": 51, "type": "code", "linesLength": 14, "startIndex": 128, "lines": ["class SimpleMnist(nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.l1_w = get_weights(28*28, 256)  # Layer 1 weights\n", "        self.l1_b = get_weights(256)   # Layer 1 bias\n", "        self.l2_w = get_weights(256, 10)  # Layer 2 weights\n", "        self.l2_b = get_weights(10)   # Layer 2 bias\n", "\n", "    def forward(self, x):\n", "        x = x.view(x.size(0), -1)\n", "        x = torch.mm(x, self.l1_w) + self.l1_b\n", "        x = x * (x > 0).float()\n", "        x = torch.mm(x, self.l2_w) + self.l2_b\n", "        return x"]}, {"block": 52, "type": "code", "linesLength": 2, "startIndex": 142, "lines": ["net2 = SimpleMnist().cuda()\n", "opt=optim.Adam(net2.parameters())"]}, {"block": 53, "type": "code", "linesLength": 1, "startIndex": 144, "lines": ["fit(net2, md, epochs=1, crit=loss, opt=opt, metrics=metrics)"]}, {"block": 54, "type": "code", "linesLength": 2, "startIndex": 145, "lines": ["preds = predict(net2, md.val_dl).max(1)[1]\n", "plots(x_imgs[:8], titles=preds[:8])"]}, {"block": 55, "type": "markdown", "linesLength": 1, "startIndex": 147, "lines": ["## what torch.mm (matrix multiplication) is doing"]}, {"block": 56, "type": "markdown", "linesLength": 1, "startIndex": 148, "lines": ["Now let's dig in to what we were doing with `torch.mm`: matrix multiplication.  First, let's start with a simpler building block: **broadcasting**."]}, {"block": 57, "type": "markdown", "linesLength": 1, "startIndex": 149, "lines": ["### Broadcasting"]}, {"block": 58, "type": "markdown", "linesLength": 1, "startIndex": 150, "lines": ["#### Background"]}, {"block": 59, "type": "markdown", "linesLength": 5, "startIndex": 151, "lines": ["Broadcasting and element-wise operations are supported in the same way by both numpy and pytorch.\n", "\n", "Operators (+,-,\\*,/,>,<,==) are usually element-wise.\n", "\n", "Examples of element-wise operations:"]}, {"block": 60, "type": "code", "linesLength": 2, "startIndex": 156, "lines": ["a = np.array([10, 6, -4])\n", "b = np.array([2, 8, 7])"]}, {"block": 61, "type": "code", "linesLength": 1, "startIndex": 158, "lines": ["a + b"]}, {"block": 62, "type": "code", "linesLength": 1, "startIndex": 159, "lines": ["a < b"]}, {"block": 63, "type": "markdown", "linesLength": 1, "startIndex": 160, "lines": ["#### Broadcasting with a scalar"]}, {"block": 64, "type": "code", "linesLength": 1, "startIndex": 161, "lines": ["a > 0"]}, {"block": 65, "type": "markdown", "linesLength": 1, "startIndex": 162, "lines": ["How are we able to do a > 0?  0 is being **broadcast** to have the same dimensions as a."]}, {"block": 66, "type": "markdown", "linesLength": 1, "startIndex": 163, "lines": ["Other examples of broadcasting with a scalar (as we did when we normalized our data):"]}, {"block": 67, "type": "code", "linesLength": 1, "startIndex": 164, "lines": ["a + 1"]}, {"block": 68, "type": "code", "linesLength": 1, "startIndex": 165, "lines": ["m = np.array([[1, 2, 3], [4,5,6], [7,8,9]]); m"]}, {"block": 69, "type": "code", "linesLength": 1, "startIndex": 166, "lines": ["m * 2"]}, {"block": 70, "type": "markdown", "linesLength": 1, "startIndex": 167, "lines": ["#### Broadcasting a vector to a matrix"]}, {"block": 71, "type": "markdown", "linesLength": 1, "startIndex": 168, "lines": ["We can also broadcast a vector to a matrix:"]}, {"block": 72, "type": "code", "linesLength": 1, "startIndex": 169, "lines": ["c = np.array([10,20,30]); c"]}, {"block": 73, "type": "code", "linesLength": 1, "startIndex": 170, "lines": ["m + c"]}, {"block": 74, "type": "code", "linesLength": 1, "startIndex": 171, "lines": ["np.broadcast_to(c, (3,3))"]}, {"block": 75, "type": "code", "linesLength": 1, "startIndex": 172, "lines": ["m.shape, c.shape"]}, {"block": 76, "type": "markdown", "linesLength": 1, "startIndex": 173, "lines": ["Expand dims"]}, {"block": 77, "type": "code", "linesLength": 1, "startIndex": 174, "lines": ["np.expand_dims(c,0).shape"]}, {"block": 78, "type": "code", "linesLength": 1, "startIndex": 175, "lines": ["m + np.expand_dims(c,0)"]}, {"block": 79, "type": "code", "linesLength": 1, "startIndex": 176, "lines": ["np.expand_dims(c,1).shape"]}, {"block": 80, "type": "code", "linesLength": 1, "startIndex": 177, "lines": ["m + np.expand_dims(c,1)"]}, {"block": 81, "type": "code", "linesLength": 1, "startIndex": 178, "lines": ["np.broadcast_to(np.expand_dims(c,1), (3,3))"]}, {"block": 82, "type": "markdown", "linesLength": 1, "startIndex": 179, "lines": ["### Matrix Multiplication"]}, {"block": 83, "type": "code", "linesLength": 1, "startIndex": 180, "lines": ["m, c"]}, {"block": 84, "type": "markdown", "linesLength": 1, "startIndex": 181, "lines": ["#### Matrix-Vector Multiplication"]}, {"block": 85, "type": "code", "linesLength": 1, "startIndex": 182, "lines": ["m @ c  # np.matmul(m, c)"]}, {"block": 86, "type": "markdown", "linesLength": 1, "startIndex": 183, "lines": ["**NOT** matrix multiplication:"]}, {"block": 87, "type": "code", "linesLength": 1, "startIndex": 184, "lines": ["m * c"]}, {"block": 88, "type": "code", "linesLength": 1, "startIndex": 185, "lines": ["(m * c).sum(axis=1)"]}, {"block": 89, "type": "code", "linesLength": 1, "startIndex": 186, "lines": ["c"]}, {"block": 90, "type": "code", "linesLength": 1, "startIndex": 187, "lines": ["np.broadcast_to(c, (3,3))"]}, {"block": 91, "type": "markdown", "linesLength": 1, "startIndex": 188, "lines": ["Nice visualization [matrixmultiplication.xyz](http://matrixmultiplication.xyz/)"]}, {"block": 92, "type": "markdown", "linesLength": 1, "startIndex": 189, "lines": ["Draw a picture"]}, {"block": 93, "type": "markdown", "linesLength": 1, "startIndex": 190, "lines": ["From a machine learning perspective, this is saying how much we want to weight each column.  Different features are different weights of each column.  Each feature can be a different set of weights.  The output of each feature is 1 column (a weighted average of the input columns)."]}, {"block": 94, "type": "code", "linesLength": 1, "startIndex": 191, "lines": ["d = np.array([30,20,10])"]}, {"block": 95, "type": "code", "linesLength": 1, "startIndex": 192, "lines": ["nn = np.stack([c, d], axis=1); nn"]}, {"block": 96, "type": "code", "linesLength": 1, "startIndex": 193, "lines": ["n = np.array([[10,30],[20,20],[30,10]])"]}, {"block": 97, "type": "code", "linesLength": 1, "startIndex": 194, "lines": ["m @ n"]}, {"block": 98, "type": "code", "linesLength": 1, "startIndex": 195, "lines": ["(m * c).sum(axis=1)"]}, {"block": 99, "type": "code", "linesLength": 1, "startIndex": 196, "lines": ["(m * d).sum(axis=1)"]}, {"block": 100, "type": "markdown", "linesLength": 1, "startIndex": 197, "lines": ["### Matrix and Tensor Products"]}, {"block": 101, "type": "markdown", "linesLength": 1, "startIndex": 198, "lines": ["#### Matrix-Vector Products:"]}, {"block": 102, "type": "markdown", "linesLength": 7, "startIndex": 199, "lines": ["The matrix below gives the probabilities of moving from 1 health state to another in 1 year.  If the current health states for a group are:\n", "- 85% asymptomatic\n", "- 10% symptomatic\n", "- 5% AIDS\n", "- 0% death\n", "\n", "what will be the % in each health state in 1 year?"]}, {"block": 103, "type": "markdown", "linesLength": 1, "startIndex": 206, "lines": ["<img src=\"images/markov_health.jpg\" alt=\"floating point\" style=\"width: 80%\"/>(Source: [Concepts of Markov Chains](https://www.youtube.com/watch?v=0Il-y_WLTo4))"]}, {"block": 104, "type": "markdown", "linesLength": 1, "startIndex": 207, "lines": ["#### Answer"]}, {"block": 105, "type": "code", "linesLength": 1, "startIndex": 208, "lines": ["import numpy as np"]}, {"block": 106, "type": "code", "linesLength": 1, "startIndex": 209, "lines": ["#Exercise: Use Numpy to compute the answer to the above\n"]}, {"block": 107, "type": "markdown", "linesLength": 1, "startIndex": 210, "lines": ["#### Matrix-Matrix Products"]}, {"block": 108, "type": "markdown", "linesLength": 1, "startIndex": 211, "lines": ["<img src=\"images/shop.png\" alt=\"floating point\" style=\"width: 100%\"/>(Source: [Several Simple Real-world Applications of Linear Algebra Tools](https://www.mff.cuni.cz/veda/konference/wds/proc/pdf06/WDS06_106_m8_Ulrychova.pdf))"]}, {"block": 109, "type": "markdown", "linesLength": 1, "startIndex": 212, "lines": ["#### Answer"]}, {"block": 110, "type": "code", "linesLength": 1, "startIndex": 213, "lines": ["#Exercise: Use Numpy to compute the answer to the above\n"]}, {"block": 111, "type": "markdown", "linesLength": 1, "startIndex": 214, "lines": ["#### Image Data"]}, {"block": 112, "type": "markdown", "linesLength": 5, "startIndex": 215, "lines": ["Images can be represented by matrices.\n", "\n", "<img src=\"images/digit.gif\" alt=\"digit\" style=\"width: 55%\"/>\n", "  (Source: [Adam Geitgey\n", "](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721))\n"]}, {"block": 113, "type": "markdown", "linesLength": 1, "startIndex": 220, "lines": ["## More Resources"]}, {"block": 114, "type": "markdown", "linesLength": 1, "startIndex": 221, "lines": ["[3 Blue 1 Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab): beautiful linear algebra videos"]}, {"block": 115, "type": "markdown", "linesLength": 9, "startIndex": 222, "lines": ["[PyTorch](http://pytorch.org/) is a Python framework for tensors and dynamic neural networks with GPU acceleration.  Many of the core contributors work on Facebook's AI team.  In many ways, it is similar to Numpy, only with the increased parallelization of using a GPU.\n", "\n", "From the [PyTorch documentation](http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html):\n", "\n", "<img src=\"images/what_is_pytorch.png\" alt=\"pytorch\" style=\"width: 80%\"/>\n", "\n", "**Further learning**: If you are curious to learn what *dynamic* neural networks are, you may want to watch [this talk](https://www.youtube.com/watch?v=Z15cBAuY7Sc) by Soumith Chintala, Facebook AI researcher and core PyTorch contributor.\n", "\n", "If you want to learn more PyTorch, you can try this [tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) or this [learning by examples](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html)."]}, {"block": 116, "type": "code", "linesLength": 0, "startIndex": 231, "lines": []}]