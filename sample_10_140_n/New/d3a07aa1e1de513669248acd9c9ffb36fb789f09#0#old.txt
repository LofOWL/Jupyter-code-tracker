[{"block": 0, "type": "markdown", "linesLength": 1, "startIndex": 0, "lines": ["# Linear Models with Stochastic Gradient Descent"]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 1, "lines": ["## ... about machine learning (a reminder from lesson 1)"]}, {"block": 2, "type": "markdown", "linesLength": 7, "startIndex": 2, "lines": ["The good news is that modern machine learning can be distilled down to a couple of key techniques that are of very wide applicability. Recent studies have shown that the vast majority of datasets can be best modeled with just two methods:\n", "\n", "1. Ensembles of decision trees (i.e. Random Forests and Gradient Boosting Machines), mainly for structured data (such as you might find in a database table at most companies).  We looked at random forests in depth as we analyzed the Blue Book for Bulldozers dataset.\n", "\n", "2. Multi-layered neural networks learnt with SGD (i.e. shallow and/or deep learning), mainly for unstructured data (such as audio, vision, and natural language)\n", "\n", "In this lesson, we will start on the 2nd approach (a neural network with SGD) by analyzing the MNIST dataset.  You may be surprised to learn that **logistic regression is actually an example of a simple neural net**!"]}, {"block": 3, "type": "markdown", "linesLength": 1, "startIndex": 9, "lines": ["## About The Data"]}, {"block": 4, "type": "markdown", "linesLength": 1, "startIndex": 10, "lines": ["In this lesson, we will be working with MNIST, a classic data set of hand-written digits.  Solutions to this problem are used by banks to automatically recognize the amounts on checks, and by the postal service to automatically recognize zip codes on mail."]}, {"block": 5, "type": "markdown", "linesLength": 1, "startIndex": 11, "lines": ["<img src=\"images/mnist.png\" alt=\"\" style=\"width: 60%\"/>"]}, {"block": 6, "type": "markdown", "linesLength": 5, "startIndex": 12, "lines": ["A matrix can represent an image, by creating a grid where each entry corresponds to a different pixel.\n", "\n", "<img src=\"images/digit.gif\" alt=\"digit\" style=\"width: 55%\"/>\n", "  (Source: [Adam Geitgey\n", "](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721))\n"]}, {"block": 7, "type": "markdown", "linesLength": 1, "startIndex": 17, "lines": ["### Imports"]}, {"block": 8, "type": "code", "linesLength": 6, "startIndex": 18, "lines": ["%load_ext autoreload\n", "%autoreload 2\n", "\n", "from fastai.imports import *\n", "from fastai.torch_imports import *\n", "from fastai.io import *"]}, {"block": 9, "type": "markdown", "linesLength": 1, "startIndex": 24, "lines": ["### Download"]}, {"block": 10, "type": "markdown", "linesLength": 1, "startIndex": 25, "lines": ["Let's download, unzip, and format the data."]}, {"block": 11, "type": "code", "linesLength": 1, "startIndex": 26, "lines": ["path = '../data/'"]}, {"block": 12, "type": "code", "linesLength": 2, "startIndex": 27, "lines": ["import os\n", "os.makedirs(path, exist_ok=True)"]}, {"block": 13, "type": "code", "linesLength": 5, "startIndex": 29, "lines": ["URL='http://deeplearning.net/data/mnist/'\n", "FILENAME='mnist.pkl.gz'\n", "\n", "def load_mnist(filename):\n", "    return pickle.load(gzip.open(filename, 'rb'), encoding='latin-1')"]}, {"block": 14, "type": "code", "linesLength": 2, "startIndex": 34, "lines": ["get_data(URL+FILENAME, path+FILENAME)\n", "((x, y), (x_valid, y_valid), _) = load_mnist(path+FILENAME)"]}, {"block": 15, "type": "code", "linesLength": 1, "startIndex": 36, "lines": ["type(x)"]}, {"block": 16, "type": "markdown", "linesLength": 1, "startIndex": 37, "lines": ["### Normalize"]}, {"block": 17, "type": "markdown", "linesLength": 1, "startIndex": 38, "lines": ["Many machine learning algorithms behave better when the data is *normalized*, that is when the mean is 0 and the standard deviation is 1. We will subtract off the mean and standard deviation from our training set in order to normalize the data:"]}, {"block": 18, "type": "code", "linesLength": 2, "startIndex": 39, "lines": ["mean = x.mean()\n", "std = x.std()"]}, {"block": 19, "type": "code", "linesLength": 2, "startIndex": 41, "lines": ["x=(x-mean)/std\n", "x.mean(), x.std()"]}, {"block": 20, "type": "markdown", "linesLength": 1, "startIndex": 43, "lines": ["Note that for consistency (with the parameters we learn when training), we subtract the mean and standard deviation of our training set from our validation set. "]}, {"block": 21, "type": "code", "linesLength": 2, "startIndex": 44, "lines": ["x_valid = (x_valid-mean)/std\n", "x_valid.mean(), x_valid.std()"]}, {"block": 22, "type": "markdown", "linesLength": 1, "startIndex": 46, "lines": ["### Look at the data"]}, {"block": 23, "type": "markdown", "linesLength": 1, "startIndex": 47, "lines": ["In any sort of data science work, it's important to look at your data, to make sure you understand the format, how it's stored, what type of values it holds, etc. To make it easier to work with, let's reshape it into 2d images from the flattened 1d format."]}, {"block": 24, "type": "markdown", "linesLength": 1, "startIndex": 48, "lines": ["#### Helper methods"]}, {"block": 25, "type": "code", "linesLength": 7, "startIndex": 49, "lines": ["%matplotlib inline\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "def show(img, title=None):\n", "    plt.imshow(img, interpolation='none', cmap=\"gray\")\n", "    if title is not None: plt.title(title)"]}, {"block": 26, "type": "code", "linesLength": 8, "startIndex": 56, "lines": ["def plots(ims, figsize=(12,6), rows=2, titles=None):\n", "    f = plt.figure(figsize=figsize)\n", "    cols = len(ims)//rows\n", "    for i in range(len(ims)):\n", "        sp = f.add_subplot(rows, cols, i+1)\n", "        sp.axis('Off')\n", "        if titles is not None: sp.set_title(titles[i], fontsize=16)\n", "        plt.imshow(ims[i], interpolation='none', cmap='gray')"]}, {"block": 27, "type": "markdown", "linesLength": 1, "startIndex": 64, "lines": ["#### Plots "]}, {"block": 28, "type": "code", "linesLength": 1, "startIndex": 65, "lines": ["x_valid.shape"]}, {"block": 29, "type": "code", "linesLength": 1, "startIndex": 66, "lines": ["x_imgs = np.reshape(x_valid, (-1,28,28)); x_imgs.shape"]}, {"block": 30, "type": "code", "linesLength": 1, "startIndex": 67, "lines": ["show(x_imgs[0], y_valid[0])"]}, {"block": 31, "type": "code", "linesLength": 1, "startIndex": 68, "lines": ["y_valid.shape"]}, {"block": 32, "type": "markdown", "linesLength": 1, "startIndex": 69, "lines": ["It's the digit 3!  And that's stored in the y value:"]}, {"block": 33, "type": "code", "linesLength": 1, "startIndex": 70, "lines": ["y_valid[0]"]}, {"block": 34, "type": "markdown", "linesLength": 1, "startIndex": 71, "lines": ["We can look at part of an image:"]}, {"block": 35, "type": "code", "linesLength": 1, "startIndex": 72, "lines": ["x_imgs[0,10:15,10:15]"]}, {"block": 36, "type": "code", "linesLength": 1, "startIndex": 73, "lines": ["show(x_imgs[0,10:15,10:15])"]}, {"block": 37, "type": "code", "linesLength": 1, "startIndex": 74, "lines": ["plots(x_imgs[:8], titles=y_valid[:8])"]}, {"block": 38, "type": "markdown", "linesLength": 1, "startIndex": 75, "lines": ["## Logistic regression"]}, {"block": 39, "type": "code", "linesLength": 1, "startIndex": 76, "lines": ["from sklearn import linear_model, metrics"]}, {"block": 40, "type": "markdown", "linesLength": 1, "startIndex": 77, "lines": ["Let's start by using the sklearn implementation:"]}, {"block": 41, "type": "code", "linesLength": 6, "startIndex": 78, "lines": ["regr = linear_model.LogisticRegression(C=50. / len(x),\n", "                                     # multi_class='multinomial',\n", "                                     # penalty='l2', \n", "                                     #  tol=0.1\n", "                                      )\n", "# %timeit regr.fit(x, y)"]}, {"block": 42, "type": "code", "linesLength": 1, "startIndex": 84, "lines": ["regr.fit(x,y)"]}, {"block": 43, "type": "markdown", "linesLength": 1, "startIndex": 85, "lines": ["Training set error:"]}, {"block": 44, "type": "code", "linesLength": 1, "startIndex": 86, "lines": ["pred = regr.predict(x)"]}, {"block": 45, "type": "code", "linesLength": 1, "startIndex": 87, "lines": ["(pred == y).sum()/len(pred)"]}, {"block": 46, "type": "code", "linesLength": 1, "startIndex": 88, "lines": ["pred = regr.predict(x_valid)"]}, {"block": 47, "type": "markdown", "linesLength": 1, "startIndex": 89, "lines": ["Validation Set Error:"]}, {"block": 48, "type": "code", "linesLength": 1, "startIndex": 90, "lines": ["(pred == y_valid).sum()/len(pred)"]}, {"block": 49, "type": "markdown", "linesLength": 1, "startIndex": 91, "lines": ["Not bad for our 1st attempt.  We are getting 91% accuracy with our logistic regression model."]}, {"block": 50, "type": "markdown", "linesLength": 1, "startIndex": 92, "lines": ["It will be helpful to have some metrics on how good our prediciton is.  We will look at the mean squared norm (L2) and mean absolute error (L1)."]}, {"block": 51, "type": "code", "linesLength": 3, "startIndex": 93, "lines": ["def regr_metrics(act, pred):\n", "    return (math.sqrt(metrics.mean_squared_error(act, pred)), \n", "     metrics.mean_absolute_error(act, pred))"]}, {"block": 52, "type": "code", "linesLength": 1, "startIndex": 96, "lines": ["regr_metrics(y_valid, regr.predict(x_valid))"]}, {"block": 53, "type": "markdown", "linesLength": 1, "startIndex": 97, "lines": ["To take a deeper look at what logistic regression is doing and how we can program it ourselves, we are going to put it in context as a specific example of a shallow neural net."]}, {"block": 54, "type": "markdown", "linesLength": 1, "startIndex": 98, "lines": ["## Neural Networks"]}, {"block": 55, "type": "markdown", "linesLength": 5, "startIndex": 99, "lines": ["**What is a neural network?**\n", "\n", "A *neural network* is an *infinitely flexible function*, consisting of *layers*.  A *layer* is matrix multiplication (which is linear) followed by a non-linear function (the *activation*).\n", "\n", "One of the tricky parts of neural networks is just keeping track of all the vocabulary!"]}, {"block": 56, "type": "markdown", "linesLength": 1, "startIndex": 104, "lines": ["### Functions, parameters, and training"]}, {"block": 57, "type": "markdown", "linesLength": 5, "startIndex": 105, "lines": ["A **function** takes inputs and returns outputs. For instance, $f(x) = 3x + 5$ is an example of a function.  If we input $2$, the output is $3\\times 2 + 5 = 11$, or if we input $-1$, the output is $3\\times -1 + 5 = 2$\n", "\n", "Functions have **parameters**. The above function $f$ is $ax + b$, with parameters a and b set to $a=3$ and $b=5$.\n", "\n", "Machine learning is often about learning the best values for those parameters.  For instance, suppose we have the data points on the chart below.  What values should we choose for $a$ and $b$?"]}, {"block": 58, "type": "markdown", "linesLength": 1, "startIndex": 110, "lines": ["<img src=\"images/sgd2.gif\" alt=\"\" style=\"width: 70%\"/>"]}, {"block": 59, "type": "markdown", "linesLength": 5, "startIndex": 111, "lines": ["In the above gif fast.ai Practical Deep Learning for Coders course, [intro to SGD notebook](https://github.com/fastai/courses/blob/master/deeplearning1/nbs/sgd-intro.ipynb)), an algorithm called stochastic gradient descent is being used to learn the best parameters to fit the line to the data (note: in the gif, the algorithm is stopping before the absolute best parameters are found).  This process is called **training** or **fitting**.\n", "\n", "Most datasets will not be well-represented by a line.  We could use a more complicated function, such as $g(x) = ax^2 + bx + c + \\sin d$.  Now we have 4 parameters to learn: $a$, $b$, $c$, and $d$.  This function is more flexible than $f(x) = ax + b$ and will be able to accurately model more datasets.\n", "\n", "Neural networks take this to an extreme, and are infinitely flexible.  They often have thousands, or even hundreds of thousands of parameters.  However the core idea is the same as above.  The neural network is a function, and we will learn the best parameters for modeling our data."]}, {"block": 60, "type": "markdown", "linesLength": 1, "startIndex": 116, "lines": ["## PyTorch"]}, {"block": 61, "type": "markdown", "linesLength": 1, "startIndex": 117, "lines": ["We will be using the open source [deep learning library, fastai](https://github.com/fastai/fastai), which provides high level abstractions and best practices on top of PyTorch.  This is the highest level, simplest way to get started with deep learning. Please note that fastai requires Python 3 to function. It is currently in pre-alpha, so items may move around and more documentation will be added in the future."]}, {"block": 62, "type": "markdown", "linesLength": 11, "startIndex": 118, "lines": ["The fastai deep learning library uses [PyTorch](http://pytorch.org/), a Python framework for dynamic neural networks with GPU acceleration, which was released by Facebook's AI team.\n", "\n", "PyTorch has two overlapping, yet distinct, purposes.  As described in the [PyTorch documentation](http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html):\n", "\n", "<img src=\"images/what_is_pytorch.png\" alt=\"pytorch\" style=\"width: 80%\"/>\n", "\n", "The neural network functionality of PyTorch is built on top of the Numpy-like functionality for fast matrix computations on a GPU. Although the neural network purpose receives way more attention, both are very useful.  We'll implement a neural net from scratch today using PyTorch.\n", "\n", "**Further learning**: If you are curious to learn what *dynamic* neural networks are, you may want to watch [this talk](https://www.youtube.com/watch?v=Z15cBAuY7Sc) by Soumith Chintala, Facebook AI researcher and core PyTorch contributor.\n", "\n", "If you want to learn more PyTorch, you can try this [introductory tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) or this [tutorial to learn by examples](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html)."]}, {"block": 63, "type": "markdown", "linesLength": 1, "startIndex": 129, "lines": ["### About GPUs"]}, {"block": 64, "type": "markdown", "linesLength": 7, "startIndex": 130, "lines": ["Graphical processing units (GPUs) allow for matrix computations to be done with much greater speed, as long as you have a library such as PyTorch that takes advantage of them.  Advances in GPU technology in the last 10-20 years have been a key part of why neural networks are proving so much more powerful now than they did a few decades ago. \n", "\n", "You may own a computer that has a GPU which can be used.  For the many people that either don't have a GPU (or have a GPU which can't be easily accessed by Python), there are a few differnt options:\n", "\n", "- **Don't use a GPU**: For the sake of this tutorial, you don't have to use a GPU, although some computations will be slower.  The only change needed to the code is to remove `.cuda()` wherever it appears.\n", "- **Use crestle, through your browser**: [Crestle](https://www.crestle.com/) is a service that gives you an already set up cloud service with all the popular scientific and deep learning frameworks already pre-installed and configured to run on a GPU in the cloud. It is easily accessed through your browser. New users get 10 hours and 1 GB of storage for free. After this, GPU usage is 34 cents per hour. I recommend this option to those who are new to AWS or new to using the console.\n", "- **Set up an AWS instance through your console**: You can create an AWS instance with a GPU by following the steps in this  [fast.ai setup lesson](http://course.fast.ai/lessons/aws.html).]  AWS charges 90 cents per hour for this."]}, {"block": 65, "type": "markdown", "linesLength": 1, "startIndex": 137, "lines": ["## Neural Net for Logistic Regression in PyTorch"]}, {"block": 66, "type": "code", "linesLength": 6, "startIndex": 138, "lines": ["from fastai.metrics import *\n", "from fastai.model import *\n", "from fastai.dataset import *\n", "from fastai.core import *\n", "\n", "import torch.nn as nn"]}, {"block": 67, "type": "markdown", "linesLength": 1, "startIndex": 144, "lines": ["We will begin with the highest level abstraction: using a neural net defined by PyTorch's Sequential class.  "]}, {"block": 68, "type": "code", "linesLength": 4, "startIndex": 145, "lines": ["net = nn.Sequential(\n", "    nn.Linear(28*28, 10),\n", "    nn.LogSoftmax()\n", ").cuda()"]}, {"block": 69, "type": "markdown", "linesLength": 3, "startIndex": 149, "lines": ["Each input is a vector of size $28\\times 28$ pixels and our output is of size $10$ (since there are 10 digits: 0, 1, ..., 9). \n", "\n", "We use the output of the final layer to generate our predictions.  Often for classification problems (like MNIST digit classification), the final layer has the same number of outputs as there are classes.  In that case, this is 10: one for each digit from 0 to 9.  These can be converted to comparative probabilities.  For instance, it may be determined that a particular hand-written image is 80% likely to be a 4, 18% likely to be a 9, and 2% likely to be a 3.  In our case, we are not interested in viewing the probabilites, and just want to see what the most likely guess is."]}, {"block": 70, "type": "code", "linesLength": 1, "startIndex": 152, "lines": ["md = ImageClassifierData.from_arrays(path, (x,y), (x_valid, y_valid))"]}, {"block": 71, "type": "code", "linesLength": 1, "startIndex": 153, "lines": ["x.shape"]}, {"block": 72, "type": "code", "linesLength": 3, "startIndex": 154, "lines": ["loss=nn.NLLLoss()\n", "metrics=[accuracy]\n", "opt=optim.Adam(net.parameters())"]}, {"block": 73, "type": "markdown", "linesLength": 1, "startIndex": 157, "lines": ["*Fitting* is the process by which the neural net learns the best parameters for the dataset."]}, {"block": 74, "type": "code", "linesLength": 1, "startIndex": 158, "lines": ["fit(net, md, epochs=1, crit=loss, opt=opt, metrics=metrics)"]}, {"block": 75, "type": "markdown", "linesLength": 3, "startIndex": 159, "lines": ["GPUs are great at handling lots of data at once (otherwise don't get performance benefit).  We break the data up into **batches**, and that specifies how many samples from our dataset we want to send to the GPU at a time.  The fastai library defaults to a batch size of 64.  On each iteration of the training loop, the error on 1 batch of data will be calculated, and the optimizer will update the parameters based on that.\n", "\n", "An **epoch** is completed once each data sample has been used once in the training loop."]}, {"block": 76, "type": "markdown", "linesLength": 1, "startIndex": 162, "lines": ["Now that we have the parameters for our model, we can make predictions on our validation set."]}, {"block": 77, "type": "code", "linesLength": 1, "startIndex": 163, "lines": ["preds = predict(net, md.val_dl)"]}, {"block": 78, "type": "code", "linesLength": 1, "startIndex": 164, "lines": ["preds.argmax(1)[:5]"]}, {"block": 79, "type": "code", "linesLength": 1, "startIndex": 165, "lines": ["preds.shape"]}, {"block": 80, "type": "code", "linesLength": 1, "startIndex": 166, "lines": ["preds = preds.argmax(1)"]}, {"block": 81, "type": "markdown", "linesLength": 1, "startIndex": 167, "lines": ["**Validation Set accuracy:**"]}, {"block": 82, "type": "code", "linesLength": 1, "startIndex": 168, "lines": ["np.sum(preds == y_valid)/len(preds)"]}, {"block": 83, "type": "markdown", "linesLength": 1, "startIndex": 169, "lines": ["Let's see how some of our preditions look!"]}, {"block": 84, "type": "code", "linesLength": 1, "startIndex": 170, "lines": ["plots(x_imgs[:8], titles=preds[:8])"]}, {"block": 85, "type": "markdown", "linesLength": 1, "startIndex": 171, "lines": ["Validation Set accuracy:"]}, {"block": 86, "type": "code", "linesLength": 1, "startIndex": 172, "lines": ["np.sum(preds == y_valid)"]}, {"block": 87, "type": "code", "linesLength": 1, "startIndex": 173, "lines": ["preds[:8]"]}, {"block": 88, "type": "markdown", "linesLength": 1, "startIndex": 174, "lines": ["## Defining Logistic Regression Ourselves"]}, {"block": 89, "type": "markdown", "linesLength": 5, "startIndex": 175, "lines": ["`Linear` is defined by a matrix multiplication and then an addition (these are also called `affine transformations`).  Let's try defining this ourselves.  \n", "\n", "Just as Numpy has `np.matmul` for matrix multiplication (in Python 3, this is equivalent to the `@` operator), PyTorch has `torch.matmul`.  \n", "\n", "PyTorch class has two things: constructor (says parameters) and a forward method (how to calculate prediction using those parameters)  The method `forward` describes how the neural net converts inputs to outputs."]}, {"block": 90, "type": "markdown", "linesLength": 1, "startIndex": 180, "lines": ["In PyTorch, the optimizer knows to try to optimize any attribute of type **Parameter**."]}, {"block": 91, "type": "code", "linesLength": 1, "startIndex": 181, "lines": ["def get_weights(*dims): return nn.Parameter(torch.randn(*dims)/dims[0])"]}, {"block": 92, "type": "code", "linesLength": 11, "startIndex": 182, "lines": ["class LogReg(nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.l1_w = get_weights(28*28, 10)  # Layer 1 weights\n", "        self.l1_b = get_weights(10)         # Layer 1 bias\n", "\n", "    def forward(self, x):\n", "        x = x.view(x.size(0), -1)\n", "        x = torch.matmul(x, self.l1_w) + self.l1_b  # Linear Layer\n", "        x = torch.log(torch.exp(x)/(1 + torch.exp(x).sum(dim=0)))        # Non-linear (LogSoftmax) Layer\n", "        return x"]}, {"block": 93, "type": "markdown", "linesLength": 1, "startIndex": 193, "lines": ["We create our neural net and the optimizer.  (We will use the same loss and metrics from above)."]}, {"block": 94, "type": "code", "linesLength": 2, "startIndex": 194, "lines": ["net2 = LogReg().cuda()\n", "opt=optim.Adam(net2.parameters())"]}, {"block": 95, "type": "code", "linesLength": 1, "startIndex": 196, "lines": ["fit(net2, md, epochs=1, crit=loss, opt=opt, metrics=metrics)"]}, {"block": 96, "type": "markdown", "linesLength": 1, "startIndex": 197, "lines": ["Now we can check our predictions:"]}, {"block": 97, "type": "code", "linesLength": 2, "startIndex": 198, "lines": ["preds = predict(net2, md.val_dl).argmax(1)\n", "plots(x_imgs[:8], titles=preds[:8])"]}, {"block": 98, "type": "code", "linesLength": 1, "startIndex": 200, "lines": ["np.sum(preds == y_valid)/len(preds)"]}, {"block": 99, "type": "markdown", "linesLength": 1, "startIndex": 201, "lines": ["## Writing Our Own Training Loop"]}, {"block": 100, "type": "code", "linesLength": 34, "startIndex": 202, "lines": ["net2 = LogReg().cuda()\n", "loss_fn=nn.NLLLoss()\n", "learning_rate = 1e-3\n", "optimizer=optim.Adam(net2.parameters(), lr=learning_rate)\n", "\n", "for epoch in range(1):\n", "    losses=[]\n", "    dl = iter(md.trn_dl)\n", "    for t in range(len(dl)):\n", "        # Forward pass: compute predicted y by passing x to the model.\n", "        x, y = next(dl)\n", "        y_pred = net2.forward(Variable(x).cuda())\n", "\n", "        # Compute and print loss.\n", "        loss = loss_fn(y_pred, Variable(y).cuda())\n", "        losses.append(loss)\n", "\n", "        # Before the backward pass, use the optimizer object to zero all of the\n", "        # gradients for the variables it will update (which are the learnable weights\n", "        # of the model)\n", "        optimizer.zero_grad()\n", "\n", "        # Backward pass: compute gradient of the loss with respect to model\n", "        # parameters\n", "        loss.backward()\n", "        # print(loss.data)\n", "\n", "        # Calling the step function on an Optimizer makes an update to its\n", "        # parameters\n", "        optimizer.step()\n", "    \n", "    val_dl = iter(md.val_dl)\n", "    val_scores = [score(*next(val_dl)) for i in range(len(val_dl))]\n", "    print(np.mean(val_scores))"]}, {"block": 101, "type": "code", "linesLength": 4, "startIndex": 236, "lines": ["def score(x, y):\n", "    y_pred = to_np(net2.forward(Variable(x.cuda())))\n", "    # print(y_pred.shape, y.shape)\n", "    return np.sum(y_pred.argmax(axis=1) == to_np(y))/len(y_pred)"]}, {"block": 102, "type": "code", "linesLength": 2, "startIndex": 240, "lines": ["x_,y_=next(iter(md.val_dl))\n", "score(x_,y_)"]}, {"block": 103, "type": "code", "linesLength": 1, "startIndex": 242, "lines": ["y_pred.shape"]}, {"block": 104, "type": "code", "linesLength": 1, "startIndex": 243, "lines": ["y_pred.argmax(axis=1)[:10]== to_np(y_[:10])"]}, {"block": 105, "type": "code", "linesLength": 1, "startIndex": 244, "lines": ["y_pred.argmax(axis=1) == y_"]}, {"block": 106, "type": "code", "linesLength": 1, "startIndex": 245, "lines": ["np.sum(y_pred.argmax(axis=1) == to_np(y_))/len(y_pred)"]}, {"block": 107, "type": "code", "linesLength": 0, "startIndex": 246, "lines": []}, {"block": 108, "type": "markdown", "linesLength": 1, "startIndex": 246, "lines": ["## Stochastic Gradient Descent"]}, {"block": 109, "type": "markdown", "linesLength": 1, "startIndex": 247, "lines": ["Nearly all of deep learning is powered by one very important algorithm: **stochastic gradient descent (SGD)**. SGD can be seeing as an approximation of **gradient descent (GD)**. In GD you have to run through all the samples in your training set to do a single itaration. In SGD you use only a subset of training samples to do the update for a parameter in a particular iteration. The subset used in each iteration is called a batch or minibatch."]}, {"block": 110, "type": "markdown", "linesLength": 1, "startIndex": 248, "lines": ["## More about Layers"]}, {"block": 111, "type": "markdown", "linesLength": 1, "startIndex": 249, "lines": ["Sequential defines layers of our network, so let's talk about layers. Neural networks consist of **linear layers alternating with non-linear layers**.  This creates functions which are incredibly flexible.  Deeper layers are able to capture more complex patterns."]}, {"block": 112, "type": "markdown", "linesLength": 5, "startIndex": 250, "lines": ["Layer 1 of a convolutional neural network:\n", "<img src=\"images/zeiler1.png\" alt=\"pytorch\" style=\"width: 40%\"/>\n", "<center>\n", "[Matthew Zeiler and Rob Fergus](http://www.matthewzeiler.com/wp-content/uploads/2017/07/arxive2013.pdf)\n", "</center>"]}, {"block": 113, "type": "markdown", "linesLength": 5, "startIndex": 255, "lines": ["Layer 2:\n", "<img src=\"images/zeiler2.png\" alt=\"pytorch\" style=\"width: 90%\"/>\n", "<center>\n", "[Matthew Zeiler and Rob Fergus](http://www.matthewzeiler.com/wp-content/uploads/2017/07/arxive2013.pdf)\n", "</center>"]}, {"block": 114, "type": "markdown", "linesLength": 5, "startIndex": 260, "lines": ["Deeper layers can learn about more complicated shapes (although we are only using 2 layers in our network):\n", "<img src=\"images/zeiler4.png\" alt=\"pytorch\" style=\"width: 90%\"/>\n", "<center>\n", "[Matthew Zeiler and Rob Fergus](http://www.matthewzeiler.com/wp-content/uploads/2017/07/arxive2013.pdf)\n", "</center>"]}, {"block": 115, "type": "code", "linesLength": 0, "startIndex": 265, "lines": []}]