[{"block": 0, "type": "markdown", "linesLength": 5, "startIndex": 0, "lines": ["# SAR with Spark and SQL\n", "\n", "In this example, we will walkthrough each step of the SAR algorithm with an implementation using Spark and SQL.\n", "\n", "Smart Adaptive Recommendations (SAR) is a fast, scalable, adaptive algorithm for personalized recommendations based on user transaction history and item descriptions. It is powered by understanding the **similarity** between items, and recommending similar items to ones a user has an existing **affinity** for. "]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 5, "lines": ["# 0 Global Variables and Imports"]}, {"block": 2, "type": "code", "linesLength": 12, "startIndex": 6, "lines": ["# specify parameters\n", "TOP_K=2\n", "RECOMMEND_SEEN=True\n", "# options are 'jaccard', 'lift' or '' to skip and use item cooccurrence directly\n", "SIMILARITY='jaccard'\n", "\n", "import pandas as pd\n", "import numpy as np\n", "import heapq\n", "import pyspark.sql.functions as F\n", "from pyspark.sql.window import Window\n", "from pyspark.sql.types import StructType, StructField, StringType, Row, ArrayType, IntegerType, FloatType"]}, {"block": 3, "type": "markdown", "linesLength": 3, "startIndex": 18, "lines": ["# 1 Load Data\n", "\n", "We'll work with a small dataset here containing customer IDs, item IDs, and the customer's rating for the item. SAR requires inputs to be of the following schema: `<User ID>, <Item ID>, <Time>, [<Event Type>], [<Event Weight>]` (we will not use time or event type in the example below, and `rating` will be used as the `Event Weight`). "]}, {"block": 4, "type": "code", "linesLength": 14, "startIndex": 21, "lines": ["# There are two versions of the dataframes - the numeric version and the alphanumeric one:\n", "# they both have similar test data for top-2 recommendations and illustrate the indexing approaches to matrix multiplication on SQL\n", "d_train = {\n", "'customerID': [1,1,1,2,2,3,3],\n", "'itemID':     [1,2,3,4,5,6,1],\n", "'rating':     [5,5,5,1,1,3,5]\n", "}\n", "pdf_train = pd.DataFrame(d_train)\n", "d_test = {\n", "'customerID': [1,1,2,2,3,3],\n", "'itemID':     [4,5,1,5,6,1],\n", "'rating':     [1,1,5,5,5,5]\n", "}\n", "pdf_test = pd.DataFrame(d_test)"]}, {"block": 5, "type": "code", "linesLength": 5, "startIndex": 35, "lines": ["a_train = np.array([[5,5,5,0,0,0],\\\n", "                    [0,0,0,1,1,0],\n", "                    [5,0,0,0,0,3]])\n", "print(a_train)\n", "print(a_train.shape)"]}, {"block": 6, "type": "code", "linesLength": 15, "startIndex": 40, "lines": ["d_alnum_train = {\n", "'customerID': ['ua','ua','ua','ub','ub','uc','uc'],\n", "'itemID':     ['ia','ib','ic','id','ie','if','ia'],\n", "'rating':     [5,5,5,1,1,3,5]\n", "}\n", "#pdf_train = pd.DataFrame(d_alnum_train)\n", "pdf_train = pd.DataFrame(d_train)\n", "d_alnum_test = {\n", "'customerID': ['ua','ua','ub','ub','uc','uc'],\n", "'itemID':     ['id','ie','ia','ie','if','ia'],\n", "'rating':     [1,1,5,5,5,5]\n", "}\n", "#pdf_test = pd.DataFrame(d_alnum_test)\n", "pdf_test = pd.DataFrame(d_test)\n", "pdf_test.head(10)"]}, {"block": 7, "type": "code", "linesLength": 3, "startIndex": 55, "lines": ["df = spark.createDataFrame(pdf_train).withColumn(\"type\", F.lit(1))\n", "df_test = spark.createDataFrame(pdf_test).withColumn(\"type\", F.lit(0))\n", "df.show()"]}, {"block": 8, "type": "markdown", "linesLength": 3, "startIndex": 58, "lines": ["# 2 Index the user and item IDs\n", "\n", "Map user and item alphanumeric IDs to matrix indices."]}, {"block": 9, "type": "code", "linesLength": 23, "startIndex": 61, "lines": ["n_train = df.count()\n", "df_all = df.union(df_test)\n", "df_all.createOrReplaceTempView(\"df_all\")\n", "query = \"\"\"\n", "SELECT customerid,\n", "       Dense_rank()\n", "         OVER(\n", "           partition BY 1\n", "           ORDER BY customerid) AS row_id,\n", "       itemid,\n", "       Dense_rank()\n", "         OVER(\n", "           partition BY 1\n", "           ORDER BY itemid)     AS col_id,\n", "       rating,\n", "       type\n", "FROM   df_all \n", "\"\"\"\n", "df_all = spark.sql(query)\n", "df_all.createOrReplaceTempView(\"df_all\")\n", "customer_index2ID = dict(df_all.select([\"row_id\", \"customerID\"]).rdd.reduceByKey(lambda k, v: v).collect())\n", "item_index2ID = dict(df_all.select([\"col_id\", \"itemID\"]).rdd.reduceByKey(lambda k, v: v).collect())\n", "df_all.show()"]}, {"block": 10, "type": "markdown", "linesLength": 8, "startIndex": 84, "lines": ["# 3 Compute Item Co-occurrence\n", "\n", "Central to how SAR defines similarity is an item-to-item ***co-occurrence matrix***. Co-occurrence is defined as the number of times two items appear together for a given user.  We can represent the co-occurrence of all items as a $mxm$ matrix $C$, where $c_{i,j}$   is the number of times item $i$ occurred with item $j$.\n", "\n", "The co-occurence matric $C$ has the following properties:\n", "- It is symmetric, so $c_{i,j} = c_{j,i}$\n", "- It is nonnegative: $c_{i,j} >= 0$\n", "- The occurrences are at least as large as the co-occurrences. I.e, the largest element for each row (and column) is on the main diagonal: $\u2200(i,j)\u2009C_{i,i},C_{j,j}>=C_{i,j}$."]}, {"block": 11, "type": "code", "linesLength": 13, "startIndex": 92, "lines": ["query = \"\"\"\n", "SELECT row_id,\n", "       col_id,\n", "       rating\n", "FROM   df_all\n", "WHERE  type = 1 \n", "\"\"\"\n", "df = spark.sql(query)\n", "df.createOrReplaceTempView(\"df_train\")\n", "df.show()\n", "df_transpose = spark.sql(\"select col_id as row_id, row_id as col_id, rating from df_train\")\n", "df_transpose.createOrReplaceTempView(\"df_train_transpose\")\n", "df_transpose.show()"]}, {"block": 12, "type": "code", "linesLength": 14, "startIndex": 105, "lines": ["query = \"\"\"\n", "SELECT A.row_id AS row_item_id,\n", "       B.col_id AS col_item_id,\n", "       Sum(1)   AS value\n", "FROM   df_train_transpose A\n", "       INNER JOIN df_train B\n", "               ON A.col_id = B.row_id\n", "GROUP  BY A.row_id,\n", "          B.col_id\n", "\"\"\"\n", "item_cooccurrence = spark.sql(query)\n", "item_cooccurrence.createOrReplaceTempView(\"item_cooccurrence\")\n", "item_cooccurrence.show()\n", "print(item_cooccurrence.count())"]}, {"block": 13, "type": "code", "linesLength": 5, "startIndex": 119, "lines": ["indicator = a_train.copy()\n", "indicator[indicator>0]=1\n", "item_cooccurrence = indicator.T.dot(indicator)\n", "print (item_cooccurrence)\n", "print ((item_cooccurrence>0).sum())"]}, {"block": 14, "type": "markdown", "linesLength": 10, "startIndex": 124, "lines": ["# 4 Compute Item Similarity\n", "\n", "\n", "Once we have a co-occurrence matrix, an ***item similarity matrix*** $S$ can be obtained by rescaling the co-occurrences according to a given metric. Options for the metric include Jaccard, lift, and counts (meaning no rescaling).\n", "\n", "The rescaling formula for Jaccard is $s_{ij}=c_{ij} / (c_{ii}+c_{jj}-c_{ij})$\n", "\n", "and that for lift is $s_{ij}=c_{ij}/(c_{ii}*c_{jj})$\n", "\n", "where $c_{ii}$ and $c_{jj}$ are the $i$th and $j$th diagonal elements of $C$. In general, using counts as a similarity metric favours predictability, meaning that the most popular items will be recommended most of the time. Lift by contrast favours discoverability/serendipity: an item that is less popular overall but highly favoured by a small subset of users is more likely to be recommended. Jaccard is a compromise between the two.\n"]}, {"block": 15, "type": "code", "linesLength": 9, "startIndex": 134, "lines": ["# show to who to compute Jaccard\n", "diag = item_cooccurrence.diagonal()\n", "diag_rows = np.expand_dims(diag, axis=0)\n", "diag_cols = np.expand_dims(diag, axis=1)\n", "# this essentially does vstack(diag_rows).T + vstack(diag_rows) - cooccurrence\n", "denom = diag_rows + diag_cols - item_cooccurrence\n", "jaccard = item_cooccurrence / denom\n", "print (\"Jaccard\")\n", "print (jaccard)"]}, {"block": 16, "type": "code", "linesLength": 9, "startIndex": 143, "lines": ["if SIMILARITY is 'jaccard' or SIMILARITY is 'lift':\n", "    query = \"\"\"\n", "    SELECT A.row_item_id AS i,\n", "           A.value       AS d\n", "    FROM   item_cooccurrence A\n", "    WHERE  A.row_item_id = A.col_item_id \n", "    \"\"\"\n", "    diagonal = spark.sql(query)\n", "    diagonal.createOrReplaceTempView(\"diagonal\")"]}, {"block": 17, "type": "code", "linesLength": 29, "startIndex": 152, "lines": ["similarity = None\n", "if SIMILARITY is \"jaccard\":\n", "    query = \"\"\"\n", "    SELECT A.row_item_id,\n", "           A.col_item_id,\n", "           ( A.value / ( B.d + C.d - A.value ) ) AS value\n", "    FROM   item_cooccurrence AS A,\n", "           diagonal AS B,\n", "           diagonal AS C\n", "    WHERE  A.row_item_id = B.i\n", "           AND A.col_item_id = C.i \n", "    \"\"\"\n", "    similarity = spark.sql(query)\n", "elif SIMILARITY is 'lift':\n", "    query = \"\"\"\n", "    SELECT A.row_item_id,\n", "           A.col_item_id,\n", "           ( A.value / ( B.d * C.d ) ) AS value\n", "    FROM   item_cooccurrence AS A,\n", "           diagonal AS B,\n", "           diagonal AS C\n", "    WHERE  A.row_item_id = B.i\n", "           AND A.col_item_id = C.i \n", "    \"\"\"\n", "    similarity = spark.sql(query)\n", "else:\n", "    similarity = item_cooccurrence\n", "similarity.createOrReplaceTempView(\"item_similarity\")\n", "similarity.show()"]}, {"block": 18, "type": "markdown", "linesLength": 11, "startIndex": 181, "lines": ["# 5 Compute User Affinity Scores\n", "\n", "The affinity matrix in SAR captures the strength of the relationship between each individual user and each item. The event types and weights are used in computing this matrix: different event types (such as \u201crate\u201d vs \u201cview\u201d) should be allowed to have an impact on a user\u2019s affinity for an item. Similarly, the time of a transaction should have an impact; an event that takes place in the distant past can be thought of as being less important in determining the affinity.\n", "\n", "Combining these effects gives us an expression for user-item affinity:\n", "$a_{ij}=\u03a3_k (w_k exp[-log_2((t_0-t_k)/T)] $\n", "\n", "where the affinity for user $i$ and item $j$ is the sum of all events involving user $i$ and item $j$, and $w_k$ is the weight of event $k$. The presence of the  $log_{2}$ factor means that the parameter $T$ in the exponential decay term can be treated as a half-life: events this far before the reference date $t_0$ will be given half the weight as those taking place at $t_0$. \n", "\n", "Repeating this computation for all $n$ users and $m$ items results in an $nxm$ matrix $A$.\n", "Simplifications of the above expression can be obtained by setting all the weights equal to 1 (effectively ignoring event types), or by setting the half-life parameter $T$ to infinity (ignoring transaction times).\n"]}, {"block": 19, "type": "code", "linesLength": 13, "startIndex": 192, "lines": ["query = \"\"\"\n", "SELECT A.row_id                AS row_user_id,\n", "       B.col_item_id,\n", "       Sum(A.rating * B.value) AS score\n", "FROM   df_train A\n", "       INNER JOIN item_similarity B\n", "               ON A.col_id = B.row_item_id\n", "GROUP  BY A.row_id,\n", "          B.col_item_id \n", "\"\"\"\n", "scores = spark.sql(query)\n", "scores.show()\n", "scores.count()\n"]}, {"block": 20, "type": "markdown", "linesLength": 3, "startIndex": 205, "lines": ["# 6 Remove Seen Items\n", "\n", "Optionally we remove items which have already been seen in the training set, i.e. don't recommend items which have been previously bought by the user again."]}, {"block": 21, "type": "code", "linesLength": 14, "startIndex": 208, "lines": ["if not RECOMMEND_SEEN:\n", "    print (\"Removing seen items\")\n", "    masked_scores = scores\\\n", "        .join(df, (scores.row_user_id == df.row_id) & (scores.col_item_id == df.col_id), \"left_outer\")    \n", "    masked_scores.show()\n", "    # now since training set is smaller, we have nulls under its value column, i.e. item is not in the\n", "    # training set\n", "    masked_scores = \\\n", "        masked_scores.withColumn(\"rating\", F.when(F.col('rating').isNull(), F.col('score')).otherwise(0))\n", "else:\n", "    print (\"Keeping seen items\")\n", "    scores.createOrReplaceTempView(\"scores\")\n", "    masked_scores = spark.sql(\"select row_user_id, col_item_id, score as rating from scores\")\n", "masked_scores.show()"]}, {"block": 22, "type": "markdown", "linesLength": 5, "startIndex": 222, "lines": ["# 7 Top-K Item Calculation\n", "\n", "The personalized recommendations for a set of users can then be obtained by multiplying the affinity matrix by the similarity matrix. The result is an recommendation score matrix, with one row per user / item pair; higher scores correspond to more strongly recommended items.\n", "\n", "This is the unoptimized way of performing top-K on Spark - although this is very readable:"]}, {"block": 23, "type": "code", "linesLength": 7, "startIndex": 227, "lines": ["window = Window.partitionBy(masked_scores[\"row_user_id\"]).orderBy(masked_scores[\"rating\"].desc())\n", "#top_scores =\\\n", "#    masked_scores.select(\"*\", F.rank().over(window).alias(\"top\")).filter(F.col(\"top\")<=TOP_K)\n", "top_scores =\\\n", "    masked_scores.select(\"*\", F.row_number().over(window).alias(\"top\")).filter(F.col(\"top\")<=TOP_K)\n", "top_scores.show()\n", "top_scores.count()"]}, {"block": 24, "type": "markdown", "linesLength": 1, "startIndex": 234, "lines": ["### 7.1 Optimized Top-K Item Calculation"]}, {"block": 25, "type": "code", "linesLength": 5, "startIndex": 235, "lines": ["# pivot the ratings by user and item\n", "pivoted_scores = masked_scores.withColumn(\"combined\", F.struct(\"col_item_ID\", \"rating\"))\\\n", "    .groupBy(\"row_user_id\").agg(F.collect_list(F.col(\"combined\")).alias(\"tuples\"))\n", "pivoted_scores.show()\n", "\n"]}, {"block": 26, "type": "code", "linesLength": 18, "startIndex": 240, "lines": ["# sort the items by their scores\n", "def wrapped_fun(tuples, params):\n", "    \"\"\"\n", "    Use heapq to sort the items by ratings for each user - complexity analysis provided here:\n", "    \n", "    \"\"\"\n", "    # TODO: can add params here if needed\n", "    n, sort_key = params\n", "    print(n, sort_key)\n", "    return heapq.nlargest(n, tuples, key = lambda l: l[sort_key])\n", "\n", "# wraps the above function so that we can pass in parameters in UDF\n", "def udf_wrapper_fun(params):\n", "    # notice that if needed, this can also pass in user_ID to the create_random_ratings function\n", "    schema = ArrayType(StructType((StructField(\"ItemID\", StringType()),\n", "                                   StructField(\"Rating\", FloatType()))))\n", "\n", "    return F.udf(lambda tuples: wrapped_fun(tuples, params), schema)\n"]}, {"block": 27, "type": "code", "linesLength": 4, "startIndex": 258, "lines": ["# top 2 items and 1 as sort key index\n", "params = (TOP_K, 1)\n", "sorted_pivoted_scores = pivoted_scores.withColumn(\"tuples\", udf_wrapper_fun(params)(F.col(\"tuples\")))\n", "sorted_pivoted_scores.show()"]}, {"block": 28, "type": "code", "linesLength": 7, "startIndex": 262, "lines": ["exploded_df = sorted_pivoted_scores.select(\"*\", F.explode(\"tuples\").alias(\"exploded_tuples\"))\n", "exploded_df = exploded_df\\\n", ".withColumn(\"ItemID\", F.col(\"exploded_tuples\").getItem(\"ItemID\"))\\\n", ".withColumn(\"Rating\", F.col(\"exploded_tuples\").getItem(\"Rating\"))\n", "\n", "top_scores = exploded_df.drop(\"tuples\").drop(\"exploded_tuples\")\n", "top_scores.show()"]}, {"block": 29, "type": "code", "linesLength": 0, "startIndex": 269, "lines": []}]