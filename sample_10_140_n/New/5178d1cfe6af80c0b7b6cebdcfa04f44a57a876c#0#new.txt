[{"block": 0, "type": "markdown", "linesLength": 5, "startIndex": 0, "lines": ["# SAR Deep Dive with Spark and SQL\n", "\n", "In this example, we will walkthrough each step of the SAR algorithm with an implementation using Spark and SQL.\n", "\n", "Smart Adaptive Recommendations (SAR) is a fast, scalable, adaptive algorithm for personalized recommendations based on user transaction history and item descriptions. It is powered by understanding the **similarity** between items, and recommending similar items to ones a user has an existing **affinity** for. "]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 5, "lines": ["# 0 Global Variables and Imports"]}, {"block": 2, "type": "code", "linesLength": 22, "startIndex": 6, "lines": ["# specify parameters\n", "TOP_K=2\n", "RECOMMEND_SEEN=True\n", "# options are 'jaccard', 'lift' or '' to skip and use item cooccurrence directly\n", "SIMILARITY='jaccard'\n", "\n", "import pandas as pd\n", "import numpy as np\n", "import heapq\n", "import os\n", "import pyspark.sql.functions as F\n", "import sys\n", "import pyspark\n", "from pyspark.sql.window import Window\n", "from pyspark.sql.types import StructType, StructField, StringType, Row, ArrayType, IntegerType, FloatType\n", "\n", "from pyspark.sql import SparkSession\n", "from pysarplus import SARPlus\n", "\n", "print(\"System version: {}\".format(sys.version))\n", "print(\"Pandas version: {}\".format(pd.__version__))\n", "print(\"PySpark version: {}\".format(pyspark.__version__))"]}, {"block": 3, "type": "markdown", "linesLength": 3, "startIndex": 28, "lines": ["# 1 Load Data\n", "\n", "We'll work with a small dataset here containing customer IDs, item IDs, and the customer's rating for the item. SAR requires inputs to be of the following schema: `<User ID>, <Item ID>, <Time>, [<Event Type>], [<Event Weight>]` (we will not use time or event type in the example below, and `rating` will be used as the `Event Weight`). "]}, {"block": 4, "type": "code", "linesLength": 14, "startIndex": 31, "lines": ["# There are two versions of the dataframes - the numeric version and the alphanumeric one:\n", "# they both have similar test data for top-2 recommendations and illustrate the indexing approaches to matrix multiplication on SQL\n", "d_train = {\n", "'customerID': [1,1,1,2,2,3,3],\n", "'itemID':     [1,2,3,4,5,6,1],\n", "'rating':     [5,5,5,1,1,3,5]\n", "}\n", "pdf_train = pd.DataFrame(d_train)\n", "d_test = {\n", "'customerID': [1,1,2,2,3,3],\n", "'itemID':     [4,5,1,5,6,1],\n", "'rating':     [1,1,5,5,5,5]\n", "}\n", "pdf_test = pd.DataFrame(d_test)"]}, {"block": 5, "type": "code", "linesLength": 5, "startIndex": 45, "lines": ["a_train = np.array([[5,5,5,0,0,0],\\\n", "                    [0,0,0,1,1,0],\n", "                    [5,0,0,0,0,3]])\n", "print(a_train)\n", "print(a_train.shape)"]}, {"block": 6, "type": "code", "linesLength": 15, "startIndex": 50, "lines": ["d_alnum_train = {\n", "'customerID': ['ua','ua','ua','ub','ub','uc','uc'],\n", "'itemID':     ['ia','ib','ic','id','ie','if','ia'],\n", "'rating':     [5,5,5,1,1,3,5]\n", "}\n", "#pdf_train = pd.DataFrame(d_alnum_train)\n", "pdf_train = pd.DataFrame(d_train)\n", "d_alnum_test = {\n", "'customerID': ['ua','ua','ub','ub','uc','uc'],\n", "'itemID':     ['id','ie','ia','ie','if','ia'],\n", "'rating':     [1,1,5,5,5,5]\n", "}\n", "#pdf_test = pd.DataFrame(d_alnum_test)\n", "pdf_test = pd.DataFrame(d_test)\n", "pdf_test.head(10)"]}, {"block": 7, "type": "markdown", "linesLength": 3, "startIndex": 65, "lines": ["### Set up Spark context\n", "\n", "The following settings work well for debugging locally on VM - change when running on a cluster. We set up a giant single executor with many threads and specify memory cap. "]}, {"block": 8, "type": "code", "linesLength": 12, "startIndex": 68, "lines": ["SUBMIT_ARGS = \"--packages eisber:sarplus:0.2.2 pyspark-shell\"\n", "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n", "\n", "spark = SparkSession \\\n", "    .builder \\\n", "    .appName(\"SAR pySpark\") \\\n", "    .master(\"local[*]\") \\\n", "    .config(\"memory\", \"4G\") \\\n", "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n", "    .config(\"spark.sql.crossJoin.enabled\", True) \\\n", "    .config(\"spark.ui.enabled\", False) \\\n", "    .getOrCreate()"]}, {"block": 9, "type": "code", "linesLength": 3, "startIndex": 80, "lines": ["df = spark.createDataFrame(pdf_train).withColumn(\"type\", F.lit(1))\n", "df_test = spark.createDataFrame(pdf_test).withColumn(\"type\", F.lit(0))\n", "df.toPandas()"]}, {"block": 10, "type": "markdown", "linesLength": 16, "startIndex": 83, "lines": ["# 3 Compute Item Co-occurrence and Item Similarity\n", "\n", "Central to how SAR defines similarity is an item-to-item ***co-occurrence matrix***. Co-occurrence is defined as the number of times two items appear together for a given user.  We can represent the co-occurrence of all items as a $mxm$ matrix $C$, where $c_{i,j}$   is the number of times item $i$ occurred with item $j$.\n", "\n", "The co-occurence matric $C$ has the following properties:\n", "- It is symmetric, so $c_{i,j} = c_{j,i}$\n", "- It is nonnegative: $c_{i,j} >= 0$\n", "- The occurrences are at least as large as the co-occurrences. I.e, the largest element for each row (and column) is on the main diagonal: $\u2200(i,j)\u2009C_{i,i},C_{j,j}>=C_{i,j}$.\n", "\n", "Once we have a co-occurrence matrix, an ***item similarity matrix*** $S$ can be obtained by rescaling the co-occurrences according to a given metric. Options for the metric include Jaccard, lift, and counts (meaning no rescaling).\n", "\n", "The rescaling formula for Jaccard is $s_{ij}=c_{ij} / (c_{ii}+c_{jj}-c_{ij})$\n", "\n", "and that for lift is $s_{ij}=c_{ij}/(c_{ii}*c_{jj})$\n", "\n", "where $c_{ii}$ and $c_{jj}$ are the $i$th and $j$th diagonal elements of $C$. In general, using counts as a similarity metric favours predictability, meaning that the most popular items will be recommended most of the time. Lift by contrast favours discoverability/serendipity: an item that is less popular overall but highly favoured by a small subset of users is more likely to be recommended. Jaccard is a compromise between the two."]}, {"block": 11, "type": "code", "linesLength": 4, "startIndex": 99, "lines": ["model = SARPlus(spark, col_user='customerID', col_item='itemID', col_rating='rating')\n", "model.fit(df, similarity_type=SIMILARITY)\n", "\n", "model.item_similarity.toPandas()"]}, {"block": 12, "type": "code", "linesLength": 1, "startIndex": 103, "lines": ["model.get_user_affinity(df_test).toPandas()"]}, {"block": 13, "type": "markdown", "linesLength": 22, "startIndex": 104, "lines": ["# 5 Compute User Affinity Scores\n", "\n", "The affinity matrix in SAR captures the strength of the relationship between each individual user and each item. The event types and weights are used in computing this matrix: different event types (such as \u201crate\u201d vs \u201cview\u201d) should be allowed to have an impact on a user\u2019s affinity for an item. Similarly, the time of a transaction should have an impact; an event that takes place in the distant past can be thought of as being less important in determining the affinity.\n", "\n", "Combining these effects gives us an expression for user-item affinity:\n", "$a_{ij}=\u03a3_k (w_k exp[-log_2((t_0-t_k)/T)] $\n", "\n", "where the affinity for user $i$ and item $j$ is the sum of all events involving user $i$ and item $j$, and $w_k$ is the weight of event $k$. The presence of the  $log_{2}$ factor means that the parameter $T$ in the exponential decay term can be treated as a half-life: events this far before the reference date $t_0$ will be given half the weight as those taking place at $t_0$. \n", "\n", "Repeating this computation for all $n$ users and $m$ items results in an $nxm$ matrix $A$.\n", "Simplifications of the above expression can be obtained by setting all the weights equal to 1 (effectively ignoring event types), or by setting the half-life parameter $T$ to infinity (ignoring transaction times).\n", "\n", "\n", "# 6 Remove Seen Items\n", "\n", "Optionally we remove items which have already been seen in the training set, i.e. don't recommend items which have been previously bought by the user again.\n", "\n", "# 7 Top-K Item Calculation\n", "\n", "The personalized recommendations for a set of users can then be obtained by multiplying the affinity matrix by the similarity matrix. The result is an recommendation score matrix, with one row per user / item pair; higher scores correspond to more strongly recommended items.\n", "\n", "This is the unoptimized way of performing top-K on Spark - although this is very readable:"]}, {"block": 14, "type": "code", "linesLength": 2, "startIndex": 126, "lines": ["model.recommend_k_items(df_test, cache_path='sar_deep_dive_cache', top_k=2)\\\n", "    .toPandas()"]}]