[{"block": 0, "type": "markdown", "linesLength": 9, "startIndex": 0, "lines": ["## Wikidata Knowledge Graph Extraction\n", "Many recommendation algorithms (DKN, RippleNet, KGCN) use Knowledge Graphs as an external source of information. We found that one of the bottlenecks to benchmark current algorithms like DKN, RippleNet or KGCN is that they used Microsoft Satori. As Satori is not open source, it's not possible to replicate the results found in the papers. The solution is using other open source KGs.\n", "\n", "The goal of this notebook is to provide examples of how to interact with Wikipedia queries and Wikidata to extract a Knowledge Graph that can be used with the mentioned algorithms.\n", "\n", "The steps covered are:\n", "- How to find a Wikidata entity (https://www.wikidata.org/wiki/Wikidata:Glossary/en from a text query\n", "- How to find surrounding entities and descriptions for an entity\n", "- Create a KG for Movielens"]}, {"block": 1, "type": "code", "linesLength": 14, "startIndex": 9, "lines": ["# set the environment path to find Recommenders\n", "import sys\n", "sys.path.append(\"../../\")\n", "print(\"System version: {}\".format(sys.version))\n", "\n", "import pandas as pd\n", "from reco_utils.dataset.wikidata import search_wikidata\n", "\n", "import networkx as nx\n", "import matplotlib.pyplot as plt\n", "from tqdm import tqdm\n", "\n", "from reco_utils.dataset import movielens\n", "from reco_utils.common.notebook_utils import is_jupyter"]}, {"block": 2, "type": "code", "linesLength": 4, "startIndex": 23, "lines": ["# Select MovieLens data size: 100k, 1m, 10m, or 20m\n", "MOVIELENS_DATA_SIZE = '100k'\n", "MOVIELENS_SAMPLE = True\n", "MOVIELENS_SAMPLE_SIZE = 50"]}, {"block": 3, "type": "markdown", "linesLength": 1, "startIndex": 27, "lines": ["## 1. Create a KG from linked entities in Wikidata"]}, {"block": 4, "type": "code", "linesLength": 1, "startIndex": 28, "lines": ["names = [\"The Godfather\", \"Al Pacino\", \"Tom Hanks\", \"Forrest Gump\", \"Julia Roberts\", \"\", \"My Best Friend's Wedding\"]"]}, {"block": 5, "type": "code", "linesLength": 3, "startIndex": 29, "lines": ["%%time\n", "results_list = search_wikidata(names)\n", "results_list.head()"]}, {"block": 6, "type": "markdown", "linesLength": 1, "startIndex": 32, "lines": ["### Visualize KG using networkx"]}, {"block": 7, "type": "code", "linesLength": 1, "startIndex": 33, "lines": ["G = nx.from_pandas_edgelist(results_list, 'original_entity', 'linked_entities')"]}, {"block": 8, "type": "code", "linesLength": 5, "startIndex": 34, "lines": ["target_names = results_list[[\"linked_entities\", \"name_linked_entities\"]].drop_duplicates().rename(columns={\"linked_entities\": \"labels\", \"name_linked_entities\": \"name\"})\n", "source_names = results_list[[\"original_entity\", \"name\"]].drop_duplicates().rename(columns={\"original_entity\": \"labels\"})\n", "names = pd.concat([target_names, source_names])\n", "names = names.set_index(\"labels\")\n", "names = names.to_dict()[\"name\"]"]}, {"block": 9, "type": "code", "linesLength": 5, "startIndex": 39, "lines": ["plt.figure(figsize=(12,12)) \n", "pos = nx.spring_layout(G)\n", "nx.draw(G,pos, node_size=60,font_size=9, width = 0.2)\n", "nx.draw_networkx_labels(G, pos, names, font_size=9)\n", "plt.show()"]}, {"block": 10, "type": "markdown", "linesLength": 1, "startIndex": 44, "lines": ["## 2. Create a KG from the Movielens Dataset"]}, {"block": 11, "type": "code", "linesLength": 8, "startIndex": 45, "lines": ["# Obtain pairs of Movie Title - IDs from Movielens\n", "df = movielens.load_pandas_df(MOVIELENS_DATA_SIZE,\n", "                              ('UserId', 'ItemId', 'Rating', 'Timestamp'),\n", "                             title_col='Title',\n", "                             genres_col='Genres',\n", "                             year_col='Year'\n", "        )\n", "movies = df[[\"Title\", \"ItemId\"]].drop_duplicates().reset_index()"]}, {"block": 12, "type": "code", "linesLength": 1, "startIndex": 53, "lines": ["movies[\"Title\"][0:5]"]}, {"block": 13, "type": "code", "linesLength": 3, "startIndex": 54, "lines": ["# For notebook testing\n", "if MOVIELENS_SAMPLE == True:\n", "    movies = movies.head(MOVIELENS_SAMPLE_SIZE)"]}, {"block": 14, "type": "code", "linesLength": 1, "startIndex": 57, "lines": ["movies.shape"]}, {"block": 15, "type": "code", "linesLength": 3, "startIndex": 58, "lines": ["names = [t + ' film' for t in movies['Title']]\n", "result = search_wikidata(names, extras=movies[['Title', 'ItemId']].to_dict())\n", "result.head()"]}, {"block": 16, "type": "code", "linesLength": 1, "startIndex": 61, "lines": ["result[\"Title\"].value_counts()"]}, {"block": 17, "type": "code", "linesLength": 1, "startIndex": 62, "lines": ["# result.to_csv(\"movielens_\" + MOVIELENS_DATA_SIZE + '_wikidata.csv', index = False)"]}, {"block": 18, "type": "code", "linesLength": 1, "startIndex": 63, "lines": ["number_movies = len(result[\"Title\"].unique())"]}, {"block": 19, "type": "code", "linesLength": 5, "startIndex": 64, "lines": ["# Record results with papermill for tests - ignore this cell\n", "if is_jupyter():\n", "    # Record results with papermill for unit-tests\n", "    import papermill as pm\n", "    pm.record(\"length_result\", number_movies)"]}, {"block": 20, "type": "code", "linesLength": 0, "startIndex": 69, "lines": []}]