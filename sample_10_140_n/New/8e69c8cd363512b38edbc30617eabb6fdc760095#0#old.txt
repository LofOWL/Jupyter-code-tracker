[{"block": 0, "type": "code", "linesLength": 1, "startIndex": 0, "lines": ["## parameters:\n## run describe on the input table\ndescribe = False\n## save the result of the preprocessing pipeline as a table.\nsave_as_table = True\n\n## for testing - pipeline will include categorical variables 0:n_sparse_features in the features table to estimate the model.\nn_sparse_features = 0  \n## for testing - pipeline will include numeric variables 0:n_num_features in the features table to estimate the model.\nn_num_features = 5\nclassifier_lightgbm_iterations = 3  \nn_folds = 4                         \nnum_leaves_grid = [2, 4, 8]"]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 1, "lines": ["## load data loader"]}, {"block": 2, "type": "code", "linesLength": 1, "startIndex": 2, "lines": ["## from reco_utils.dataset.criteo_dac import load_spark_df"]}, {"block": 3, "type": "code", "linesLength": 1, "startIndex": 3, "lines": ["## read in the data - this takes some time...8-10 minutes\n## df = load_spark_df(spark=spark, dbutils=dbutils)\ndf = sqlContext.read.parquet(\"/FileStore/dac_train.parquet\")\n# Could ADLS be causing issues?\n# df = sqlContext.read.parquet(\"/mnt/adlsgen2/dac_train.parquet\")"]}, {"block": 4, "type": "code", "linesLength": 1, "startIndex": 4, "lines": ["if describe:\n  ## This can take quite a bit of time...\n  cur_descr = df.describe()\n  display(cur_descr)"]}, {"block": 5, "type": "code", "linesLength": 1, "startIndex": 5, "lines": ["## boundary check n_sparse_features\nif n_sparse_features < 0 or n_sparse_features > 26:\n  raise ValueError('n_sparse_features must be between 0 and 26...')\nelse:\n  print('Running with {} sparse (i.e. categorical) features.'.format(n_sparse_features))\n  \nif n_num_features < 0 or n_num_features > 13:\n  raise ValueError('n_sparse_features must be between 0 and 26...')\nelse:\n  print('Running with {} numeric features.'.format(n_num_features))\n  \nif n_num_features+n_sparse_features < 1:\n  raise ValueError('total number of features is less than 1.')"]}, {"block": 6, "type": "markdown", "linesLength": 1, "startIndex": 6, "lines": ["## Imports"]}, {"block": 7, "type": "code", "linesLength": 1, "startIndex": 7, "lines": ["## for feature engineering:\nfrom pyspark.ml.feature import (Imputer,StringIndexer,VectorAssembler)\nfrom pyspark.ml.pipeline import Pipeline\n\n## for modeling:\nfrom mmlspark import LightGBMClassifier\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n"]}, {"block": 8, "type": "markdown", "linesLength": 1, "startIndex": 8, "lines": ["## Define what features to process\n\n- `features` maps to numeric features that need to have missing values replaced\n- `sparse_features` maps to the first `n_sparse_features` categorical / string variables"]}, {"block": 9, "type": "code", "linesLength": 1, "startIndex": 9, "lines": ["\n## features are int features (does median imputation)\nfeatures = [x for x in df.columns if x[0:3] == 'int'][0:n_num_features]\n## sparse_features are str features \nsparse_features = [x for x in df.columns if x[0:3] == 'cat'][0:n_sparse_features]\n"]}, {"block": 10, "type": "markdown", "linesLength": 1, "startIndex": 10, "lines": ["## Recast `int` variables to `float`\n\n`Imputer()` only works with `float` or `double` type. We could import the data as floats, or run directly on ints using the `df.na.fill()` method.\n\nCurrently using this approach to keep the work in the pipeline."]}, {"block": 11, "type": "code", "linesLength": 1, "startIndex": 11, "lines": ["## cast ints to floats, because Imputer only works with floats\nsql_lst = ['cast({} as float) {}'.format(x, x) for x in features] + sparse_features + ['label']\nrecast_df = df.selectExpr(*[sql_lst])"]}, {"block": 12, "type": "code", "linesLength": 1, "startIndex": 12, "lines": ["pipeline = Pipeline(stages=[\n  Imputer(strategy='median',\n          inputCols=features,\n          outputCols=[f + '_imp' for f in features]),\n  # LightGBM can handle categoricals directly if StringIndexer is used through meta-data\n  *[StringIndexer(inputCol=f , outputCol=f+'_vec') for f in sparse_features],\n  VectorAssembler(inputCols= [f + '_imp' for f in features] +\n                  [f + '_vec' for f in sparse_features],\n                  outputCol='features')\n])"]}, {"block": 13, "type": "code", "linesLength": 1, "startIndex": 13, "lines": ["# fit is needed if you use imputer..\ntrain_proc_df = pipeline.fit(recast_df).transform(recast_df)"]}, {"block": 14, "type": "code", "linesLength": 1, "startIndex": 14, "lines": ["table_to_save = 'criteo_dac_proc_{}sparse_{}num'.format(n_sparse_features,n_num_features)\n\ntry:\n  train_proc_df.write.saveAsTable(table_to_save)\nexcept:\n  pass"]}, {"block": 15, "type": "markdown", "linesLength": 1, "startIndex": 15, "lines": ["## Set up the Classifier:"]}, {"block": 16, "type": "code", "linesLength": 1, "startIndex": 16, "lines": ["model = LightGBMClassifier(featuresCol='features',\n                           labelCol='label',\n                           numIterations=classifier_lightgbm_iterations,\n                           numLeaves=8,\n                           isUnbalance=True)\n\ngrid = (ParamGridBuilder()\n        .addGrid(model.numLeaves, num_leaves_grid) \n        .build())\n\nevaluator = BinaryClassificationEvaluator(labelCol='label')\n\ncv = CrossValidator(estimator=model, estimatorParamMaps=grid, evaluator=evaluator, numFolds=n_folds)"]}, {"block": 17, "type": "markdown", "linesLength": 1, "startIndex": 17, "lines": ["## Fit the model."]}, {"block": 18, "type": "code", "linesLength": 1, "startIndex": 18, "lines": ["## try just fitting the model, not with CV\n## model fit works, sometimes.\nmodel_fit = model.fit(train_proc_df)"]}, {"block": 19, "type": "code", "linesLength": 1, "startIndex": 19, "lines": ["## estimate the model:\n## throws an error:\ncv_fit = cv.fit(train_proc_df)\n"]}, {"block": 20, "type": "code", "linesLength": 1, "startIndex": 20, "lines": [""]}]