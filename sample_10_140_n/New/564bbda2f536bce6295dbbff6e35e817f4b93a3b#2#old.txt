[{"block": 0, "type": "code", "linesLength": 8, "startIndex": 0, "lines": ["%reload_ext autoreload\n", "%autoreload 2\n", "%matplotlib inline\n", "\n", "from fastai.io import *\n", "from fastai.conv_learner import *\n", "\n", "from fastai.column_data import *"]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 8, "lines": ["## Setup"]}, {"block": 2, "type": "markdown", "linesLength": 1, "startIndex": 9, "lines": ["We're going to download the collected works of Nietzsche to use as our data for this class."]}, {"block": 3, "type": "code", "linesLength": 1, "startIndex": 10, "lines": ["PATH='data/'"]}, {"block": 4, "type": "code", "linesLength": 3, "startIndex": 11, "lines": ["get_data(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\", f'{PATH}nietzsche.txt')\n", "text = open(f'{PATH}nietzsche.txt').read()\n", "print('corpus length:', len(text))"]}, {"block": 5, "type": "code", "linesLength": 3, "startIndex": 14, "lines": ["chars = sorted(list(set(text)))\n", "vocab_size = len(chars)+1\n", "print('total chars:', vocab_size)"]}, {"block": 6, "type": "markdown", "linesLength": 1, "startIndex": 17, "lines": ["Sometimes it's useful to have a zero value in the dataset, e.g. for padding"]}, {"block": 7, "type": "code", "linesLength": 1, "startIndex": 18, "lines": ["chars.insert(0, \"\\0\")"]}, {"block": 8, "type": "code", "linesLength": 1, "startIndex": 19, "lines": ["''.join(chars[1:-6])"]}, {"block": 9, "type": "markdown", "linesLength": 1, "startIndex": 20, "lines": ["Map from chars to indices and back again"]}, {"block": 10, "type": "code", "linesLength": 2, "startIndex": 21, "lines": ["char_indices = dict((c, i) for i, c in enumerate(chars))\n", "indices_char = dict((i, c) for i, c in enumerate(chars))"]}, {"block": 11, "type": "markdown", "linesLength": 1, "startIndex": 23, "lines": ["*idx* will be the data we use from now own - it simply converts all the characters to their index (based on the mapping above)"]}, {"block": 12, "type": "code", "linesLength": 1, "startIndex": 24, "lines": ["idx = [char_indices[c] for c in text]"]}, {"block": 13, "type": "code", "linesLength": 1, "startIndex": 25, "lines": ["idx[:10]"]}, {"block": 14, "type": "code", "linesLength": 1, "startIndex": 26, "lines": ["''.join(indices_char[i] for i in idx[:70])"]}, {"block": 15, "type": "markdown", "linesLength": 1, "startIndex": 27, "lines": ["## Three char model"]}, {"block": 16, "type": "markdown", "linesLength": 1, "startIndex": 28, "lines": ["### Create inputs"]}, {"block": 17, "type": "markdown", "linesLength": 1, "startIndex": 29, "lines": ["Create a list of every 4th character, starting at the 0th, 1st, 2nd, then 3rd characters"]}, {"block": 18, "type": "code", "linesLength": 5, "startIndex": 30, "lines": ["cs=3\n", "c1_dat = [idx[i]   for i in range(0, len(idx)-1-cs, cs)]\n", "c2_dat = [idx[i+1] for i in range(0, len(idx)-1-cs, cs)]\n", "c3_dat = [idx[i+2] for i in range(0, len(idx)-1-cs, cs)]\n", "c4_dat = [idx[i+3] for i in range(0, len(idx)-1-cs, cs)]"]}, {"block": 19, "type": "markdown", "linesLength": 1, "startIndex": 35, "lines": ["Our inputs"]}, {"block": 20, "type": "code", "linesLength": 3, "startIndex": 36, "lines": ["x1 = np.stack(c1_dat[:-2])\n", "x2 = np.stack(c2_dat[:-2])\n", "x3 = np.stack(c3_dat[:-2])"]}, {"block": 21, "type": "markdown", "linesLength": 1, "startIndex": 39, "lines": ["Our output"]}, {"block": 22, "type": "code", "linesLength": 1, "startIndex": 40, "lines": ["y = np.stack(c4_dat[:-2])"]}, {"block": 23, "type": "markdown", "linesLength": 1, "startIndex": 41, "lines": ["The first 4 inputs and outputs"]}, {"block": 24, "type": "code", "linesLength": 1, "startIndex": 42, "lines": ["x1[:4], x2[:4], x3[:4]"]}, {"block": 25, "type": "code", "linesLength": 1, "startIndex": 43, "lines": ["y[:4]"]}, {"block": 26, "type": "code", "linesLength": 1, "startIndex": 44, "lines": ["x1.shape, y.shape"]}, {"block": 27, "type": "markdown", "linesLength": 1, "startIndex": 45, "lines": ["The number of latent factors to create (i.e. the size of the embedding matrix)"]}, {"block": 28, "type": "code", "linesLength": 1, "startIndex": 46, "lines": ["n_fac = 42"]}, {"block": 29, "type": "markdown", "linesLength": 1, "startIndex": 47, "lines": ["Create inputs and embedding outputs for each of our 3 character inputs"]}, {"block": 30, "type": "markdown", "linesLength": 1, "startIndex": 48, "lines": ["### Create and train model"]}, {"block": 31, "type": "markdown", "linesLength": 1, "startIndex": 49, "lines": ["Pick a size for our hidden state"]}, {"block": 32, "type": "code", "linesLength": 1, "startIndex": 50, "lines": ["n_hidden = 256"]}, {"block": 33, "type": "code", "linesLength": 25, "startIndex": 51, "lines": ["class Char3Model(nn.Module):\n", "    def __init__(self, vocab_size, n_fac):\n", "        super().__init__()\n", "        self.e = nn.Embedding(vocab_size, n_fac)\n", "\n", "        # The 'green arrow' from our diagram - the layer operation from input to hidden\n", "        self.l_in = nn.Linear(n_fac, n_hidden)\n", "\n", "        # The 'orange arrow' from our diagram - the layer operation from hidden to hidden\n", "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n", "        \n", "        # The 'blue arrow' from our diagram - the layer operation from hidden to output\n", "        self.l_out = nn.Linear(n_hidden, vocab_size)\n", "        \n", "    def forward(self, c1, c2, c3):\n", "        in1 = F.relu(self.l_in(self.e(c1)))\n", "        in2 = F.relu(self.l_in(self.e(c2)))\n", "        in3 = F.relu(self.l_in(self.e(c3)))\n", "        \n", "        h = V(torch.zeros(in1.size()).cuda())\n", "        h = F.tanh(self.l_hidden(h+in1))\n", "        h = F.tanh(self.l_hidden(h+in2))\n", "        h = F.tanh(self.l_hidden(h+in3))\n", "        \n", "        return F.log_softmax(self.l_out(h))"]}, {"block": 34, "type": "code", "linesLength": 1, "startIndex": 76, "lines": ["md = ColumnarModelData.from_arrays('.', [-1], np.stack([x1,x2,x3], axis=1), y, bs=512)"]}, {"block": 35, "type": "code", "linesLength": 1, "startIndex": 77, "lines": ["m = Char3Model(vocab_size, n_fac).cuda()"]}, {"block": 36, "type": "code", "linesLength": 3, "startIndex": 78, "lines": ["it = iter(md.trn_dl)\n", "*xs,yt = next(it)\n", "t = m(*V(xs))"]}, {"block": 37, "type": "code", "linesLength": 1, "startIndex": 81, "lines": ["opt = optim.Adam(m.parameters(), 1e-2)"]}, {"block": 38, "type": "code", "linesLength": 1, "startIndex": 82, "lines": ["fit(m, md, 1, opt, F.nll_loss)"]}, {"block": 39, "type": "code", "linesLength": 1, "startIndex": 83, "lines": ["set_lrs(opt, 0.001)"]}, {"block": 40, "type": "code", "linesLength": 1, "startIndex": 84, "lines": ["fit(m, md, 1, opt, F.nll_loss)"]}, {"block": 41, "type": "markdown", "linesLength": 1, "startIndex": 85, "lines": ["### Test model"]}, {"block": 42, "type": "code", "linesLength": 5, "startIndex": 86, "lines": ["def get_next(inp):\n", "    idxs = T(np.array([char_indices[c] for c in inp]))\n", "    p = m(*VV(idxs))\n", "    i = np.argmax(to_np(p))\n", "    return chars[i]"]}, {"block": 43, "type": "code", "linesLength": 1, "startIndex": 91, "lines": ["get_next('y. ')"]}, {"block": 44, "type": "code", "linesLength": 1, "startIndex": 92, "lines": ["get_next('ppl')"]}, {"block": 45, "type": "code", "linesLength": 1, "startIndex": 93, "lines": ["get_next(' th')"]}, {"block": 46, "type": "code", "linesLength": 1, "startIndex": 94, "lines": ["get_next('and')"]}, {"block": 47, "type": "markdown", "linesLength": 1, "startIndex": 95, "lines": ["## Our first RNN!"]}, {"block": 48, "type": "markdown", "linesLength": 1, "startIndex": 96, "lines": ["### Create inputs"]}, {"block": 49, "type": "markdown", "linesLength": 1, "startIndex": 97, "lines": ["This is the size of our unrolled RNN."]}, {"block": 50, "type": "code", "linesLength": 1, "startIndex": 98, "lines": ["cs=8"]}, {"block": 51, "type": "markdown", "linesLength": 1, "startIndex": 99, "lines": ["For each of 0 through 7, create a list of every 8th character with that starting point. These will be the 8 inputs to out model."]}, {"block": 52, "type": "code", "linesLength": 2, "startIndex": 100, "lines": ["c_in_dat = [[idx[i+n] for i in range(0, len(idx)-1-cs, cs)]\n", "            for n in range(cs)]"]}, {"block": 53, "type": "code", "linesLength": 1, "startIndex": 102, "lines": ["c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(len(idx)-cs-1)]"]}, {"block": 54, "type": "markdown", "linesLength": 1, "startIndex": 103, "lines": ["Then create a list of the next character in each of these series. This will be the labels for our model."]}, {"block": 55, "type": "code", "linesLength": 1, "startIndex": 104, "lines": ["c_out_dat = [idx[i+cs] for i in range(0, len(idx)-1-cs, cs)]"]}, {"block": 56, "type": "code", "linesLength": 1, "startIndex": 105, "lines": ["c_out_dat = [idx[j+cs] for j in range(len(idx)-cs-1)]"]}, {"block": 57, "type": "code", "linesLength": 1, "startIndex": 106, "lines": ["xs = np.stack(c_in_dat, axis=1)"]}, {"block": 58, "type": "code", "linesLength": 1, "startIndex": 107, "lines": ["xs.shape"]}, {"block": 59, "type": "code", "linesLength": 1, "startIndex": 108, "lines": ["y = np.stack(c_out_dat)"]}, {"block": 60, "type": "markdown", "linesLength": 1, "startIndex": 109, "lines": ["So each column below is one series of 8 characters from the text."]}, {"block": 61, "type": "code", "linesLength": 1, "startIndex": 110, "lines": ["[xs[n][:cs] for n in range(cs)]"]}, {"block": 62, "type": "markdown", "linesLength": 1, "startIndex": 111, "lines": ["...and this is the next character after each sequence."]}, {"block": 63, "type": "code", "linesLength": 1, "startIndex": 112, "lines": ["y[:cs]"]}, {"block": 64, "type": "markdown", "linesLength": 1, "startIndex": 113, "lines": ["### Create and train model"]}, {"block": 65, "type": "code", "linesLength": 1, "startIndex": 114, "lines": ["val_idx = get_cv_idxs(len(idx)-cs-1)"]}, {"block": 66, "type": "code", "linesLength": 1, "startIndex": 115, "lines": ["md = ColumnarModelData.from_arrays('.', val_idx, np.stack(xs, axis=1), y, bs=512)"]}, {"block": 67, "type": "code", "linesLength": 16, "startIndex": 116, "lines": ["class CharLoopModel(nn.Module):\n", "    def __init__(self, vocab_size, n_fac):\n", "        super().__init__()\n", "        self.e = nn.Embedding(vocab_size, n_fac)\n", "        self.l_in = nn.Linear(n_fac, n_hidden)\n", "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n", "        self.l_out = nn.Linear(n_hidden, vocab_size)\n", "        \n", "    def forward(self, *cs):\n", "        bs = cs[0].size(0)\n", "        h = V(torch.zeros(bs, n_hidden).cuda())\n", "        for c in cs:\n", "            inp = F.relu(self.l_in(self.e(c)))\n", "            h = F.tanh(self.l_hidden(h+inp))\n", "        \n", "        return F.log_softmax(self.l_out(h))"]}, {"block": 68, "type": "code", "linesLength": 2, "startIndex": 132, "lines": ["m = CharLoopModel(vocab_size, n_fac).cuda()\n", "opt = optim.Adam(m.parameters(), 1e-2)"]}, {"block": 69, "type": "code", "linesLength": 1, "startIndex": 134, "lines": ["fit(m, md, 1, opt, F.nll_loss)"]}, {"block": 70, "type": "code", "linesLength": 1, "startIndex": 135, "lines": ["set_lrs(opt, 0.001)"]}, {"block": 71, "type": "code", "linesLength": 1, "startIndex": 136, "lines": ["fit(m, md, 1, opt, F.nll_loss)"]}, {"block": 72, "type": "code", "linesLength": 17, "startIndex": 137, "lines": ["class CharLoopConcatModel(nn.Module):\n", "    def __init__(self, vocab_size, n_fac):\n", "        super().__init__()\n", "        self.e = nn.Embedding(vocab_size, n_fac)\n", "        self.l_in = nn.Linear(n_fac+n_hidden, n_hidden)\n", "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n", "        self.l_out = nn.Linear(n_hidden, vocab_size)\n", "        \n", "    def forward(self, *cs):\n", "        bs = cs[0].size(0)\n", "        h = V(torch.zeros(bs, n_hidden).cuda())\n", "        for c in cs:\n", "            inp = torch.cat((h, self.e(c)), 1)\n", "            inp = F.relu(self.l_in(inp))\n", "            h = F.tanh(self.l_hidden(inp))\n", "        \n", "        return F.log_softmax(self.l_out(h))"]}, {"block": 73, "type": "code", "linesLength": 2, "startIndex": 154, "lines": ["m = CharLoopConcatModel(vocab_size, n_fac).cuda()\n", "opt = optim.Adam(m.parameters(), 1e-3)"]}, {"block": 74, "type": "code", "linesLength": 3, "startIndex": 156, "lines": ["it = iter(md.trn_dl)\n", "*xs,yt = next(it)\n", "t = m(*V(xs))"]}, {"block": 75, "type": "code", "linesLength": 1, "startIndex": 159, "lines": ["fit(m, md, 1, opt, F.nll_loss)"]}, {"block": 76, "type": "code", "linesLength": 1, "startIndex": 160, "lines": ["set_lrs(opt, 1e-4)"]}, {"block": 77, "type": "code", "linesLength": 1, "startIndex": 161, "lines": ["fit(m, md, 1, opt, F.nll_loss)"]}, {"block": 78, "type": "markdown", "linesLength": 1, "startIndex": 162, "lines": ["### Test model"]}, {"block": 79, "type": "code", "linesLength": 5, "startIndex": 163, "lines": ["def get_next(inp):\n", "    idxs = T(np.array([char_indices[c] for c in inp]))\n", "    p = m(*VV(idxs))\n", "    i = np.argmax(to_np(p))\n", "    return chars[i]"]}, {"block": 80, "type": "code", "linesLength": 1, "startIndex": 168, "lines": ["get_next('for thos')"]}, {"block": 81, "type": "code", "linesLength": 1, "startIndex": 169, "lines": ["get_next('part of ')"]}, {"block": 82, "type": "code", "linesLength": 1, "startIndex": 170, "lines": ["get_next('queens a')"]}, {"block": 83, "type": "markdown", "linesLength": 1, "startIndex": 171, "lines": ["## RNN with pytorch"]}, {"block": 84, "type": "code", "linesLength": 14, "startIndex": 172, "lines": ["class CharRnn(nn.Module):\n", "    def __init__(self, vocab_size, n_fac):\n", "        super().__init__()\n", "        self.e = nn.Embedding(vocab_size, n_fac)\n", "        self.rnn = nn.RNN(n_fac, n_hidden)\n", "        self.l_out = nn.Linear(n_hidden, vocab_size)\n", "        \n", "    def forward(self, *cs):\n", "        bs = cs[0].size(0)\n", "        h = V(torch.zeros(1, bs, n_hidden))\n", "        inp = self.e(torch.stack(cs))\n", "        outp,h = self.rnn(inp, h)\n", "        \n", "        return F.log_softmax(self.l_out(outp[-1]))"]}, {"block": 85, "type": "code", "linesLength": 2, "startIndex": 186, "lines": ["m = CharRnn(vocab_size, n_fac).cuda()\n", "opt = optim.Adam(m.parameters(), 1e-3)"]}, {"block": 86, "type": "code", "linesLength": 2, "startIndex": 188, "lines": ["it = iter(md.trn_dl)\n", "*xs,yt = next(it)"]}, {"block": 87, "type": "code", "linesLength": 2, "startIndex": 190, "lines": ["t = m.e(V(torch.stack(xs)))\n", "t.size()"]}, {"block": 88, "type": "code", "linesLength": 3, "startIndex": 192, "lines": ["ht = V(torch.zeros(1, 512,n_hidden))\n", "outp, hn = m.rnn(t, ht)\n", "outp.size(), hn.size()"]}, {"block": 89, "type": "code", "linesLength": 1, "startIndex": 195, "lines": ["t = m(*V(xs)); t.size()"]}, {"block": 90, "type": "code", "linesLength": 1, "startIndex": 196, "lines": ["fit(m, md, 4, opt, F.nll_loss)"]}, {"block": 91, "type": "code", "linesLength": 1, "startIndex": 197, "lines": ["set_lrs(opt, 1e-4)"]}, {"block": 92, "type": "code", "linesLength": 1, "startIndex": 198, "lines": ["fit(m, md, 2, opt, F.nll_loss)"]}, {"block": 93, "type": "markdown", "linesLength": 1, "startIndex": 199, "lines": ["### Test model"]}, {"block": 94, "type": "code", "linesLength": 5, "startIndex": 200, "lines": ["def get_next(inp):\n", "    idxs = T(np.array([char_indices[c] for c in inp]))\n", "    p = m(*VV(idxs))\n", "    i = np.argmax(to_np(p))\n", "    return chars[i]"]}, {"block": 95, "type": "code", "linesLength": 1, "startIndex": 205, "lines": ["get_next('for thos')"]}, {"block": 96, "type": "code", "linesLength": 7, "startIndex": 206, "lines": ["def get_next_n(inp, n):\n", "    res = inp\n", "    for i in range(n):\n", "        c = get_next(inp)\n", "        res += c\n", "        inp = inp[1:]+c\n", "    return res"]}, {"block": 97, "type": "code", "linesLength": 1, "startIndex": 213, "lines": ["get_next_n('for thos', 40)"]}, {"block": 98, "type": "code", "linesLength": 0, "startIndex": 214, "lines": []}]