[{"block": 0, "type": "markdown", "linesLength": 26, "startIndex": 0, "lines": ["# Configure Azure Databricks for Reference Architecture for Recommendation Systems.\n", "\n", "The goal of this notebook is to simplify the deployment of the real-time recommendation API on Azure architecture as described [here](https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/ai/real-time-recommendation).\n", "\n", "Specifically, this notebook allows you to programmatically do the following:\n", "\n", "1. Create a databricks cluster of the appropriate version\n", "2. Install necessary libraries.\n", "3. Upload the primary notebook to the databricks workspace if it doesn't alreadyd exist.\n", "\n", "The primary impetus for this is to simplify the creation of the databricks cluster for the end-to-end walkthrough in the notebook [here](./als_movie_o16n.ipynb). As of 2019-01-17, the recommended version of spark can no longer be deployed through the databricks portal because it is deprecated.\n", "\n", "## Running this notebook\n", "\n", "You can run this notebook in two different ways. In both cases, please see the notes regarding dependencies. \n", "\n", "1. You can manually run through the notebook through jupyter. When doing so, make sure to adjust your `TOKEN`, `DOMAIN`, and other parameters in the first code cell below.\n", "2. You can install [`papermill`](https://github.com/nteract/papermill) and run it using papermill. When doing so, the easiest approach is to pass the relevant parameters (from the first code cell below) as arguments, e.g.:\n", "\n", "```\n", "papermill create_and_configure_cluster.ipynb OUTPUT.ipynb -p TOKEN XXXXXXXXXXXXXXXXXXXXX -p DOMAIN westus.azuredatabricks.net -p cluster_name my_papermill_cluster\n", "```\n", "\n", "## Known Issues\n", "\n", "You may see SSL errors occasionally, or libraries may fail to get installed. The best way to do fix this is to restart the cluster."]}, {"block": 1, "type": "markdown", "linesLength": 53, "startIndex": 26, "lines": ["## Dependencies\n", "\n", "\n", "### Python dependencies\n", "\n", "In order to execute this notebook, you must have two python libraries installed:\n", "\n", "- `python-dotenv` to load and manage an access token if it is not being run via papermill (see authentication below)\n", "- `requests` to send the REST calls\n", "\n", "If you want to run the associated tests, then the environment also requires\n", "\n", "- `papermill`\n", "\n", "Both are installed into a conda environment by running the following command in the same directory as this notebook:\n", "\n", "```\n", "conda env create -f dbapi_conda.yml\n", "```\n", "\n", "### Other dependencies\n", "\n", "Additionally, you must also have:\n", "\n", "- An azure databricks workspace\n", "- Appropriate permissions to create and modify clusters \n", "- A personal access token for authentication to the workspace (see below).\n", "\n", "We also assume that you have cloned or downloaded the [Microsoft Recommenders repository](https://github.com/Microsoft/Recommenders). This notebook is a part\n", "of that repository, \n", "You must adjust the variable `path_to_recommenders_repo_root` in order to upload that as a library.\n", "\n", "### Authentication Instructions\n", "\n", "This will work through an example using a personal access token.\n", "\n", "In order to use this approach, you need to generate a databricks personal access token.\n", "\n", "To do so, do the following:\n", "\n", "- Click on the `user` icon on the top right\n", "- Click on the `User Settings` menu item\n", "- Select the `Access Tokens` tab\n", "- Click on the `Generate Access Token`\n", "- Fill in the requested fields\n", "\n", "Copy that token into a file called `.env` with the format:\n", "\n", "```\n", "DB_ACCESS_TOKEN=<THISISMYLONGSTRINGFROMDATABRICKS>\n", "```\n", "\n", "If `TOKEN` is left empty, we will use this file later with the `python-dotenv` package to load the access token in such a way that it is not visible\n"]}, {"block": 2, "type": "markdown", "linesLength": 3, "startIndex": 79, "lines": ["## Parameters\n", "\n", "First, set up parameters so that the notebook can be executed programmatically using [`papermill`](https://github.com/nteract/papermill). Note that the following cell has the `parameters` tag."]}, {"block": 3, "type": "code", "linesLength": 28, "startIndex": 82, "lines": ["## variables that need to be updated\n", "\n", "\n", "## Personal access token for the existing databricks workspace\n", "##     NOTE: If this is left blank, it will then look for a .env file to load \n", "##           using dotenv. If it is left blank, and no .env is available, \n", "##           then the notebook will fail.\n", "TOKEN = \"\" \n", "\n", "## location of the recommenders repository root directory.\n", "## If you have downloaded the Recommenders repository and are running this notebook \n", "## manually with Jupyter, then the relative path default should be accurate.\n", "## If you have downloaded this file separately\n", "## then you need to adjust the value.\n", "## If you are running this with papermill, the value is relative to where you invoke papermill\n", "path_to_recommenders_repo_root = \"../../\" \n", "\n", "## record data and outcomes for testing?\n", "record_for_tests = False\n", "\n", "## additional things that can be configured:\n", "DOMAIN = 'eastus.azuredatabricks.net'\n", "cluster_name = 'reco-db4.1-api'\n", "node_type_id = \"Standard_D3_v2\"\n", "min_workers = 2\n", "max_workers = 6\n", "autotermination_minutes = 60\n", "upload_location_for_endtoend_notebook = \"/Shared\"\n"]}, {"block": 4, "type": "markdown", "linesLength": 1, "startIndex": 110, "lines": ["## Load modules"]}, {"block": 5, "type": "code", "linesLength": 12, "startIndex": 111, "lines": ["## should be available on a standard python install\n", "import os\n", "import json\n", "import base64\n", "import sys\n", "import shutil\n", "\n", "## installed for this project:\n", "import requests\n", "## add to log results with papermill, only\n", "if record_for_tests:\n", "    import papermill as pm\n"]}, {"block": 6, "type": "code", "linesLength": 3, "startIndex": 123, "lines": ["print('\\n**** Current working directory ****\\n%s\\n' %(os.getcwd()))\n", "print('\\n**** Root of Recommenders repository ****\\n%s\\n' %(os.path.abspath(path_to_recommenders_repo_root)))\n", "print(sys.executable)"]}, {"block": 7, "type": "markdown", "linesLength": 1, "startIndex": 126, "lines": ["## Setup the base URL and header for the requests"]}, {"block": 8, "type": "code", "linesLength": 11, "startIndex": 127, "lines": ["if TOKEN == \"\":\n", "    ## If the token isn't passed or updated above, try loading it from env variable:\n", "    from dotenv import load_dotenv\n", "    load_dotenv(verbose=True)\n", "    TOKEN = os.getenv('DB_ACCESS_TOKEN') \n", "    \n", "assert (TOKEN is not None),\"Token not update in the notebook, not passed as arg, and not found as environment variable.\"\n", "\n", "# setup the base url for the api\n", "BASE_URL = 'https://%s/api/2.0/' % (DOMAIN)\n", "my_header = {\"Authorization\": b\"Basic \" + base64.standard_b64encode(b\"token:\" + str.encode(TOKEN))}"]}, {"block": 9, "type": "markdown", "linesLength": 3, "startIndex": 138, "lines": ["## Confirm connectivity works\n", "\n", "Just use workspace/list to confirm that connectivity to the workspace works."]}, {"block": 10, "type": "code", "linesLength": 7, "startIndex": 141, "lines": ["response = requests.get(\n", "    BASE_URL + \"workspace/list\",\n", "    headers = my_header,\n", "    json={\n", "        \"path\": \"/Users/\"\n", "    }\n", ")"]}, {"block": 11, "type": "code", "linesLength": 1, "startIndex": 148, "lines": ["response.json()"]}, {"block": 12, "type": "markdown", "linesLength": 3, "startIndex": 149, "lines": ["## Set up cluster configuration and create it if necessary\n", "\n", "The recommended and tested version of spark is set at `4.1.x-scala2.11`, and python 3 clusters are set by specifying the `spark_env_var` `PYSPARK_PYTHON`. Other fields are configurable, but those two should not be changed unless you are testing."]}, {"block": 13, "type": "code", "linesLength": 14, "startIndex": 152, "lines": ["## setup the config\n", "my_cluster_config = {\n", "  \"cluster_name\": cluster_name,\n", "  \"node_type_id\": node_type_id,\n", "  \"autoscale\" : {\n", "    \"min_workers\": min_workers,\n", "    \"max_workers\": max_workers\n", "  },\n", "  \"autotermination_minutes\": autotermination_minutes,\n", "  \"spark_version\": \"4.1.x-scala2.11\",\n", "  \"spark_env_vars\": {\n", "    \"PYSPARK_PYTHON\": \"/databricks/python3/bin/python3\"\n", "  }\n", "}"]}, {"block": 14, "type": "markdown", "linesLength": 3, "startIndex": 166, "lines": ["## List clusters\n", "\n", "This starts by listing the available clusters. If a cluster of the same name already exists. It grabs its cluster_id and attempts to continue. If a cluster of that name does NOT already exist, it will send an API call to create it.\n"]}, {"block": 15, "type": "code", "linesLength": 5, "startIndex": 169, "lines": ["response = requests.get(\n", "    BASE_URL + \"clusters/list\",\n", "    headers = my_header\n", ")\n", "cluster_list = response.json()['clusters']"]}, {"block": 16, "type": "markdown", "linesLength": 6, "startIndex": 174, "lines": ["## Search the list for cluster_name\n", "\n", "Only create the cluster if a cluster of the same name doesn't already exist...\n", "\n", "Databricks allows multiple clusters with the same name, so this goes through and checks to see if the same name already exists. If so, this notebook will\n", "install to that cluster."]}, {"block": 17, "type": "code", "linesLength": 28, "startIndex": 180, "lines": ["cluster_ids = [c['cluster_id'] for c in cluster_list if c['cluster_name'] == cluster_name]\n", "\n", "if len(cluster_ids) == 0:\n", "    print(\"\"\"\n", "    no clusters with cluster_name (\"\"\"+cluster_name+\"\"\") found. \n", "    Trying to create it...\n", "    \"\"\")\n", "    ## Post the request...\n", "    response = requests.post(\n", "        BASE_URL + \"clusters/create\",\n", "        headers = my_header,\n", "        json=my_cluster_config\n", "    )\n", "    cluster_id = response.json()['cluster_id']\n", "else:\n", "    print(\"\"\"\n", "    Cluster named \"\"\"+cluster_name+\"\"\" found! \n", "    Using that one. \n", "    Note: It may not have the same configuration as defined in this notebook, \n", "          so you may want to use a different name.\n", "    \"\"\")\n", "    if len(cluster_ids) > 1:\n", "        print(\"\"\"Warning: Multiple clusters with the same name found. Using the first identified.\"\"\")\n", "    cluster_id = cluster_ids[0]\n", "    print(cluster_id)\n", "\n", "if record_for_tests:\n", "    pm.record('cluster_id', cluster_id)"]}, {"block": 18, "type": "markdown", "linesLength": 3, "startIndex": 208, "lines": ["## Now install libraries\n", "\n", "Need to install several from Pypi, a specific jar that needs to be uploaded, and an egg that is built from the Recommenders repository. Can do them all in 1 REST call, so download the relevant data, construct the egg, and upload them to dbfs."]}, {"block": 19, "type": "code", "linesLength": 5, "startIndex": 211, "lines": ["## paths for downloading and uplaoding of relevant jars and eggs\n", "cosmosdb_jar_url = 'https://search.maven.org/remotecontent?filepath=com/microsoft/azure/azure-cosmosdb-spark_2.3.0_2.11/1.2.2/azure-cosmosdb-spark_2.3.0_2.11-1.2.2-uber.jar'\n", "local_jar_filename = cosmosdb_jar_url.split(\"/\")[-1]\n", "upload_cosmosdb_jar_path = \"/tmp/\"+local_jar_filename\n", "upload_reco_utils_egg_path = \"/tmp/reco_utils.egg\""]}, {"block": 20, "type": "code", "linesLength": 9, "startIndex": 216, "lines": ["## download the jar\n", "print(\"*** Downloading cosmosdb jar file to %s\" %(local_jar_filename))\n", "\n", "jar_response = requests.get(cosmosdb_jar_url, stream=True)\n", "with open(local_jar_filename, \"wb\") as handle:\n", "    for chunk in jar_response.iter_content(chunk_size=1024): \n", "        if chunk: # filter out keep-alive new chunks\n", "            handle.write(chunk)\n", "print(\"*** Done.\")"]}, {"block": 21, "type": "markdown", "linesLength": 3, "startIndex": 225, "lines": ["## Upload the jar to the filestore.\n", "\n", "Following the example [here](https://docs.databricks.com/api/latest/examples.html#upload-a-big-file-into-dbfs). Also, looked at the cli github repository to figure out exactly how to encode the [body](https://github.com/databricks/databricks-cli/blob/master/databricks_cli/dbfs/api.py)."]}, {"block": 22, "type": "code", "linesLength": 29, "startIndex": 228, "lines": ["def dbfs_rpc(action, my_header, body):\n", "    \"\"\" A helper function to make the DBFS API request, request/response is encoded/decoded as JSON \"\"\"\n", "    response = requests.post(\n", "        BASE_URL + \"dbfs/\" + action,\n", "        headers=my_header,\n", "        json=body\n", "    )\n", "    return response.json()\n", "\n", "def upload_large_file(local_name, upload_path, my_header):\n", "    print(\"\\tUploading the data to %s...\\n\\tThis can take a few moments...\" %(upload_path))\n", "    # Create a handle that will be used to add blocks\n", "    handle = dbfs_rpc(\"create\", \n", "                      my_header, \n", "                      {\"path\": upload_path, \"overwrite\": \"true\"})['handle']\n", "\n", "    ## go through the blocks...\n", "    with open(local_name, 'rb') as f:\n", "        while True:\n", "            # A block can be at most 1MB\n", "            block = f.read(1 << 20)\n", "            if not block:\n", "                break\n", "            b64data = base64.b64encode(block)\n", "            dbfs_rpc(\"add-block\", my_header, {\"handle\": handle, \"data\": b64data.decode()})\n", "            sys.stdout.write('.')\n", "    # close the handle to finish uploading\n", "    dbfs_rpc(\"close\", my_header, {\"handle\": handle})\n", "    print(\"Done!\")"]}, {"block": 23, "type": "code", "linesLength": 1, "startIndex": 257, "lines": ["upload_large_file(local_jar_filename, upload_cosmosdb_jar_path, my_header)"]}, {"block": 24, "type": "markdown", "linesLength": 3, "startIndex": 258, "lines": ["## Zip up the reco_utils directory\n", "\n", "This assumes you have adjusted the variable `path_to_recommenders_repo_root` so that it points to the directory where you have cloned or downloaded the recommenders [repository](https://github.com/Microsoft/Recommenders/).\n"]}, {"block": 25, "type": "code", "linesLength": 5, "startIndex": 261, "lines": ["myzipfile = shutil.make_archive('reco_utils',\n", "                    'zip', \n", "                    root_dir = path_to_recommenders_repo_root, \n", "                    base_dir = 'reco_utils'\n", "                   )"]}, {"block": 26, "type": "code", "linesLength": 6, "startIndex": 266, "lines": ["local_eggname = myzipfile.replace(\".zip\",\".egg\")\n", "\n", "## overwrite egg if it previously existed\n", "if os.path.exists(local_eggname):\n", "    os.unlink(local_eggname)\n", "os.rename(myzipfile,local_eggname)"]}, {"block": 27, "type": "markdown", "linesLength": 3, "startIndex": 272, "lines": ["## Upload the egg.\n", "\n", "This is a small file, but do it this way in case it ever gets bigger... Reuse `upload_large_file()` from above..."]}, {"block": 28, "type": "code", "linesLength": 1, "startIndex": 275, "lines": ["upload_large_file(local_eggname, upload_reco_utils_egg_path, my_header)"]}, {"block": 29, "type": "markdown", "linesLength": 3, "startIndex": 276, "lines": ["## Install Libraries\n", "\n", "Set up the libraries configuration, then post the request."]}, {"block": 30, "type": "code", "linesLength": 26, "startIndex": 279, "lines": ["my_lib_config = {\n", "  \"cluster_id\": cluster_id,\n", "  \"libraries\": [\n", "    {\n", "      \"jar\": \"dbfs:\"+upload_cosmosdb_jar_path\n", "    },\n", "    {\n", "      \"egg\": \"dbfs:\"+upload_reco_utils_egg_path\n", "    },\n", "    {\n", "      \"pypi\": {\n", "        \"package\": \"azure-cli\"\n", "      }\n", "    },\n", "    {\n", "      \"pypi\": {\n", "        \"package\": \"azureml-sdk[databricks]\"\n", "      }\n", "    },\n", "    {\n", "      \"pypi\": {\n", "        \"package\": \"pydocumentdb\"\n", "      }\n", "    }\n", "  ]\n", "}"]}, {"block": 31, "type": "code", "linesLength": 11, "startIndex": 305, "lines": ["## This requires the cluster to be started or at least pending.\n", "## it will return an \"unknown cluster\" error if the cluster is off.\n", "response = requests.post(\n", "        BASE_URL + \"libraries/install\",\n", "        headers = my_header,\n", "        json=my_lib_config\n", ")\n", "response.json()\n", "if record_for_tests:\n", "    pm.record('lib_install_code', response.status_code)\n", "    pm.record('lib_install_json', response.json())"]}, {"block": 32, "type": "markdown", "linesLength": 1, "startIndex": 316, "lines": ["## Check Install Status\n"]}, {"block": 33, "type": "code", "linesLength": 9, "startIndex": 317, "lines": ["response = requests.get(\n", "        BASE_URL + 'libraries/cluster-status?cluster_id='+cluster_id,\n", "        headers = my_header\n", ")\n", "response.json()\n", "\n", "if record_for_tests:\n", "    pm.record('lib_status_code', response.status_code)\n", "    pm.record('lib_status_json', response.json())"]}, {"block": 34, "type": "markdown", "linesLength": 6, "startIndex": 326, "lines": ["## Upload the end-to-end Notebook\n", "\n", "This also just uploads the end-to-end notebook that creates all resources for the architecture and trains and deploys a model.\n", "\n", "Need to use import here, because `/Shared` on DBFS is not the same as `/Shared`\n", "in the workspace."]}, {"block": 35, "type": "code", "linesLength": 11, "startIndex": 332, "lines": ["local_path_to_ref_arch_notebook = path_to_recommenders_repo_root+\"notebooks/05_operationalize/als_movie_o16n.ipynb\"\n", "\n", "with open(local_path_to_ref_arch_notebook, 'rb') as f:\n", "    notebook_data = f.read()\n", "\n", "import_config = {\n", "  \"path\": upload_location_for_endtoend_notebook+'/als_movie_o16n.ipynb',\n", "  \"format\": \"JUPYTER\",\n", "  \"language\": \"PYTHON\",\n", "  \"overwrite\": \"false\"\n", "}"]}, {"block": 36, "type": "code", "linesLength": 11, "startIndex": 343, "lines": ["response = requests.post(\n", "        BASE_URL + \"workspace/import\",\n", "        headers = my_header,\n", "        data=import_config,\n", "        files = {\"content\": notebook_data}\n", ")\n", "response.json()\n", "\n", "if record_for_tests:\n", "    pm.record('nb_upload_code', response.status_code)\n", "    pm.record('nb_upload_json', response.json())"]}, {"block": 37, "type": "markdown", "linesLength": 3, "startIndex": 354, "lines": ["## List the directory contents\n", "\n", "Just to confirm it's there."]}, {"block": 38, "type": "code", "linesLength": 8, "startIndex": 357, "lines": ["response = requests.get(\n", "    BASE_URL + \"workspace/list\",\n", "    headers = my_header,\n", "    json={\n", "        \"path\": upload_location_for_endtoend_notebook\n", "    }\n", ")\n", "response.json()"]}, {"block": 39, "type": "markdown", "linesLength": 3, "startIndex": 365, "lines": ["## Done!\n", "\n", "Now, just navigate to your cluster, and you should be able to navigate to where you uploaded the notebook, and walk through it to create the end-to-end architecture."]}, {"block": 40, "type": "code", "linesLength": 0, "startIndex": 368, "lines": []}]