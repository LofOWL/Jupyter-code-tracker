[{"block": 0, "type": "code", "linesLength": 8, "startIndex": 0, "lines": ["%reload_ext autoreload\n", "%autoreload 2\n", "%matplotlib inline\n", "\n", "from fastai.io import *\n", "from fastai.conv_learner import *\n", "\n", "from fastai.column_data import *"]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 8, "lines": ["## Setup"]}, {"block": 2, "type": "markdown", "linesLength": 1, "startIndex": 9, "lines": ["We're going to download the collected works of Nietzsche to use as our data for this class."]}, {"block": 3, "type": "code", "linesLength": 1, "startIndex": 10, "lines": ["PATH='data/nietzsche/'"]}, {"block": 4, "type": "code", "linesLength": 3, "startIndex": 11, "lines": ["get_data(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\", f'{PATH}nietzsche.txt')\n", "text = open(f'{PATH}nietzsche.txt').read()\n", "print('corpus length:', len(text))"]}, {"block": 5, "type": "code", "linesLength": 1, "startIndex": 14, "lines": ["text[:400]"]}, {"block": 6, "type": "code", "linesLength": 3, "startIndex": 15, "lines": ["chars = sorted(list(set(text)))\n", "vocab_size = len(chars)+1\n", "print('total chars:', vocab_size)"]}, {"block": 7, "type": "markdown", "linesLength": 1, "startIndex": 18, "lines": ["Sometimes it's useful to have a zero value in the dataset, e.g. for padding"]}, {"block": 8, "type": "code", "linesLength": 1, "startIndex": 19, "lines": ["chars.insert(0, \"\\0\")"]}, {"block": 9, "type": "code", "linesLength": 1, "startIndex": 20, "lines": ["''.join(chars[1:-6])"]}, {"block": 10, "type": "markdown", "linesLength": 1, "startIndex": 21, "lines": ["Map from chars to indices and back again"]}, {"block": 11, "type": "code", "linesLength": 2, "startIndex": 22, "lines": ["char_indices = dict((c, i) for i, c in enumerate(chars))\n", "indices_char = dict((i, c) for i, c in enumerate(chars))"]}, {"block": 12, "type": "markdown", "linesLength": 1, "startIndex": 24, "lines": ["*idx* will be the data we use from now own - it simply converts all the characters to their index (based on the mapping above)"]}, {"block": 13, "type": "code", "linesLength": 1, "startIndex": 25, "lines": ["idx = [char_indices[c] for c in text]"]}, {"block": 14, "type": "code", "linesLength": 1, "startIndex": 26, "lines": ["idx[:10]"]}, {"block": 15, "type": "code", "linesLength": 1, "startIndex": 27, "lines": ["''.join(indices_char[i] for i in idx[:70])"]}, {"block": 16, "type": "markdown", "linesLength": 1, "startIndex": 28, "lines": ["## Three char model"]}, {"block": 17, "type": "markdown", "linesLength": 1, "startIndex": 29, "lines": ["### Create inputs"]}, {"block": 18, "type": "markdown", "linesLength": 1, "startIndex": 30, "lines": ["Create a list of every 4th character, starting at the 0th, 1st, 2nd, then 3rd characters"]}, {"block": 19, "type": "code", "linesLength": 5, "startIndex": 31, "lines": ["cs=3\n", "c1_dat = [idx[i]   for i in range(0, len(idx)-1-cs, cs)]\n", "c2_dat = [idx[i+1] for i in range(0, len(idx)-1-cs, cs)]\n", "c3_dat = [idx[i+2] for i in range(0, len(idx)-1-cs, cs)]\n", "c4_dat = [idx[i+3] for i in range(0, len(idx)-1-cs, cs)]"]}, {"block": 20, "type": "markdown", "linesLength": 1, "startIndex": 36, "lines": ["Our inputs"]}, {"block": 21, "type": "code", "linesLength": 3, "startIndex": 37, "lines": ["x1 = np.stack(c1_dat[:-2])\n", "x2 = np.stack(c2_dat[:-2])\n", "x3 = np.stack(c3_dat[:-2])"]}, {"block": 22, "type": "markdown", "linesLength": 1, "startIndex": 40, "lines": ["Our output"]}, {"block": 23, "type": "code", "linesLength": 1, "startIndex": 41, "lines": ["y = np.stack(c4_dat[:-2])"]}, {"block": 24, "type": "markdown", "linesLength": 1, "startIndex": 42, "lines": ["The first 4 inputs and outputs"]}, {"block": 25, "type": "code", "linesLength": 1, "startIndex": 43, "lines": ["x1[:4], x2[:4], x3[:4]"]}, {"block": 26, "type": "code", "linesLength": 1, "startIndex": 44, "lines": ["y[:4]"]}, {"block": 27, "type": "code", "linesLength": 1, "startIndex": 45, "lines": ["x1.shape, y.shape"]}, {"block": 28, "type": "markdown", "linesLength": 1, "startIndex": 46, "lines": ["### Create and train model"]}, {"block": 29, "type": "markdown", "linesLength": 1, "startIndex": 47, "lines": ["Pick a size for our hidden state"]}, {"block": 30, "type": "code", "linesLength": 1, "startIndex": 48, "lines": ["n_hidden = 256"]}, {"block": 31, "type": "markdown", "linesLength": 1, "startIndex": 49, "lines": ["The number of latent factors to create (i.e. the size of the embedding matrix)"]}, {"block": 32, "type": "code", "linesLength": 1, "startIndex": 50, "lines": ["n_fac = 42"]}, {"block": 33, "type": "code", "linesLength": 25, "startIndex": 51, "lines": ["class Char3Model(nn.Module):\n", "    def __init__(self, vocab_size, n_fac):\n", "        super().__init__()\n", "        self.e = nn.Embedding(vocab_size, n_fac)\n", "\n", "        # The 'green arrow' from our diagram - the layer operation from input to hidden\n", "        self.l_in = nn.Linear(n_fac, n_hidden)\n", "\n", "        # The 'orange arrow' from our diagram - the layer operation from hidden to hidden\n", "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n", "        \n", "        # The 'blue arrow' from our diagram - the layer operation from hidden to output\n", "        self.l_out = nn.Linear(n_hidden, vocab_size)\n", "        \n", "    def forward(self, c1, c2, c3):\n", "        in1 = F.relu(self.l_in(self.e(c1)))\n", "        in2 = F.relu(self.l_in(self.e(c2)))\n", "        in3 = F.relu(self.l_in(self.e(c3)))\n", "        \n", "        h = V(torch.zeros(in1.size()).cuda())\n", "        h = F.tanh(self.l_hidden(h+in1))\n", "        h = F.tanh(self.l_hidden(h+in2))\n", "        h = F.tanh(self.l_hidden(h+in3))\n", "        \n", "        return F.log_softmax(self.l_out(h))"]}, {"block": 34, "type": "code", "linesLength": 1, "startIndex": 76, "lines": ["md = ColumnarModelData.from_arrays('.', [-1], np.stack([x1,x2,x3], axis=1), y, bs=512)"]}, {"block": 35, "type": "code", "linesLength": 1, "startIndex": 77, "lines": ["m = Char3Model(vocab_size, n_fac).cuda()"]}, {"block": 36, "type": "code", "linesLength": 3, "startIndex": 78, "lines": ["it = iter(md.trn_dl)\n", "*xs,yt = next(it)\n", "t = m(*V(xs))"]}, {"block": 37, "type": "code", "linesLength": 1, "startIndex": 81, "lines": ["opt = optim.Adam(m.parameters(), 1e-2)"]}, {"block": 38, "type": "code", "linesLength": 1, "startIndex": 82, "lines": ["fit(m, md, 1, opt, F.nll_loss)"]}, {"block": 39, "type": "code", "linesLength": 1, "startIndex": 83, "lines": ["set_lrs(opt, 0.001)"]}, {"block": 40, "type": "code", "linesLength": 1, "startIndex": 84, "lines": ["fit(m, md, 1, opt, F.nll_loss)"]}, {"block": 41, "type": "markdown", "linesLength": 1, "startIndex": 85, "lines": ["### Test model"]}, {"block": 42, "type": "code", "linesLength": 5, "startIndex": 86, "lines": ["def get_next(inp):\n", "    idxs = T(np.array([char_indices[c] for c in inp]))\n", "    p = m(*VV(idxs))\n", "    i = np.argmax(to_np(p))\n", "    return chars[i]"]}, {"block": 43, "type": "code", "linesLength": 1, "startIndex": 91, "lines": ["get_next('y. ')"]}, {"block": 44, "type": "code", "linesLength": 1, "startIndex": 92, "lines": ["get_next('ppl')"]}, {"block": 45, "type": "code", "linesLength": 1, "startIndex": 93, "lines": ["get_next(' th')"]}, {"block": 46, "type": "code", "linesLength": 1, "startIndex": 94, "lines": ["get_next('and')"]}, {"block": 47, "type": "markdown", "linesLength": 1, "startIndex": 95, "lines": ["## Our first RNN!"]}, {"block": 48, "type": "markdown", "linesLength": 1, "startIndex": 96, "lines": ["### Create inputs"]}, {"block": 49, "type": "markdown", "linesLength": 1, "startIndex": 97, "lines": ["This is the size of our unrolled RNN."]}, {"block": 50, "type": "code", "linesLength": 1, "startIndex": 98, "lines": ["cs=8"]}, {"block": 51, "type": "markdown", "linesLength": 1, "startIndex": 99, "lines": ["For each of 0 through 7, create a list of every 8th character with that starting point. These will be the 8 inputs to out model."]}, {"block": 52, "type": "code", "linesLength": 1, "startIndex": 100, "lines": ["c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(len(idx)-cs-1)]"]}, {"block": 53, "type": "markdown", "linesLength": 1, "startIndex": 101, "lines": ["Then create a list of the next character in each of these series. This will be the labels for our model."]}, {"block": 54, "type": "code", "linesLength": 1, "startIndex": 102, "lines": ["c_out_dat = [idx[j+cs] for j in range(len(idx)-cs-1)]"]}, {"block": 55, "type": "code", "linesLength": 1, "startIndex": 103, "lines": ["xs = np.stack(c_in_dat, axis=0)"]}, {"block": 56, "type": "code", "linesLength": 1, "startIndex": 104, "lines": ["xs.shape"]}, {"block": 57, "type": "code", "linesLength": 1, "startIndex": 105, "lines": ["y = np.stack(c_out_dat)"]}, {"block": 58, "type": "markdown", "linesLength": 1, "startIndex": 106, "lines": ["So each column below is one series of 8 characters from the text."]}, {"block": 59, "type": "code", "linesLength": 1, "startIndex": 107, "lines": ["xs[:cs,:cs]"]}, {"block": 60, "type": "markdown", "linesLength": 1, "startIndex": 108, "lines": ["...and this is the next character after each sequence."]}, {"block": 61, "type": "code", "linesLength": 1, "startIndex": 109, "lines": ["y[:cs]"]}, {"block": 62, "type": "markdown", "linesLength": 1, "startIndex": 110, "lines": ["### Create and train model"]}, {"block": 63, "type": "code", "linesLength": 1, "startIndex": 111, "lines": ["val_idx = get_cv_idxs(len(idx)-cs-1)"]}, {"block": 64, "type": "code", "linesLength": 1, "startIndex": 112, "lines": ["md = ColumnarModelData.from_arrays('.', val_idx, xs, y, bs=512)"]}, {"block": 65, "type": "code", "linesLength": 16, "startIndex": 113, "lines": ["class CharLoopModel(nn.Module):\n", "    def __init__(self, vocab_size, n_fac):\n", "        super().__init__()\n", "        self.e = nn.Embedding(vocab_size, n_fac)\n", "        self.l_in = nn.Linear(n_fac, n_hidden)\n", "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n", "        self.l_out = nn.Linear(n_hidden, vocab_size)\n", "        \n", "    def forward(self, *cs):\n", "        bs = cs[0].size(0)\n", "        h = V(torch.zeros(bs, n_hidden).cuda())\n", "        for c in cs:\n", "            inp = F.relu(self.l_in(self.e(c)))\n", "            h = F.tanh(self.l_hidden(h+inp))\n", "        \n", "        return F.log_softmax(self.l_out(h))"]}, {"block": 66, "type": "code", "linesLength": 2, "startIndex": 129, "lines": ["m = CharLoopModel(vocab_size, n_fac).cuda()\n", "opt = optim.Adam(m.parameters(), 1e-2)"]}, {"block": 67, "type": "code", "linesLength": 1, "startIndex": 131, "lines": ["fit(m, md, 1, opt, F.nll_loss)"]}, {"block": 68, "type": "code", "linesLength": 1, "startIndex": 132, "lines": ["set_lrs(opt, 0.001)"]}, {"block": 69, "type": "code", "linesLength": 1, "startIndex": 133, "lines": ["fit(m, md, 1, opt, F.nll_loss)"]}, {"block": 70, "type": "code", "linesLength": 17, "startIndex": 134, "lines": ["class CharLoopConcatModel(nn.Module):\n", "    def __init__(self, vocab_size, n_fac):\n", "        super().__init__()\n", "        self.e = nn.Embedding(vocab_size, n_fac)\n", "        self.l_in = nn.Linear(n_fac+n_hidden, n_hidden)\n", "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n", "        self.l_out = nn.Linear(n_hidden, vocab_size)\n", "        \n", "    def forward(self, *cs):\n", "        bs = cs[0].size(0)\n", "        h = V(torch.zeros(bs, n_hidden).cuda())\n", "        for c in cs:\n", "            inp = torch.cat((h, self.e(c)), 1)\n", "            inp = F.relu(self.l_in(inp))\n", "            h = F.tanh(self.l_hidden(inp))\n", "        \n", "        return F.log_softmax(self.l_out(h))"]}, {"block": 71, "type": "code", "linesLength": 2, "startIndex": 151, "lines": ["m = CharLoopConcatModel(vocab_size, n_fac).cuda()\n", "opt = optim.Adam(m.parameters(), 1e-3)"]}, {"block": 72, "type": "code", "linesLength": 3, "startIndex": 153, "lines": ["it = iter(md.trn_dl)\n", "*xs,yt = next(it)\n", "t = m(*V(xs))"]}, {"block": 73, "type": "code", "linesLength": 1, "startIndex": 156, "lines": ["fit(m, md, 1, opt, F.nll_loss)"]}, {"block": 74, "type": "code", "linesLength": 1, "startIndex": 157, "lines": ["set_lrs(opt, 1e-4)"]}, {"block": 75, "type": "code", "linesLength": 1, "startIndex": 158, "lines": ["fit(m, md, 1, opt, F.nll_loss)"]}, {"block": 76, "type": "markdown", "linesLength": 1, "startIndex": 159, "lines": ["### Test model"]}, {"block": 77, "type": "code", "linesLength": 5, "startIndex": 160, "lines": ["def get_next(inp):\n", "    idxs = T(np.array([char_indices[c] for c in inp]))\n", "    p = m(*VV(idxs))\n", "    i = np.argmax(to_np(p))\n", "    return chars[i]"]}, {"block": 78, "type": "code", "linesLength": 1, "startIndex": 165, "lines": ["get_next('for thos')"]}, {"block": 79, "type": "code", "linesLength": 1, "startIndex": 166, "lines": ["get_next('part of ')"]}, {"block": 80, "type": "code", "linesLength": 1, "startIndex": 167, "lines": ["get_next('queens a')"]}, {"block": 81, "type": "markdown", "linesLength": 1, "startIndex": 168, "lines": ["## RNN with pytorch"]}, {"block": 82, "type": "code", "linesLength": 14, "startIndex": 169, "lines": ["class CharRnn(nn.Module):\n", "    def __init__(self, vocab_size, n_fac):\n", "        super().__init__()\n", "        self.e = nn.Embedding(vocab_size, n_fac)\n", "        self.rnn = nn.RNN(n_fac, n_hidden)\n", "        self.l_out = nn.Linear(n_hidden, vocab_size)\n", "        \n", "    def forward(self, *cs):\n", "        bs = cs[0].size(0)\n", "        h = V(torch.zeros(1, bs, n_hidden))\n", "        inp = self.e(torch.stack(cs))\n", "        outp,h = self.rnn(inp, h)\n", "        \n", "        return F.log_softmax(self.l_out(outp[-1]))"]}, {"block": 83, "type": "code", "linesLength": 2, "startIndex": 183, "lines": ["m = CharRnn(vocab_size, n_fac).cuda()\n", "opt = optim.Adam(m.parameters(), 1e-3)"]}, {"block": 84, "type": "code", "linesLength": 2, "startIndex": 185, "lines": ["it = iter(md.trn_dl)\n", "*xs,yt = next(it)"]}, {"block": 85, "type": "code", "linesLength": 2, "startIndex": 187, "lines": ["t = m.e(V(torch.stack(xs)))\n", "t.size()"]}, {"block": 86, "type": "code", "linesLength": 3, "startIndex": 189, "lines": ["ht = V(torch.zeros(1, 512,n_hidden))\n", "outp, hn = m.rnn(t, ht)\n", "outp.size(), hn.size()"]}, {"block": 87, "type": "code", "linesLength": 1, "startIndex": 192, "lines": ["t = m(*V(xs)); t.size()"]}, {"block": 88, "type": "code", "linesLength": 1, "startIndex": 193, "lines": ["fit(m, md, 4, opt, F.nll_loss)"]}, {"block": 89, "type": "code", "linesLength": 1, "startIndex": 194, "lines": ["set_lrs(opt, 1e-4)"]}, {"block": 90, "type": "code", "linesLength": 1, "startIndex": 195, "lines": ["fit(m, md, 2, opt, F.nll_loss)"]}, {"block": 91, "type": "markdown", "linesLength": 1, "startIndex": 196, "lines": ["### Test model"]}, {"block": 92, "type": "code", "linesLength": 5, "startIndex": 197, "lines": ["def get_next(inp):\n", "    idxs = T(np.array([char_indices[c] for c in inp]))\n", "    p = m(*VV(idxs))\n", "    i = np.argmax(to_np(p))\n", "    return chars[i]"]}, {"block": 93, "type": "code", "linesLength": 1, "startIndex": 202, "lines": ["get_next('for thos')"]}, {"block": 94, "type": "code", "linesLength": 7, "startIndex": 203, "lines": ["def get_next_n(inp, n):\n", "    res = inp\n", "    for i in range(n):\n", "        c = get_next(inp)\n", "        res += c\n", "        inp = inp[1:]+c\n", "    return res"]}, {"block": 95, "type": "code", "linesLength": 1, "startIndex": 210, "lines": ["get_next_n('for thos', 40)"]}, {"block": 96, "type": "markdown", "linesLength": 1, "startIndex": 211, "lines": ["## Multi-output model"]}, {"block": 97, "type": "markdown", "linesLength": 1, "startIndex": 212, "lines": ["### Setup"]}, {"block": 98, "type": "markdown", "linesLength": 1, "startIndex": 213, "lines": ["Let's take non-overlapping sets of characters this time"]}, {"block": 99, "type": "code", "linesLength": 1, "startIndex": 214, "lines": ["c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(0, len(idx)-cs-1, cs)]"]}, {"block": 100, "type": "markdown", "linesLength": 1, "startIndex": 215, "lines": ["Then create the exact same thing, offset by 1, as our labels"]}, {"block": 101, "type": "code", "linesLength": 1, "startIndex": 216, "lines": ["c_out_dat = [[idx[i+j] for i in range(cs)] for j in range(1, len(idx)-cs, cs)]"]}, {"block": 102, "type": "code", "linesLength": 2, "startIndex": 217, "lines": ["xs = np.stack(c_in_dat)\n", "xs.shape"]}, {"block": 103, "type": "code", "linesLength": 2, "startIndex": 219, "lines": ["ys = np.stack(c_out_dat)\n", "ys.shape"]}, {"block": 104, "type": "code", "linesLength": 1, "startIndex": 221, "lines": ["xs[:cs,:cs]"]}, {"block": 105, "type": "code", "linesLength": 1, "startIndex": 222, "lines": ["ys[:cs,:cs]"]}, {"block": 106, "type": "markdown", "linesLength": 1, "startIndex": 223, "lines": ["### Create and train model"]}, {"block": 107, "type": "code", "linesLength": 1, "startIndex": 224, "lines": ["val_idx = get_cv_idxs(len(xs)-cs-1)"]}, {"block": 108, "type": "code", "linesLength": 1, "startIndex": 225, "lines": ["md = ColumnarModelData.from_arrays('.', val_idx, xs, ys, bs=512)"]}, {"block": 109, "type": "code", "linesLength": 13, "startIndex": 226, "lines": ["class CharSeqRnn(nn.Module):\n", "    def __init__(self, vocab_size, n_fac):\n", "        super().__init__()\n", "        self.e = nn.Embedding(vocab_size, n_fac)\n", "        self.rnn = nn.RNN(n_fac, n_hidden)\n", "        self.l_out = nn.Linear(n_hidden, vocab_size)\n", "        \n", "    def forward(self, *cs):\n", "        bs = cs[0].size(0)\n", "        h = V(torch.zeros(1, bs, n_hidden))\n", "        inp = self.e(torch.stack(cs))\n", "        outp,h = self.rnn(inp, h)\n", "        return F.log_softmax(self.l_out(outp))"]}, {"block": 110, "type": "code", "linesLength": 2, "startIndex": 239, "lines": ["m = CharSeqRnn(vocab_size, n_fac).cuda()\n", "opt = optim.Adam(m.parameters(), 1e-3)"]}, {"block": 111, "type": "code", "linesLength": 2, "startIndex": 241, "lines": ["it = iter(md.trn_dl)\n", "*xst,yt = next(it)"]}, {"block": 112, "type": "code", "linesLength": 4, "startIndex": 243, "lines": ["def nll_loss_seq(inp, targ):\n", "    sl,bs,nh = inp.size()\n", "    targ = targ.transpose(0,1).contiguous().view(-1)\n", "    return F.nll_loss(inp.view(-1,nh), targ)"]}, {"block": 113, "type": "code", "linesLength": 1, "startIndex": 247, "lines": ["fit(m, md, 4, opt, nll_loss_seq)"]}, {"block": 114, "type": "code", "linesLength": 1, "startIndex": 248, "lines": ["set_lrs(opt, 1e-4)"]}, {"block": 115, "type": "code", "linesLength": 1, "startIndex": 249, "lines": ["fit(m, md, 1, opt, nll_loss_seq)"]}, {"block": 116, "type": "markdown", "linesLength": 1, "startIndex": 250, "lines": ["### Identity init!"]}, {"block": 117, "type": "code", "linesLength": 2, "startIndex": 251, "lines": ["m = CharSeqRnn(vocab_size, n_fac).cuda()\n", "opt = optim.Adam(m.parameters(), 1e-2)"]}, {"block": 118, "type": "code", "linesLength": 1, "startIndex": 253, "lines": ["m.rnn.weight_hh_l0.data.copy_(torch.eye(n_hidden))"]}, {"block": 119, "type": "code", "linesLength": 1, "startIndex": 254, "lines": ["fit(m, md, 4, opt, nll_loss_seq)"]}, {"block": 120, "type": "code", "linesLength": 1, "startIndex": 255, "lines": ["set_lrs(opt, 1e-3)"]}, {"block": 121, "type": "code", "linesLength": 1, "startIndex": 256, "lines": ["fit(m, md, 4, opt, nll_loss_seq)"]}, {"block": 122, "type": "code", "linesLength": 1, "startIndex": 257, "lines": ["set_lrs(opt, 1e-4)"]}, {"block": 123, "type": "code", "linesLength": 1, "startIndex": 258, "lines": ["fit(m, md, 4, opt, nll_loss_seq)"]}, {"block": 124, "type": "markdown", "linesLength": 1, "startIndex": 259, "lines": ["## Stateful model"]}, {"block": 125, "type": "markdown", "linesLength": 1, "startIndex": 260, "lines": ["### Setup"]}, {"block": 126, "type": "code", "linesLength": 4, "startIndex": 261, "lines": ["from torchtext import vocab, data\n", "\n", "from fastai.nlp import *\n", "from fastai.lm_rnn import *"]}, {"block": 127, "type": "code", "linesLength": 8, "startIndex": 265, "lines": ["PATH='data/nietzsche/'\n", "\n", "TRN_PATH = 'trn/'\n", "VAL_PATH = 'val/'\n", "TRN = f'{PATH}{TRN_PATH}'\n", "VAL = f'{PATH}{VAL_PATH}'\n", "\n", "%ls {PATH}"]}, {"block": 128, "type": "code", "linesLength": 1, "startIndex": 273, "lines": ["TEXT = data.Field(lower=True, tokenize=list)"]}, {"block": 129, "type": "code", "linesLength": 1, "startIndex": 274, "lines": ["bs=64; bptt=8; n_fac=42; n_hidden=256"]}, {"block": 130, "type": "code", "linesLength": 2, "startIndex": 275, "lines": ["FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n", "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)"]}, {"block": 131, "type": "code", "linesLength": 1, "startIndex": 277, "lines": ["len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"]}, {"block": 132, "type": "code", "linesLength": 15, "startIndex": 278, "lines": ["class CharSeqStatefulRnn(nn.Module):\n", "    def __init__(self, vocab_size, n_fac, bs):\n", "        super().__init__()\n", "        self.e = nn.Embedding(vocab_size, n_fac)\n", "        self.rnn = nn.RNN(n_fac, n_hidden)\n", "        self.l_out = nn.Linear(n_hidden, vocab_size)\n", "        self.h = V(torch.zeros(1, bs, n_hidden))\n", "        \n", "    def forward(self, cs):\n", "        bs = cs[0].size(0)\n", "        if self.h.size(1) != bs: self.h = V(torch.zeros(1, bs, n_hidden))\n", "        inp = self.e(cs)\n", "        outp,h = self.rnn(inp, self.h)\n", "        self.h = repackage_var(h)\n", "        return F.log_softmax(self.l_out(outp)).view(-1,vocab_size)"]}, {"block": 133, "type": "code", "linesLength": 2, "startIndex": 293, "lines": ["m = CharSeqStatefulRnn(vocab_size, n_fac, 512).cuda()\n", "opt = optim.Adam(m.parameters(), 1e-3)"]}, {"block": 134, "type": "code", "linesLength": 1, "startIndex": 295, "lines": ["m.rnn.weight_hh_l0.data.copy_(torch.eye(n_hidden));"]}, {"block": 135, "type": "code", "linesLength": 1, "startIndex": 296, "lines": ["fit(m, md, 1, opt, F.nll_loss)"]}, {"block": 136, "type": "code", "linesLength": 1, "startIndex": 297, "lines": ["set_lrs(opt, 1e-3)"]}, {"block": 137, "type": "code", "linesLength": 1, "startIndex": 298, "lines": ["fit(m, md, 2, opt, F.nll_loss)"]}, {"block": 138, "type": "code", "linesLength": 1, "startIndex": 299, "lines": ["fit(m, md, 2, opt, F.nll_loss)"]}, {"block": 139, "type": "code", "linesLength": 1, "startIndex": 300, "lines": ["fit(m, md, 4, opt, F.nll_loss)"]}, {"block": 140, "type": "code", "linesLength": 1, "startIndex": 301, "lines": ["fit(m, md, 4, opt, F.nll_loss)"]}, {"block": 141, "type": "code", "linesLength": 1, "startIndex": 302, "lines": ["set_lrs(opt, 1e-4)"]}, {"block": 142, "type": "code", "linesLength": 1, "startIndex": 303, "lines": ["fit(m, md, 4, opt, F.nll_loss)"]}, {"block": 143, "type": "markdown", "linesLength": 1, "startIndex": 304, "lines": ["### Test"]}, {"block": 144, "type": "code", "linesLength": 7, "startIndex": 305, "lines": ["def get_next(inp):\n", "    idxs = TEXT.numericalize(inp)\n", "    p = m(VV(idxs.transpose(0,1)))\n", "#     print(p)\n", "    r = np.argmax(to_np(p), axis=1)\n", "#     print(r.shape)\n", "    return TEXT.vocab.itos[r[-1]]"]}, {"block": 145, "type": "code", "linesLength": 1, "startIndex": 312, "lines": ["get_next('for thos')"]}, {"block": 146, "type": "code", "linesLength": 7, "startIndex": 313, "lines": ["def get_next_n(inp, n):\n", "    res = inp\n", "    for i in range(n):\n", "        c = get_next(inp)\n", "        res += c\n", "        inp = inp[1:]+c\n", "    return res"]}, {"block": 147, "type": "code", "linesLength": 1, "startIndex": 320, "lines": ["get_next_n('for thos', 40)"]}, {"block": 148, "type": "code", "linesLength": 0, "startIndex": 321, "lines": []}]