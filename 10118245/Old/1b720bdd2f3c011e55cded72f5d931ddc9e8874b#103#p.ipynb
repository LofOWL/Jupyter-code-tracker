{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2", 
   "language": "python", 
   "name": "python2"
  }, 
  "name": "", 
  "notebookname": "Web crawler", 
  "version": "2.0"
 }, 
 "nbformat": 2, 
 "nbformat_minor": 0, 
 "orig_nbformat": 4, 
 "orig_nbformat_minor": 0, 
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "<span style=\"float:left;\">Licence CC BY-NC-ND</span><span style=\"float:right;\">Thierry Parmentelat &amp; Arnaud Legout,<img src=\"media/inria-25.png\" style=\"display:inline\"></span><br/>"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "# Crawler Web "
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "## Mini-Projet"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "### Objectifs"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "Dans ce projet, nous allons impl\u00e9menter un simple [crawler Web](http://fr.wikipedia.org/wiki/Robot_d%27indexation), c'est-\u00e0-dire un outil capable de parcourir des pages Web en suivant les URLs. C'est typiquement ce que font les moteurs de recherche comme Google. Notre objectif ici est de jouer avec certains des concepts importants que nous avons d\u00e9couvert dans ce MOOC et de pratiquer quelques modules de la librairie standard, mais nous ne chercherons pas la performance parce que \u00e7a augmenterait tr\u00e8s rapidement la complexit\u00e9 du code et la difficult\u00e9 du sujet. Cependant, vous constaterez que m\u00eame si ce crawler n'est pas adapt\u00e9 \u00e0 crawler des millions de pages, il est parfaitement capable de crawler des milliers de pages et de vous rendre des services (comme identifier les liens morts sur votre site Web)."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "### R\u00e9alisation du crawler Web"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "Ce projet est d\u00e9coup\u00e9 en deux niveaux de difficult\u00e9. Nous allons commencer par le niveau avanc\u00e9 qui va vous demander d'\u00e9crire vous m\u00eame tout le code en fonction de nos sp\u00e9cifications de haut niveau. Pour le niveau interm\u00e9diaire, nous vous fournirons une description plus pr\u00e9cise de notre impl\u00e9mentation. \u00c0 vous de choisir o\u00f9 vous voulez commencer, mais notez que si vous voulez faire le niveau interm\u00e9diaire, vous devrez quand m\u00eame lire le niveau avanc\u00e9, parce qu'il contient des informations importantes sur le fonctionnement du crawler. ", 
      "", 
      "Je vous rappelle qu'une page Web est \u00e9crite dans un langage d\u00e9claratif qui s'appelle [HTML](http://fr.wikipedia.org/wiki/Hypertext_Markup_Language) et que l'on acc\u00e8de \u00e0 ces pages au travers d'un protocole qui s'appelle [HTTP](http://fr.wikipedia.org/wiki/Hypertext_Transfer_Protocol). Nous utiliserons dans ce mini projet la librairie standard `urllib2` qui permet d'utiliser le protocole HTTP de mani\u00e8re tr\u00e8s simple. Par contre, pour l'interpr\u00e9tation du code HTML, nous ferons tout le traitement \u00e0 la main. Il existe des librairies pour vous faciliter la tache (nous en parlerons tout \u00e0 la fin), mais elles supposent une bonne compr\u00e9hension des concepts sous-jacents \u00e0  HTML et XML. Naturellement, vous pouvez les explorer et les essayer dans ce mini projet si vous le souhaitez. "
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "### Niveau avanc\u00e9"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "Le but de notre crawler est le suivant. Nous supposons que nous avons un ensemble (set) de pages Web \u00e0 crawler, c'est-\u00e0-dire, un ensemble de pages Web pour lesquelles on va t\u00e9l\u00e9charger le code HTML. On commence avec une seule page que l'on appelle la page initiale. Dans la suite cette page sera", 
      "", 
      "http://www-sop.inria.fr/members/Arnaud.Legout/", 
      "", 
      "On suppose \u00e9galement que l'on restreint le crawl \u00e0 un sous ensemble de pages Web. Pour cela on d\u00e9finit une liste de pages filtres (page_filter) tel que si n'importe quel \u00e9l\u00e9ment de page_filter est contenu dans l'URL de la page que l'on crawl, alors la page passe le filter. Dans la suite, on aura ", 
      "", 
      "`page_filter = www-sop.inria.fr/members/Arnaud.Legout`", 
      "", 
      "Ensuite notre crawler va&nbsp;:", 
      " * prendre un lien (sans ordre particulier) dans l'ensemble des pages Web \u00e0 crawler", 
      " * se connecter \u00e0 la page correspondant \u00e0 ce lien, puis", 
      "   * enregistrer le [code HTTP](http://fr.wikipedia.org/wiki/Liste_des_codes_HTTP) associ\u00e9 \u00e0 cette page (par exemple, 202 lorsque la requ\u00eate a \u00e9t\u00e9 trait\u00e9e correctement et 404 lorsque le lien est mort). On consid\u00e9rera aussi un code -1 lorsque le site Web ne r\u00e9pond pas.", 
      "   * recup\u00e9rer le code HTML de la page", 
      "   * si la page passe page_filter (typiquement si `filter in page` pour n'importe quel filter dans page_filter) extraire toutes les URLs dans le corps de la page Web (c'est-\u00e0-dire, apr\u00e8s la balise `<body>`) et ajouter \u00e0 l'ensemble des pages Web \u00e0 crawler toutes les URLs commen\u00e7ant par http ou https et toutes les URLs relatives reconstruites commen\u00e7ant par `./` ou `/`", 
      "      * par exemple, pour `./ma_page.html` et le site `http://mon_site.fr/rep1/` on ajoute l'url `http://mon_site.fr/rep1/ma_page.html`", 
      "      * par exemple, pour `/ma_page.html` et le site `http://mon_site.fr/rep1/` on ajoute l'url `http://mon_site.fr/ma_page.html` (sans `rep1`)", 
      " * on recommence au premier point jusqu'\u00e0 ce que l'ensemble des pages Web \u00e0 crawler soit vide.  ", 
      "", 
      "Pour simplifier, on va manipuler le crawler comme un it\u00e9rable, \u00e0 chaque appel de `next()` on fait avancer le crawler d'une page (dans l'ensemble des pages \u00e0 crawler) et on obtient un objet qui contient le code HTTP pour la page, l'URL de la page, et la liste de toutes les URLs contenues dans la page (extraites comme expliqu\u00e9 ci-dessus). "
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "Le but de ce mini projet est d'utiliser le crawler pour identifier tous les liens d\u00e9fectueux (c'est-\u00e0-dire ceux pour lesquels HTTP ne retourne pas un code entre 200 et 299) pour le site `http://www-sop.inria.fr/members/Arnaud.Legout/`. ", 
      "", 
      "Voici le r\u00e9sultat de l'ex\u00e9cution du crawler avec comme page initiale `http://www-sop.inria.fr/members/Arnaud.Legout/` et comme `page_filter` `[www-sop.inria.fr/members/Arnaud.Legout]`. Donc le crawler va  uniquement tester les liens qui sont dans le site `http://www-sop.inria.fr/members/Arnaud.Legout/` "
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "    Page contenant des liens defecteux : ", 
      "    http://www-sop.inria.fr/members/Arnaud.Legout/Projects/p2p_cd.html", 
      "    ------------------------------------------------------------------------------", 
      "    CODE HTTP 404", 
      "            http://dx.doi.org/10.1016/j.comnet.2010.09.014", 
      "            http://www.cs.ucla.edu/~nikitas/", 
      "    ==============================================================================", 
      "    ", 
      "    Page contenant des liens defecteux : ", 
      "    http://www-sop.inria.fr/members/Arnaud.Legout/publications.html", 
      "    ------------------------------------------------------------------------------", 
      "    CODE HTTP 404", 
      "            http://dx.doi.org/10.1016/j.comnet.2010.09.014", 
      "    ==============================================================================", 
      "    ", 
      "    Page contenant des liens defecteux : ", 
      "    http://www-sop.inria.fr/members/Arnaud.Legout/Projects/bluebear.html", 
      "    ------------------------------------------------------------------------------", 
      "    CODE HTTP 303", 
      "            http://bits.blogs.nytimes.com/2011/11/29/skype-can-expose-your-location-researchers-say/", 
      "    CODE HTTP 403", 
      "            https://threatpost.com/en_us/blogs/attacking-and-defending-tor-network-032911", 
      "            http://www.pcinpact.com/actu/news/66544-skype-bittorrent-etude-scientifiques-faille.htm", 
      "    CODE HTTP 404", 
      "            http://www.zataz.com/news/21651/faille--vulnerability-skype.html", 
      "    ==============================================================================", 
      "    ", 
      "    Page contenant des liens defecteux : ", 
      "    http://www-sop.inria.fr/members/Arnaud.Legout/", 
      "    ------------------------------------------------------------------------------", 
      "    CODE HTTP -1", 
      "            http://www.castify.net", 
      "    ==============================================================================", 
      "    ", 
      "    Page contenant des liens defecteux : ", 
      "    http://www-sop.inria.fr/members/Arnaud.Legout/index.html", 
      "    ------------------------------------------------------------------------------", 
      "    CODE HTTP -1", 
      "            http://www.castify.net", 
      "    ==============================================================================", 
      "    ", 
      "    "
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "### Niveau interm\u00e9diaire"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "Tout notre programme peut-\u00eatre \u00e9crit dans un m\u00eame module `webcrawler`. Nous avons dans ce module deux classes. Voici l'aide correspondant \u00e0 ces deux classes"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "##### Classe `HTMLPage`"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "    Help on class HTMLPage in module webcrawler:", 
      "    ", 
      "    class HTMLPage(__builtin__.object)", 
      "     |  represente une page HTML.", 
      "     |", 
      "     |  L'objet a 4 attributs:", 
      "     |      -url: l'URL qui correspond a la page Web", 
      "     |      -_html_it: un iterateur qui parcourt le code HTML, une ligne", 
      "     |                 a la fois", 
      "     |      -urls: la liste de toutes les URLs contenues dans la page", 
      "     |      -http_code: le code retourne par le protocol HTTP lors de", 
      "     |                  l'acces a la page", 
      "     |                  *http_code=0 signifie une erreur dans l'URL,", 
      "     |                  *http_code=-1 signifie que le site de repond pas", 
      "     |                  *http_code=-2 signifie une exception en accedant", 
      "     |                  a l'URL", 
      "     |", 
      "     |  Methods defined here:", 
      "     |", 
      "     |  __init__(self, url)", 
      "     |      Constructeur de la classe. Le constructeur prend comme", 
      "     |      argument une URL et construit un objet HTMLPage en definissant", 
      "     |      les 4 attributs url, _html_it, urls, http_code", 
      "     |", 
      "     |  extract_urls_from_page(self)", 
      "     |      Construit la liste de toutes les URLs contenues dans le corps de", 
      "     |      la page HTML en parcourant l'iterateur retourne par", 
      "     |      page_fetcher()", 
      "     |", 
      "     |      On identifie une URL parce qu'elle est precedee de href= et", 
      "     |      dans le corps (body) de la page. Le parsing que l'on implement", 
      "     |      est imparfait, mais un vrai parsing intelligent demanderait", 
      "     |      une analyse syntaxique trop complexe pour nos besoins.", 
      "     |", 
      "     |      Plus en details, notre parsing consiste a chercher dans le", 
      "     |      corps de la page (body):", 
      "     |", 
      "     |      -les urls contenues dans le champ href (essentiellement on", 
      "     |       cherche le tag 'href=' et on extrait ce qui est entre", 
      "     |       guillemets ou apostrophes)", 
      "     |", 
      "     |      -on ne garde ensuite que les urls qui commencent par http ou", 
      "     |       https et", 
      "     |", 
      "     |           * les urls qui commencent par ./ auxquelles on ajoute", 
      "     |        devant (a la place du point) l'Url de la page d'origine", 
      "     |        (self.url) exemple : pour './ma_page.html' et self.url =", 
      "     |        http://mon_site.fr/rep1/ on obtient l'url", 
      "     |        http://mon_site.fr/rep1/ma_page.html", 
      "     |", 
      "     |          * les urls qui commencent par /ma_page.html auxquelles on", 
      "     |         ajoute devant uniquement le hostname de la page d'origine", 
      "     |         (self.url) exemple : pour '/ma_page.html' et self.url =", 
      "     |         http://mon_site.fr/rep1/ on obtient l'url", 
      "     |         http://mon_site.fr/ma_page.html", 
      "     |", 
      "     |      Cette methode retourne la liste des URLs contenues dans la", 
      "     |      page.", 
      "     |", 
      "     |  page_fetcher(self, url)", 
      "     |      accede a l'URL et retourne un objet qui permet de parcourir le", 
      "     |      code HTML (voir la documentation de urllib2.urlopen) ou une", 
      "     |      liste vide en cas d'erreur.", 
      "     |"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "##### Classe `Crawler`"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "    Help on class Crawler in module webcrawler:", 
      "    ", 
      "    class Crawler(__builtin__.object)", 
      "     |  Cette classe permet de creer l'objet qui va gerer le crawl. Cet", 
      "     |  objet est iterable et l'iterateur va, a chaque tour, retourner un", 
      "     |  nouvel objet HTMLPage.", 
      "     |", 
      "     |  L'instance du crawler va avoir comme principaux attributs", 
      "     |    * l'ensemble des pages a crawler pages_to_be_crawled", 
      "     |    * l'ensemble des pages deja crawles pages_crawled", 
      "     |    * un dictionnaire qui a chaque URL fait correspondre la liste de", 
      "     |    toutes les pages qui ont reference cette URL lors du crawl", 
      "     |    pages_to_be_crawled_dict", 
      "     |", 
      "     |  Methods defined here:", 
      "     |", 
      "     |  __init__(self, seed_url, max_crawled_pages=10000000000L, page_filter=None)", 
      "     |      Constructeur du crawler", 
      "     |", 
      "     |      Le constructeur prend comme arguments", 
      "     |      -seed_url: l'URL de la page a partir de laquelle on demarre le crawl", 
      "     |      -max_crawled_pages: le nombre maximum de pages que l'on va crawler", 
      "     |      (10**10 par defaut)", 
      "     |      -page_filter: la liste des pages sur lesquels le crawler", 
      "     |      doit rester (pas de filtre par defaut). Typiquement, une URL", 
      "     |      passe le filtre si n'importe lequel des elements de page_filter", 
      "     |      est contenu dans l'URL", 
      "     |", 
      "     |  __iter__(self)", 
      "     |      Cette methode est implementee comme une fonction generatrice. A", 
      "     |      chaque appel de next() sur l'iterateur, on obtient un nouvel", 
      "     |      objet HTMLPage qui correspond a une URL qui etait dans", 
      "     |      l'ensemble des URLs a crawler.", 
      "     |", 
      "     |      On ne donne aucune garantie sur l'ordre de parcours des URLs", 
      "     |", 
      "     |  __repr__(self)", 
      "     |      permet d'afficher simplement des informations sur l'etat", 
      "     |      courant du crawl.", 
      "     |", 
      "     |      retourne une chaine de caracteres donnant:", 
      "     |      -le nombre de pages et domaines deja crawle", 
      "     |      -le nombre de pages encore a crawler", 
      "     |      -la duree du dernier crawl", 
      "     |", 
      "     |  update_pages_to_be_crawled(self, page)", 
      "     |      Prend un objet HTMLpage comme argument et trouve toutes les", 
      "     |      URLs presente dans la page HTML correspondante. Cette methode", 
      "     |      met a jour le dictionnaire pages_to_be_crawled_dict et", 
      "     |      l'ensemble pages_to_be_crawled. On ne met pas a jour le", 
      "     |      dictionnaire et le set si l'URL correspondant a l'objet", 
      "     |      HTMLpage n'est pas dans la liste de pages acceptees dans", 
      "     |      self.page_filter.", 
      "     |"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "##### Fonctions utilitaires"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "On a \u00e9galements deux fonctions, qui sont utilis\u00e9es par les classes, dont voici l'aide."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "    Help on function extract_domains_from_url in module webcrawler:", 
      "    ", 
      "    extract_domains_from_url(url)", 
      "        Extrait un domaine d'une URL", 
      "    ", 
      "        Retourne le tuple T qui contient", 
      "        T[0]: domaine avec le bon protocol (http://domain or https://domain)", 
      "        T[1]: domaine sans le protocol (sans http:// or https://)"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "    Help on function is_html_page in module webcrawler:", 
      "    ", 
      "    is_html_page(url)", 
      "        simple heuristique pour tester si une page est ecrite en", 
      "        HTML. Il y a des cas mal identifies par cette heuristique,", 
      "        mais elle est suffisante pour nos besoins. Par exemple:", 
      "        http://inria.fr sera identifie comme non html de meme que", 
      "        toutes les pages qui utilisent des points dans le nom d'un", 
      "        repertoire."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "##### Le mot de la fin"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "Nous avons \u00e0 de nombreuses reprises \u00e9voqu\u00e9 la puissance de la librairie standard, mais aussi des librairies tierces. En particulier, nous avons insist\u00e9 sur le fait qu'au d\u00e9marrage d'un projet, il vaut mieux commencer par chercher si une librairie Python ne fait pas d\u00e9j\u00e0 tout ou partie de ce que vous voulez faire. ", 
      "", 
      "Il existe une librairie Python tr\u00e8s puissante qui permet justement de faire des crawlers&nbsp;: il s'agit de [Scrapy](http://scrapy.org/). Maintenant que vous avez compris les bases d'un crawler Web, vous pourrez tirer pleinement b\u00e9n\u00e9fice de Scrapy. ", 
      "", 
      "Il existe \u00e9galement un librairie pour parser du code HTML, c'est [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/)."
     ]
    }
   ], 
   "metadata": {}
  }
 ]
}
