[{"block": 0, "type": "heading", "linesLength": 1, "startIndex": 0, "lines": ["Crawler Web "]}, {"block": 1, "type": "markdown", "linesLength": 1, "startIndex": 1, "lines": ["Dans ce projet, nous allons impl\u00e9menter un simple [crawler Web](http://fr.wikipedia.org/wiki/Robot_d%27indexation), c'est-\u00e0-dire un outil capable de parcourir des pages Web en suivant les URLs. C'est typiquement ce que font les moteurs de recherche comme Google. Notre objectif ici est de jouer avec certains de concepts importants que nous avons d\u00e9couvert dans ce MOOC et de pratiquer quelques librairies standards, mais nous ne chercherons pas la performance parce que \u00e7a augmenterait tr\u00e8s rapidement la complexit\u00e9 du code et la difficult\u00e9 du sujet. Cependant, vous constaterez que m\u00eame si ce crawler n'est pas adapt\u00e9 \u00e0 crawler des millions de pages, il est parfaitement capable de crawler des dizaines de milliers de pages et de vous rendre des services (comme identifier les liens morts sur un site Web)."]}, {"block": 2, "type": "heading", "linesLength": 1, "startIndex": 2, "lines": ["R\u00e9alisation du crawler Web"]}, {"block": 3, "type": "markdown", "linesLength": 5, "startIndex": 3, "lines": ["Ce projet est d\u00e9coup\u00e9 en trois niveaux de difficult\u00e9s. Nous allons commencer par le niveau avanc\u00e9 qui va vous demander d'\u00e9crire vous m\u00eame tout le code en fonction de nos sp\u00e9cifications. Pour le niveau interm\u00e9diaire, nous vous fournirons une partie du code, et pour le niveau facile, nous vous fournirons tout le code, votre travail se limitant \u00e0 l'utilisation du crawler. \u00c0 vous de choisir o\u00f9 vous voulez commencer. \n", "\n", "Il est tr\u00e8s important de comprendre que le code que l'on vous propose d'\u00e9crire n'est ni totalement fiable, ni valid\u00e9 par des tests. Est-ce que cela est un probl\u00e8me pour ce projet ? Non, mais il est important dans du vrai code de production de le fiabiliser au maximum (en capturant les exceptions avec des reactions appropri\u00e9s et en testant toutes les entr\u00e9es) et de le tester (en ajoutant, par exemple, des tests unitaires et des tests fonctionnels). Cela a \u00e9videment un co\u00fbt tr\u00e8s \u00e9l\u00e9v\u00e9 en terme d'augmentation du code (on peut facilement multiplier par 2 ou 3 le nombre de lignes de code) et de temps de d\u00e9veloppement (il va falloir imaginer tous les cas \u00e0 tester).\n", "\n", "Le but de notre crawler est, \u00e0 partir de l'URL d'une page Web initiale, d'extraire tous les liens hypertexts des pages parcourues et d'utiliser ces liens pour parcourir de nouvelles pages et extraire de nouveaux liens. "]}, {"block": 4, "type": "heading", "linesLength": 1, "startIndex": 8, "lines": ["Niveau avanc\u00e9"]}, {"block": 5, "type": "markdown", "linesLength": 14, "startIndex": 9, "lines": ["Tout notre programme peut-\u00eatre \u00e9crit dans un m\u00eame module. Nous avons dans ce module deux classes.\n", "\n", " * La classe `HTMLPage` repr\u00e9sente une page HTML. En particulier, une instance de cette classe a&nbsp;:\n", "  * un attribut `url` qui est une cha\u00eene de caract\u00e8res repr\u00e9sentant l'URL correspondant \u00e0 la page Web, \n", "  * un attribut `urls` qui est une liste de toutes les URLs trouv\u00e9es dans cette page,\n", "  * un attribut `http_code` qui est le code HTTP retourn\u00e9 par la requ\u00eate sur `url`.\n", "  \n", " Cette classe a trois m\u00e9thodes.\n", "  * Le constructeur prend comme argument une URL (sous forme d'une cha\u00eene de caract\u00e8res).\n", "  * La m\u00e9thode `page_fetcher` prend comme argument une URL et retourne un it\u00e9rateur sur la page HTML ou une liste vide en cas d'erreur. On utilisera la m\u00e9thode `urlopen` dans librairie standard `urllib2`. \n", "  * une m\u00e9thode `extract_urls_from_page` qui va parcourir l'it\u00e9rateur retourn\u00e9 par la m\u00e9thode `page_fetcher` et extraire toutes les URLs dans la page pour cr\u00e9er une liste de toutes les URLs dans la page, liste qui sera r\u00e9f\u00e9renc\u00e9e par l'attribut `urls` de l'instance. Pour extraire une URL, on cherchera dans le `body` de la page Web toutes les cha\u00eene de caract\u00e8res qui sont des valeurs de l'attribut `href` et qui commencent par `http` ou `https` (notons que c'est loin d'\u00eatre parfait). \n", "  \n", " * La classe `Crawler` \n", "  "]}, {"block": 6, "type": "heading", "linesLength": 1, "startIndex": 23, "lines": ["Le mot de la fin"]}, {"block": 7, "type": "markdown", "linesLength": 3, "startIndex": 24, "lines": ["Nous avons \u00e0 de nombreuses reprises \u00e9voqu\u00e9 la puissance de la librairie standard, mais aussi des librairies tierces. En particulier, nous avons insist\u00e9 sur le fait qu'au d\u00e9marrage d'un projet, il vaut mieux commencer par chercher si une librairie Python ne fait pas d\u00e9j\u00e0 tout ou partie de ce que vous voulez faire. \n", "\n", "Il existe une librairie Python tr\u00e8s puissante qui permet justement de faire des crawlers&nbsp;: il s'agit de [Scrapy](http://scrapy.org/). Maintenant que vous avez compris les bases d'un crawler Web, vous pourrez tirer pleinement b\u00e9n\u00e9fice des Scrapy. "]}]